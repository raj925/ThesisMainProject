---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{r, echo=FALSE}

# Colour coding for figures
confidenceColour <- "#03c200"
difficultyColour <- "#bf00c2"
infoSeekingColour <- "#ca0600"
differentialColour <- "skyblue"
likelihoodColour <- "orange"
accuracyColour <- "black"
resolutionColour <- "yellow"
```


# Study 3 - Diagnostic Reasoning Strategies via a Think-Aloud Paradigm {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Think Aloud}{}
```
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

```{r install_packages, include=FALSE}
source('scripts_and_filters/install_packages_if_missing.R')

binarysimilarityMean <- function(m){
  mat <- binarysimilarityMat(m)
  values <- mat[upper.tri(mat)]
  return(c(mean(values),sd(values)^2))
}
```

```{r AggregateData2, include=FALSE, eval=knitr::is_latex_output()}

df <- as.data.frame(read.csv("./study2data.csv",header=TRUE))
source('scripts_and_filters/Study2/AggregateData.R')

TAData <- as.data.frame(read.csv("./study3data.csv",header=TRUE))

###### Info Seeking Variance and Value

infoSeekingMatrixTA <- data.frame()
studentInfoDf <- as.data.frame(read.csv("./studentInfoDf.csv",row.names = 1,header=TRUE))

for (p in 1:nrow(TAData))
{
  infoVector <- TAData$reqTestArray[p]
  infoVector <- str_split(infoVector,", ")[[1]]
  infoVector <- as.numeric(infoVector)
  condition <- TAData$Condition[p]
  
  infoValues <- studentInfoDf[rownames(studentInfoDf) == condition,]

  valueVector <- infoVector
  # Iterate through each element in infoVector
  for (i in 1:length(valueVector)) {
   # Check if the current element is 1
    if (valueVector[i] == 1) {
      # If it is, replace it with the corresponding value from infoValues
      valueVector[i] <- as.double(infoValues[i])
    }
  }
  
  TAData$infoValue[p] <- sum(valueVector)
  
  infoSeekingMatrixTA <- rbind(infoSeekingMatrixTA, infoVector)
  rownames(infoSeekingMatrixTA)[p] <- paste(TAData$ID[p],
                                            "-OverallStrat",
                                            TAData$InterraterStrat[p],
                                            "-CaseDominantStrat",
                                            TAData$caseDominantStrat[p],
                                            "-usingCaseDominantStrat",
                                            TAData$usingCaseDominantStrat[p],
                                            "-matchingIndividualDominantStrat",
                                            TAData$matchingIndividualDominantStrat[p],
                                            "-",
                                            TAData$Condition[p],
                                            "-Correct",
                                            TAData$Correct[p],
                                            sep="")
  
  
  
}

colnames(infoSeekingMatrixTA) <- c(1:29)
infoSeekingMatrixTA$Condition <- TAData$Condition
infoSeekingMatrixTA$ID <- TAData$ID
infoSeekingMatrixTA$Correct <- TAData$Correct
infoSeekingMatrixTA$Strat <- TAData$InterraterStrat
infoSeekingMatrixTA$caseDominantStrat <- TAData$caseDominantStrat
infoSeekingMatrixTA$usingCaseDominantStrat <- TAData$usingCaseDominantStrat
infoSeekingMatrixTA$matchingIndividualDominantStrat <- TAData$matchingIndividualDominantStrat

distancesTA <- infoSeekingMatrixTA[,1:29] %>% proxy::dist(method = "Dice") %>% as.matrix()
colnames(distancesTA) <- rownames(distancesTA)

```

## Introduction {.unnumbered}

In the previous study, we presented results from an online vignette study where we investigated confidence and information seeking within evolving diagnostic decisions. Overall, we found that students provided confidence judgements that were well-calibrated with their objective accuracy. We found that higher information seeking was associated with larger increases in confidence over the course of diagnostic decisions. However, seeking more information was not associated with accuracy. Rather, accuracy was associated with selectivity in information seeking, such that certain information was beneficial to students regardless of the patient's condition. We also found that medical students tended to broaden the range of differentials they were considering with more information, as they rarely removed differentials from their consideration. However, students who considered more differentials early on tended to seek more information and increase their confidence to a greater extent. Given that we find that certain information seeking patterns were associated with accuracy, and that students were overall calibrated, we now aim to better understand how students approach the task in terms of their decision making strategies. As the previous study was conducted online, we are not able to ascertain why students sought certain information or why they considered certain differentials (as likely or unlikely). To address this, we present in this chapter a study using a modified version of the previous study's vignette methodology where students verbalise their thought process as they were performing their diagnoses. 

\
In this mixed-methods study, we aim to gain insight on the types of reasoning strategies used by medical students and how these strategies influence both their information seeking patterns and changes in confidence over the course of their diagnostic decisions. We also investigate why reasoning strategies may vary on a case-by-case basis. We utilise a very similar experimental procedure to our previous study (using the same patient vignettes), but rather than explicitly asking students to report the differentials they were considering, we instead prompted students to think out loud as they were performing the diagnostic task. Everything said by participants was audio-recorded, transcribed and then coded for both quantitative and qualitative analysis. We aim to use this method to derive a richer understanding of the diagnostic process as it pertains to medical students' evolving thought processes. 
\

In our previous study, we observed a general tendency for participants to broaden the set of differentials they considered as they received more information. This was reflected in the average number of differentials reported by the end of the case being higher than the average number of initial differentials (based on the patient history). This effect was driven by participants (74 out of 85 participants, 87%) never reporting fewer differentials at the Testing stage compared to the Patient History stage. This is a surprising result, as we may have expected participants to use a 'process of elimination', which would manifest in decreasing the number of differentials considered as participants receive more information. What this speaks to however is a general reticence to remove differentials from consideration. One of our focuses for this study then is to replicate this finding by examining if students' thought processes reflect this tendency to focus on broadening rather than narrowing differentials being considered. This is to ensure that the finding in our previous study is not merely a quirk of our experimental interface and design, as it is possible that participants are not sufficiently encouraged to disregard differentials within our online paradigm. Replicating this finding would potentially reveal this general reticence to remove differentials as a potential driver of diagnostic uncertainty.  
\

Another focus of this study is to understand how differences in information seeking patterns arise. One possibility is that these differences stem from reasoning strategies adopted when making diagnoses. Clinical reasoning is a key skill that is taught, either explicitly or implicitly, within medical education. Medical decisions are frequently made in uncertainty, with clinical reasoning taught as a skill to navigate this uncertainty (with perhaps an intention to reduced the uncertainty perceived by clinicians). However, clinical reasoning has a broad remit and covers multiple different approaches to making medical decisions. Doctors may have different considerations when diagnosing a patient, and may draw on different approaches to making medical decisions accordingly. As we noted in the design of our previous study, doctors may have consider what conditions are likely for a patient and what conditions are too severe to not miss. A doctor's reasoning strategy may then reflect this dichotomy, such that the focus is either on determining what is most likely or being thorough such as to consider all possible diagnoses. Diagnostic decisions have traditionally been thought of as ‘ideal’ when using the hypothetico-deductive process (Kuipers & Kassirer, 1984), whereby hypotheses are initially formulated based on patient symptoms and established criteria for diagnoses. Further information is then gathered to test these hypotheses (Higgs et al., 2008) or eliminate others. One can think of this approach as akin to a 'process of elimination': by starting broad, clinicians then seek information to reduce the potential set of diagnoses to a more manageable set. This theory, that hypothetico-deductive processes are the gold standard for diagnostic decisions, was challenged by the results of Coderre et al. (2003). The authors found that reasoning strategies differed between novice and expert clinicians and that, crucially, a pattern recognition approach (rather than hypothethico-deductive) was associated with higher diagnostic accuracy. A pattern recognition approach would involve considering fewer diagnostic hypotheses and instead matching the symptoms to prototypical cases of a patient condition. Hypothetico-deductive reasoning represents a more structured, systematic approach to make diagnoses, in which clinicians maintain a more open mind to different diagnostic possibilities, whilst pattern recognition is more directed and driven by intuition. The difference between these approaches can be related to past work that has connected medical decision making to the dual-system model of thought (Kahneman, 2011). System 1 thinking represents an automatic, intuitive mode for making decisions (akin to pattern recognition), whilst System 2 represents a more deliberative, rational mode for decision making (with hypothethico-deductive reasoning as an example). 

\
In their paper, Coderre et al. (2003) asked novice and expert clinicians to provide diagnoses based on patient vignettes and asked them to verbalise their thought process during the diagnostic process. Using these verbalisations, the researchers categorised the clinician's reasoning strategy on each case. The researchers defined three reasoning strategies as follows (paraphrased from their paper):

* *Hypothetico-deductive (HD) strategy*: prior to selecting the most likely diagnosis, the clinician analysed, one by one, each alternative diagnosis. 

* *Scheme-inductive (SI) strategy*: This strategy consisted of key predetermined
propositions that linked categories and thus provided evidence for chunking (i.e. scheme use). These propositions were presented as structuring diagnoses by different pathophysiological systems/processes (e.g. Small Bowel vs. Large Bowel, Gastrointestinal vs. non-GI causes).

* *Pattern Recognition (PR) strategy*: The clinician directly reached a single
diagnosis with only perfunctory attention to alternative diagnoses.

These reasoning strategies may have been explicitly taught during a clinician's medical education or implicitly developed with experience. With these differing strategies, we can consider whether there are normatively 'better' strategies to use in certain clinical situations. As highlighted with the previous set of conflicting literature, there is currently not a consensus within medicine as to which strategy is ideal for diagnostic accuracy. In their study, Coderre et al. found that novice clinicians tended to adopt a HD strategy more often, whilst experts tended to use a PR strategy more. In addition, using a PR strategy was associated with higher diagnostic accuracy, which in turn was used to explain why experts were more accurate than novices. In addition to explaining differences in accuracy, these strategies point to different approaches in how diagnostic differentials are generated and considered. A PR strategy, by definition, considers a clinician as seeking to identify the correct condition whilst SI and HD strategies reflect a more thorough, systematic consideration of all possible differentials. The work of Coderre et al. did not reveal whether these strategies result in differences in information seeking and confidence. We would expect that with a more thorough SI or HD reasoning strategy, participants would seek more information in order to consider a larger number of diagnostic possibilities. By contrast, we would expect that a PR strategy would result in selective (but less) information seeking in order to gather evidence in support of the single diagnosis being considered. By better understanding how different reasoning strategies manifest in differences to diagnostic decisions, we aim to inform future work within medical education and cognitive intervention design to consider when each strategy is appropriate and prompt clinicians accordingly to follow that strategy's decision process. This also can shift our understand of clinical reasoning as having a 'one size fits all' approach where one method for making decisions is 'ideal' for all possible medical decisions.  
\

In order to pick apart these different reasoning strategies within our vignette-based paradigm, we adopted a think-aloud methodology similar to Coderre et al. (2003), whereby participants verbalise their thought process as they are doing the vignette-based diagnosis task. Think-aloud methodologies are useful for directly accessing ongoing thought processes during decisions (van Someren, Barnard & Sandberg, 1994). The use of thinking aloud (or 'verbal protocols') in research is useful for being able to access the information attended to participants in short term memory (Payne, 1994) and can be treated as the ongoing behavioural state of a participant’s knowledge (Newell & Simon, 1972). Think-aloud protocols have historically been used to study problem solving, particularly for comparing how novices and experts solve problems such as finding the best move in chess (de Groot, 1946, Bilalić, McLeod & Gobet, 2008). Diagnosis is a decisional process that develops over time and allowing participants to think aloud reflects this by providing a time-ordered sequence of how thought processes develop (Payne, 1994). This is especially well-suited to our task where the information available to participants is controlled at discrete time points, allowing us to investigate how diagnostic thinking develops with more information. We also are able to connect the verbalisations in our task to the exact information participants received to prompt that thought. As mentioned, a think-aloud methodology has previously been used to study the differences between novice and expert clinicians during diagnostic reasoning (Coderre et al., 2003). We build on the work of Coderre et al. here to further investigate how reasoning strategies contribute to differences in both accuracy and confidence, as well as understanding why certain cases result in differing strategies. 
\

In order to bolster our findings from this study, we aim to use the reasoning strategies determined from this study to reanalyse the cases in Study 2. Whilst we record information seeking, confidence and differential/hypothesis generation behaviour during the previous study, we do not have an understanding of how participants are approaching each case from a reasoning strategy perspective. One way in which we can to some extent infer this is via the information seeking patterns that participants adopt. If our hypothesis is correct that reasoning strategies result in different patterns of information seeking, we should then we able to predict what strategy a participant is using solely from their information seeking. If we can also establish that these predictions are reliable, we can then study the properties of these reasoning strategies with the larger sample size afforded to us in Study 2. This work would then improve our understanding of how these reasoning strategies not only affect diagnostic accuracy, but how they contribute to information seeking, confidence and differential generation. 

### Research Questions {.unnumbered}

In this study, we investigate the following research questions:

* What is the thought process of students as they performing our diagnosis task? Do students report ruling out differentials as they seek information on patient during diagnoses?
* Can we define different reasoning strategies based on the think-aloud utterances of medical students?
* If so, what reasoning strategies are medical students using when making diagnoses and weighing up differentials?
* How do differences in reasoning strategy manifest in terms of information seeking, both in terms of the quality and quantity of information sought?
* Are differences in reasoning strategy related to the individual or are they dependent on the case at hand? Do better performing medical students utilise specific reasoning strategies?
* What considerations do medical students report having whilst they are making diagnoses? And how these differ from how medical students reflect on their thought process after performing the task?

## Methods {.unnumbered}

### Participants {.unnumbered}

16 participants were recruited for this study. Participants were 5th or 6th year medical students at the University of Oxford (including 2nd year Graduate Entry Medical students) recruited via physical posters at Oxford's John Radcliffe Hospital and via a mailing list for students managed by the Medical Sciences Division at the University of Oxford. The study was conducted onsite at the John Radcliffe hospital. Participants were recruited between July 5th 2023 and December 1st 2023. This study was reviewed and granted ethical approval as an amendment to our existing protocol to allow for audio recordings by the Oxford Medical Sciences Interdivisional Research Ethics Committee under reference R81158/RE004.

### Materials {.unnumbered}

The same set of vignettes and a similar computer interface to Study 2 was used for this study, with the exception that participants no longer explicitly recorded their differentials at the end of each information gathering stage. Instead, participants’ differentials were recorded in a more naturalistic way. Participants verbalised out loud  their thought process as they worked through each diagnostic case. The study was conducted onsite using a laptop, with actions on screen recorded on video and the audio of participants’ thinking aloud recorded via a microphone. Informed consent was obtained anonymously using an online electronic information sheet and consent form. Information, including experimental data and audio recordings, collected during the study were stored under anonymised IDs with no linkages to participants. Data was kept on a password-protected computer and hard drive.

### Procedure {.unnumbered}

The general procedure was very similar to that of Study 1, except that participants were given the following instructions at the start of the study:

*“Whilst you are doing the task, you will be asked to think aloud. This means that you verbalise what you are thinking about, especially how you interpret the information you receive and what conditions or diagnoses you are considering or are concerned about for each patient case. If you have nothing to say or nothing on your mind, there’s no need to say anything but do say whatever is on your mind once it pops up. If you are unsure about anything you see or do not know about what something means, you will not receive any help but verbalise when you are unsure about anything during the task. Please make sure that you speak clearly ‘to the room’.”*
\

The experimenter occasionally prompted participants with content-neutral probes: *“can you tell me what you are thinking?”* in cases of periods of long silence, and *“can you tell me more?”* when the participant said something vague that may warrant further detail. We emphasise that these are non-leading questions. The audio of the participants’ verbalisations was recorded and then transcribed. An initial transcript was generated using Microsoft Office’s transcription feature, but the transcript was checked and modified for accuracy by listening through the audio recordings again. The screen of the experimental interface was also recorded, such that the audio could be linked to specific actions within the task. The focus of this study is on verbal utterances rather than any non-verbal or inferential aspects of the participants’ qualitative data. Given that participants were encouraged to verbalise their considered differentials as they were performing the task, we did not show participants the screen where they explicitly listed the differentials they were considered. At the end of the experiment, the researcher administered a semi-structured interview to better understand what the participants feel their diagnostic reasoning approach tends to be. These questions are provided in the Appendices.
\

Aside from these differences, participants performed the same six patient vignettes (in a randomised order) from the first study using the same interface that allows them to seek information that they think is useful for that particular case.

### Data Analysis {.unnumbered}

Our data analysis process for this mixed-methods study is split into a few parts. We first describe the main quantitative variables and analysis for this study. We then detail our coding process for detecting reasoning strategies based on participants' think-aloud utterances, followed by quantitative analysis we perform based on these coded reasoning strategies. We then describe the qualitative analysis performed based on the recorded debrief interviews with participants. Finally, we detail the process by which we code for reasoning strategies in the previous study's (Study 2) dataset. 

#### Descriptive Quantitative Analysis {.unnumbered}

The variables defined for this study are similar to Study 2, as we utilised the same interface and vignettes. Specifically, the variables for confidence, subjective difficulty and information seeking are the same as in Study 2. We note some key differences however. Firstly, given that participants did not explicitly report the differentials they were considered at each information stage, we are not able to record the number of differentials in the same way at each stage. Secondly, we also define accuracy differently due to the lack of this differential reporting screen: 
\
* _Accuracy_: Each case is defined as correct if a differential that is considered correct (as per our marking criteria in the Appendices) is mentioned by the participant at some point during the case. 

We also code all utterances related to differential/hypothesis generation. We define instances of Differential Evaluation, which is a main code that comprises a number of subcodes that we apply to think-aloud utterances. These are defined as follows:

* **Differential Evaluation**: any time that the participant (each of the following is considered a separate subcode):
 + * _Differential Added:_ - Mentions a new condition that they are considering 
 + * _Differential Removed:_ - Rules out or eliminates a condition from consideration
 + * _Likelihood Increased:_ - Mention of increased likelihood of a previously mentioned condition, or that information seems to correspond with a condition
 + * _Likelihood Decreased:_ - Mention of decreased likelihood of a previously mentioned condition, or that information seems to contradict with a condition
\
Based on this, in lieu of the Number of Differentials variable from our study, we define a new variable to look at the number of instances in which participants evaluate or reevaluate the differentials they are considering:

* _Number of Differential Evaluations_: The number of instances of the above subcodes belonging to the main Differential Evaluation code. The number of such utterances are defined for each individual case. 

#### Coding of Reasoning Strategies {.unnumbered}

We aim to detect which reasoning strategies are used by students on each case. To code for reasoning strategies, we adopt a similar approach to Coderre et al. (2003). We define coding criteria that indicate three different diagnostic reasoning strategies: hypothetico-deductive reasoning, scheme-inductive reasoning and pattern recognition (Coderre et al.., 2003). These were defined as follows:

* **Hypothetico-Deductive Reasoning (HD)** - prior to selecting the most likely diagnosis, the participant analysed any alternative differentials one by one through something akin to a process of elimination.
* **Scheme Inductive Reasoning (SI)** - participant structures their diagnosis by pathophysiological systems or categories of conditions (e.g., infective vs cardiovascular causes) to determine root causes of patient symptoms rather than focusing on specific conditions.
* **Pattern Recognition (PR)** - participant considers only a single diagnosis with only perfunctory attention to the alternatives, or makes reference to pattern matching when using a prototypical condition to match its symptoms against the current observed symptoms for the patient (e.g., “these symptoms sound like X” or “this fits with a picture of Y”).
* **None** - cases are defined as not having a clear reasoning strategy if there are insufficient utterances to make an inference of strategy (as agreed by both coders).
\

We first code specific statements within each case that suggested one of these strategies, and then determined which strategy was most prevalent or influential for cases as a whole such that each case was categorised under one of these strategies. Coding of utterances and case-wise reasoning strategies were conducted with a second independent coder. For reasoning strategies, initial interrater reliability was low, with both coders agreeing on 58.3% of cases. Conflict resolution led to changes made to the coding criteria by prioritising strategies used early on in a case, as some participants were noted to utilise multiple strategies within a single case. For example, a participant may use a SI approach to focus on different pathophysiological systems and then adopt a PR approach to identify an appropriate diagnosis within a specific pathophysiological system. The coding criteria was also changed to allow cases to be coded as not having a clear strategy due to a lack of utterances. The coding criteria provided above are after these changes were made. Cases were then independently coded for a second time with these updated criteria. Both coders agreed on 78% of cases when coding for correctness, with conflicts resolved in consultation with a member of expert panel used to develop the vignettes (as mentioned in Study 2). 
\

We hypothesise not only that this think-aloud methodology can be used to detect different reasoning strategies but also that usage of these reasoning strategies will vary. There are two possible ways in which they vary: they may vary as depdending on the individual medical student and/or they may vary as a function of the specific case/patient condition being treated. For the former, reasoning strategy would be more related to the individual medical student, in that each student will have their own strategy that they tend to use regardless of the patient. For the latter, there would be properties of specific patient conditions that prompt usage of certain reasoning strategies. We investigate both of these competing theories within this study. To look at individual-level strategy, we ask participants about their diagnostic reasoning process during the debrief interview. This includes questions such as _"What's your general approach to making diagnoses?"_ and _"Do you tend to keep a broad set of differentials in mind?"_ (full list of questions available in the Appendices). Based on their responses, participants are each categorised as belonging to one of the three reasoning strategies. This is considered their 'subjective strategy'. To look at condition-level strategies, after classifying each case using independent coders, we find the most commonly used strategy for each condition. This is considered the condition's 'dominant strategy'. Once both of these are defined, we compare the strategies coded for each case against both the subjective and dominant strategies. By comparing the cases' reasoning strategies with both the subjective and dominant strategies, we can investigate whether it is the individual medical student or the patient's medical condition that is responsible for the choice of reasoning strategy on a given case. 

\
We then compare our dependent variables (Accuracy, Confidence, Information Seeking, Differential Evaluations) as a function of reasoning strategy. As we compare observations on a case-by-case basis, observations are not independent, as student each record multiple cases and there are multiple cases for each condition. As such, we conduct generalised mixed effect modelling to analyse the effect of reasoning strategy on these variables. For Accuracy, we fit a binomial logistic model for correctness as a binary outcome variable. For the other variables, as we are using non-normally distributed continuous outcome variables, we compare model fit metrics (AIC and BIC) for generalised models using inverse Gaussian and Gamma distributions, reporting results for the better fitting model. 

#### Qualitative Thematic Analysis {.unnumbered}

The aim of this thematic analysis is to identify the reasoning strategies that medical students in this study reflectively report using in their medical practice, as well as understanding considerations made by students when making diagnoses. Similar to the think-aloud utterances by participants during the vignette task, we record and transcribe the responses given by participants during the debrief interviews (administered after the vignette task). With these interviews, we aimed to understand how participants report making diagnostic decisions, including how they seek information and weigh up differentials against each other. Based on these transcribed responses, we conducted a theory-driven semantic thematic analysis (as per definitions detailed by Braun and Clarke, 2006) to code utterances under specific categories. This kind of thematic analysis is suitable given that our qualitative data is from a structured interview with predefined research questions of interest, rather than a dataset with a looser structure. 

#### Recoding of Reasoning Strategies in Online Vignette Study {.unnumbered}

As laid out in the Introduction, we ask with this study is whether we are able to detect each participant's reasoning strategy on a case-by-case basis in Study 2. As Study 2 was an online study, we do not have access to participants' thought process via thinking aloud. However, we can ask whether the information seeking patterns in Study 3, in which we do have coded reasoning strategies based on think-aloud utterances, can be used to predict reasoning strategies in Study 2. In order for this to be possible, we assume that the reasoning strategies we have defined are differentiated from each other by their information seeking. To test this assumption, we establish whether the predictions of reasoning strategy are able to reliably correspond with the 'ground truth' labels of reasoning strategy via our independent interrater coding process. If these predictions of reasoning strategy are reliable within the think-aloud study, we can then apply the same method for predicting based on information seeking in the online study. If they are indeed reliable, we can determine, given medical students' reasoning strategies, whether these strategies are associated with differences in information seeking and confidence (with the increased robustness of the online study's higher sample size). In order to apply reasoning strategies to the data from Study 2, we train a classifier using penalised multinomial logistic regression to classify cases as HD, PR or SI using the cases from the think aloud study (with Leave One Out Cross Validation). As mentioned, some cases are not coded with a reasoning strategy if there are insufficient utterances to infer a clear strategy, and these case are excluded from this training. The input parameters for the classifier are the 29 pieces of information as binary predictors (similar to the approach depicted in Figure 8 of the previous study) and the cases’ condition. In other words, the cases from the think-aloud study make up the training data for the classifier whilst the cases from the larger online study make up the test dataset. The classifier was implemented using R’s caret package with the train() and glmnet() functions. The testing data is then labelled with predicted strategies using R’s predict function. We report accuracy values for the classifier when applied to think-aloud study in terms of how well the classifier is able to reproduce the coded reasoning strategies. 
\

As well as predicting reasoning strategy on a case-by-case basis to determine differences in information seeking, confidence and differential generation, we also look at differences across cases with a given reasoning strategy. As we describe earlier, we use the think-aloud to determine the dominant strategy for a given case by observing which reasoning strategy was used by a majority of participants. We then look across cases for a given dominant strategy, how our key dependent variables are affected by the type of case. We group cases in Study 2 by the dominant strategy used in Study 3 for each condition. For example, if a majority of participants use a HD strategy in the think-aloud study for the UC case, we then consider UC to be HD-dominant.

## Results {.unnumbered}

### Descriptive Quantiative Results {.unnumbered}

#### Overall Performance and Calibration {.unnumbered}

```{r casewiseStatsTable, include=TRUE, echo=FALSE}

caseBreakdown <- TAData %>%
  group_by(Condition) %>%
  dplyr::summarise(Accuracy = round(mean(Correct),2),
                   `Final Confidence` = round((mean(finalConfidence)/100),2),
                    Difficulty = round(mean(subjectiveDifficulty,na.rm=T),1),
                   `Information Seeking` = round(mean(proportionOfInfo),2))

colnames(caseBreakdown)[1] <- "Case"

knitr::kable(caseBreakdown) %>% 
  kableExtra::kable_styling(latex_options="HOLD_position")
```
_Table 1: Table showing, by case, from left to right, average values across participants for Accuracy (the proportion of participants who mentioned a correct differential during the case), Final Confidence (reported at the Testing stage), Difficulty (as rated by participants at the end of the case on a scale of 1-10) and Information Seeking (the proportion of available information sought)._

\
Participants varied in how much they spoke during the study, uttering 1038-7730 words (M = 4194) across the scenarios. Part of this range is driven by some participants repeating information they see during the task, but participants also varied in terms of how much they externalised their thought process. 

```{r calibrationtest, include=TRUE, echo=FALSE}

calibrationtest <- lmerTest::lmer(finalConfidence ~ Correct + (1|ID),data=TAData)
calibrationtest <- summary(calibrationtest)

```
\
When looking at accuracy (the proportion of cases where a correct differential was mentioned by the participant), accuracy was `r round(mean(caseBreakdown$Accuracy),2)` across all cases. This varied by condition, as can be seen above in Table 1. Similar to the previous study, we ask whether participants provided confidence judgements that were, on the whole, calibration to their objective accuracy. In order to investigate this for this study, we compare the final confidence reported by participants on cases when they were objective correct and when they were objectively incorrect. Participants would be considered calibrated if we found evidence of higher confidence when correct. Participants did indeed report higher confidence when correct (M = `r round(mean(TAData[TAData$Correct==1,]$finalConfidence),2)`, SD = `r round(sd(TAData[TAData$Correct==1,]$finalConfidence),2)`) compared to when they were incorrect (M = `r round(mean(TAData[TAData$Correct==0,]$finalConfidence),2)`, SD = `r round(sd(TAData[TAData$Correct==0,]$finalConfidence),2)`). Given that the samples from each of these groups are not independent, we test for a difference between these groups using a mixed effects model that predicts final confidence using the correctness of the case as a fixed effect and the individual participant as a random effect. We find evidence that the case correctness was predictive of final confidence ($\beta$ = `r round(calibrationtest$coefficients[2],2)`, SE = `r round(calibrationtest$coefficients[,"Std. Error"][2],2)`, t = `r round(calibrationtest$coefficients[,"t value"][2],2)`, p = `r round(calibrationtest$coefficients[,"Pr(>|t|)"][2],2)`), indicating that participants provided confidence judgements that were well calibrated to their objective accuracy. 

#### Differential Evaluations {.unnumbered}

```{r diffStatsTable, include=TRUE, echo=FALSE}

diffBreakdown <- TAData %>%
  group_by(Condition) %>%
  dplyr::summarise(`Differential Added` = round(mean(DE.DAs),2),
                  `Differential Removed` = round(mean(DE.DRs),2),
                   `Increased Likelihood` = round(mean(DE.IL),2),
                   `Decreased Likelihood` = round(mean(DE.DL),2),
                  `Total Differential Evaluations` = round(mean(DEs),2))

colnames(diffBreakdown)[1] <- "Case"

diffBreakdownShort <- diffBreakdown
colnames(diffBreakdownShort) <- c("Case","DA","DR","IL","DL","TDE")

knitr::kable(diffBreakdown) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 2: Descriptive Statistics for subcodes within the Differential Evaluation main code as detailed above in the Data Analysis section. Shown above are mean values for the number of instances/utterances for each of the following subcodes (from left to right): a new differential being considered, a differential being removed from consideration, a differential being seen as more likely given a piece of information, a differential being seen as less likely given a piece of information, the average total of these subcodes. _

```{r diffbars, include=FALSE, message=FALSE, echo=FALSE, warning=FALSE}

pptData <- TAData %>%
  group_by(ID) %>%
  dplyr::summarise(meanDiffsAdded = mean(DE.DAs),
                   meanDiffsRemoved = mean(DE.DRs),
                   meanIncreasedLikelihoods = mean(DE.IL),
                   meanDecreasedLikelihoods = mean(DE.DL))

colnames(pptData) <- c("ID","meanDiffsAdded","meanDiffsRemoved","meanDecreasedLikelihoods","meanIncreasedLikelihoods")

ttest1 <- t.test(pptData$meanDiffsAdded,pptData$meanDiffsRemoved,paired=T)
ttest2 <- t.test(pptData$meanIncreasedLikelihoods,pptData$meanDecreasedLikelihoods,paired=T)

```
\
For utterances coded as Differential Evaluations, participants on average made `r round(mean(diffBreakdownShort$TDE),2)` such utterances per case. The mean number of Differential Evaluations was relatively constant by condition except for the AD case, for which we observed a higher amount of Differential Evaluations (see table 2 above). As previously mentioned, Differential Evaluations can be further categorised into one of four subcodes: Differential Added, Differential Removed, Likelihood Increased and Likelihood Decreased. As found in the previous study, there is a general reticence to disregard differentials completely. Participants expressed significantly more statements adding differentials (M = `r round(mean(TAData$DE.DAs),2)`, SD = `r round(sd(TAData$DE.DAs),2)`) than removing differentials (M = `r round(mean(TAData$DE.DRs),2)`, SD = `r round(sd(TAData$DE.DRs),2)`) (t(`r round(ttest1$parameter)`) = `r round(ttest1$statistic,2)`, MDiff = `r round(ttest1$estimate,2)`, p < .001). Participants expressed more statements of decreasing likelihoods (M = `r round(mean(TAData$DE.DL),2)`, SD = `r round(sd(TAData$DE.DL),2)`) rather than increasing likelihoods (M = `r round(mean(TAData$DE.IL),2)`, SD = `r round(sd(TAData$DE.IL),2)`) but we did not find evidence of a significant difference (t(`r round(ttest2$parameter)`) = `r round(ttest2$statistic,2)`, MDiff = `r round(ttest2$estimate,2)`, p = `r round(ttest2$p.value,2)`). 

\newpage

```{r diffbarsplot, include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,out.width='100%', fig.align='center', fig.height=6}

Subcode <- c("DiffsAdded","DiffsRemoved")

means <- c(mean(pptData$meanDiffsAdded),
                mean(pptData$meanDiffsRemoved))
sds <- c(sd(pptData$meanDiffsAdded)/sqrt(nrow(pptData)),
              sd(pptData$meanDiffsRemoved)/sqrt(nrow(pptData)))

temp <-data.frame(Subcode, means,sds)

p <- ggplot(temp) +
  geom_bar(aes(x = Subcode, y = means,fill=Subcode),stat="identity",alpha=0.7) +
  scale_fill_manual(values=c("#69b3a2", "#404080")) +
  geom_errorbar(aes(x = Subcode,ymin=means-sds, ymax=means+sds), width=.2, position=position_dodge(0.05),color="orange") +
  labs(x="Strategy",y="Mean") +
  theme_classic() +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        line = element_blank())

Subcode <- c("Likelihood-","Likelihood+")

means <- c(mean(pptData$meanDecreasedLikelihoods),
                mean(pptData$meanIncreasedLikelihoods))
sds <- c(sd(pptData$meanDecreasedLikelihoods)/sqrt(nrow(pptData)),
              sd(pptData$meanIncreasedLikelihoods)/sqrt(nrow(pptData)))

temp <-data.frame(Subcode, means,sds)

p2 <- ggplot(temp) +
  geom_bar(aes(x = Subcode, y = means,fill=Subcode),stat="identity",alpha=0.7) +
  scale_fill_manual(values=c("#69b3e9", "#620a1a")) +
  geom_errorbar(aes(x = Subcode,ymin=means-sds, ymax=means+sds), width=.2, position=position_dodge(0.05),color="orange") +
  labs(x="Strategy",y="Mean") +
  theme_classic() +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        line = element_blank())

cow <- plot_grid(p,p2,ncol=2, align = "v", axis="1", labels=c('A','B'))
print(cow) #view the multi-panel figure  

```
_Figure 1: Bar graphs comparing incidences of each of the four subcodes within Differential Evaluations. We compare instances of differentials being added (green) and removed (purple) from consideration (Figure 1A) and compare instances of differentials decreasing (blue) and increasing (red) in likelihood (Figure 1B)._


### Reasoning Strategies {.unnumbered}

#### Incidence of Strategies {.unnumbered}

```{r stratppttable, warning=FALSE, message=FALSE, echo=FALSE}

stratppttable <- TAData %>%
  group_by(Condition, ID) %>%
  dplyr::summarise(Strategy = InterraterStrat) %>%
  pivot_wider(names_from = Condition, values_from = Strategy)

```

In Table 3 below, we show all `r nrow(TAData)` cases from the think-aloud strategy and the strategy coded to each after resolving all interrater conflicts. Of these cases, `r sum(stratppttable=="NONE")` were coded as not having a clear reasoning strategy due to both an insufficient amount of think-aloud utterances and no diagnostic differentials being mentioned. `r sum(stratppttable=="HD")` cases were coded as having a HD strategy, `r sum(stratppttable=="PR")` cases were assigned a PR strategy and `r sum(stratppttable=="SI")` cases were coded as SI. In Figure 2 below, we plot the proportion of cases for each patient condition that were categorised under each of the reasoning strategies. We note that the types of reasoning strategy used varies by condition (see Figure 2 above), with the MTB and TTP cases in particular exhibiting higher usage of PR than others, whilst HD was used by the majority of participants for the UC and AD cases in particular. In Table 4, we show examples of key quotes that resulted in coding of reasoning strategies. 

```{r stratppttableshow, warning=FALSE, message=FALSE, echo=FALSE}

knitr::kable(stratppttable) %>% 
  row_spec(0,bold=TRUE) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 3: Table that shows the strategy coded for each case by participant (rows) and by patient condition (column) after resolving conflicts between both independent coders. Anonymised participant IDs are used._

```{r stratbreakdown, include = TRUE, warning=FALSE, message=FALSE, echo=FALSE}

stratBreakdown <- TAData %>%
  group_by(Condition, InterraterStrat) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(m_N = mean(N)/16)

colnames(stratBreakdown) <- c("Condition","Strategy","m_N")

nCases <- nrow(TAData)
props <- c(nrow(TAData[TAData$InterraterStrat=="PR",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="HD",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="SI",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="NONE",])/nCases)

temp <- data.frame(c("Overall","Overall","Overall","Overall"),
                   c("PR","HD","SI","NONE"),
                   props)
colnames(temp) <- c("Condition","Strategy","m_N")

stratBreakdown <- rbind(stratBreakdown,temp)

stratBreakdown$Strategy <- factor(stratBreakdown$Strategy, levels = c('HD', 'PR', 'SI', 'NONE'))

p <- ggplot(stratBreakdown, aes(fill=Strategy, y=m_N, x=Condition)) + 
    geom_bar(position="dodge", stat="identity",alpha=0.7) +
    scale_fill_manual(values=c("#E69F00", "#56B4E9", "#009E73",  "grey")) +
  scale_x_discrete(limits = c("UC","GBS","TTP","AD","TA","MTB","Overall")) +
  labs(x="Case (Decreasing Accuracy)",y="Proportion of Participants") +
  theme_minimal() +
    theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        line = element_blank())

print(p)

```
_Figure 2: Proportion of participants who use each type of reasoning strategy for each condition/case, with the overall proportions across all cases shown by the rightmost bars. The strategies shown are: Hypothetico-Deductive (where multiple differentials are considered simultaneously, orange), Pattern Recognition (where a single differential is considered in turn, blue), Scheme-Inductive (where participants evaluate pathophysiological systems as causes of patients rather than specific conditions, green) and None (for cases where a clear differential is not mentioned or if there are not enough utterances to infer a clear strategy, grey)._

\
```{r strategyexamples, include = TRUE,warning=FALSE, message=FALSE, echo=FALSE}

stratexamples <- as.data.frame(read.csv("./assets/TAStrategiesExampleTable.csv",header=TRUE))

colnames(stratexamples) <- c("Participant (Case)","Quote","Coded Strategy","Interpretation")

knitr::kable(stratexamples, longtable = TRUE) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","repeat_header")) %>%
  column_spec(1, width = "6em") %>%
  column_spec(2, width = "15em") %>%
  column_spec(3, width = "4em") %>%
  column_spec(4, width = "9em")

```
_Table 4: Table that shows examples of cases that were coded for a particular reasoning strategy and key quotes that suggest the coded reasoning strategy. We provide the quotes from each case, as well as how we interpret the quotes in line with the particular reasoning strategy._

#### Reasoning Strategies' Effect on Dependent Variables {.unnumbered}

```{r stratstats, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

strattable <- TAData %>%
  group_by(InterraterStrat) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(N = mean(N),
                    Accuracy = round(mean(Correct),2),
                   `Differential Evaluations` = round(mean(DEs),2), 
                   `Information Seeking` = round(mean(proportionOfInfo),2),
                   `Information Value` = round(mean(infoValue),2),
                   `Confidence Change` = round(mean(confidenceChange)/100,2))

colnames(strattable)[1] <- c("Strategy")

strattable <- strattable[order(strattable$N, decreasing = TRUE), , drop = FALSE]

TADataNoNons <- TAData[TAData$InterraterStrat!="NONE",]

# ACCURACY - BINARY VARIABLE
accTest <- lme4::glmer(Correct ~ InterraterStrat + (1|Condition) + (1|ID),family=binomial(link = "logit"),data=TADataNoNons)
accTest <- joint_tests(accTest)

# DIFFERENTIAL EVALUATIONS - DISCRETE (COUNTS)
# inverse binomial
DETest <- lme4::glmer(DEs ~ InterraterStrat + (1|Condition) + (1|ID),family=poisson(link = "log"),data=TADataNoNons[TADataNoNons$DEs>0,])
DETest <- joint_tests(DETest)

# INFORMATION SEEKING AND CONFIDENCE - CONTINUOUS

infoTest <- lme4::glmer(proportionOfInfo ~ InterraterStrat + (1|Condition) + (1|ID),family=Gamma(link = "inverse"),data=TADataNoNons)
infoTest2 <- lme4::glmer(proportionOfInfo ~ InterraterStrat + (1|Condition) + (1|ID),family=inverse.gaussian(link = "1/mu^2"),data=TADataNoNons)
infoAnova <- anova(infoTest,infoTest2)
# InfoTest2 better fit
infoTest2 <- joint_tests(infoTest2)

valTest <- lme4::glmer(infoValue ~ InterraterStrat + (1|Condition) + (1|ID),family=Gamma(link = "inverse"),data=TADataNoNons)
valTest2 <- lme4::glmer(infoValue ~ InterraterStrat + (1|Condition) + (1|ID),family=inverse.gaussian(link = "1/mu^2"),data=TADataNoNons)
valAnova <- anova(valTest,valTest2)
# valTest better fit
valTest <- joint_tests(valTest)

# Can't have negative values for confidence

#conTest <- lme4::glmer(confidenceChange ~ InterraterStrat + (1|Condition) + (1|ID),family=Gamma(link = "inverse"),data=TADataNoNons)
#conTest2 <- lme4::glmer(confidenceChange ~ InterraterStrat + (1|Condition) + (1|ID),family=inverse.gaussian(link = "1/mu^2"),data=TADataNoNons)
#conAnova <- anova(conTest,conTest2)

```

```{r stratstable, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

knitr::kable(strattable) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

colnames(strattable) <- c("Strategy","N","Accuracy","DiffEval", "InfoSeeking","ConfidenceChange")

```
_Table 5: Mean values for dependent variables broken down by the reasoning strategy coded after resolving conflicts between the two independent coders. From left to right, Accuracy refers to the proportion of cases where a correct differential was mentioned. Differential Evaluations refers to the number of coded utterances under one of the subcodes (Differential Added, Differential Removed, Increased Likelihood, Decreased Likelihood). Information Seeking refers to the proportion of available information sought across cases. Confidence Change refers to difference between initial confidence and final confidence._
\

Next we look at our coding of reasoning strategies at a case level (see Table 5 above). Accuracy was higher for cases coded as Hypothetico-Deductive (`r strattable[strattable$Strategy=="HD",]$Accuracy`) compared to both Pattern Recognition cases (`r strattable[strattable$Strategy=="PR",]$Accuracy`) and Scheme Inductive (`r strattable[strattable$Strategy=="SI",]$Accuracy`), although the effect of reasoning strategy on accuracy was not statistically significant (F = `r round(accTest$F.ratio,2)`, p = `r round(accTest$p.value,2)`). On cases with a SI strategy, participants gained more confidence over the case (`r strattable[strattable$Strategy=="SI",]$ConfidenceChange`) when compared to PR (`r strattable[strattable$Strategy=="PR",]$ConfidenceChange`) and HD cases (`r strattable[strattable$Strategy=="HD",]$ConfidenceChange`). Participants evaluated differentials more often during HD cases (`r strattable[strattable$Strategy=="HD",]$DiffEval`) when compared to other strategies, and we find evidence of an effect of reasoning strategy on the number of differential evaluations (F = `r round(DETest$F.ratio,2)`, p = `r round(DETest$p.value,3)`). We do not find evidence for a significant effect of reasoning strategy on information seeking or informational value (ps > .1).

#### Dominant Reasoning Strategies {.unnumbered}

```{r dominantstrats, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

TAData$caseDominantStrat <- ifelse(TAData$Condition %in% c("AD","UC","GBS"),"HD","PR")
TAData$matchingCaseDominantStrat <- ifelse(TAData$caseDominantStrat==TAData$InterraterStrat,1,0)

dominantStrat <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(caseDominantStrat,matchingCaseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2),                     
              `Differential Evaluations` = round(mean(DEs),2), 
              `Information Seeking` = round(mean(proportionOfInfo),2),
              `Confidence Change` = round(mean(confidenceChange)/100,2))

model <- summary(lme4::glmer(Correct ~ matchingCaseDominantStrat + (1|Condition) + (1|ID), data = TAData[TAData$InterraterStrat!="NONE",], family = binomial(link = "logit")))

dominantStratCompare <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(matchingCaseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2)) 

```

We aim now to establish if participants are more accurate when using each condition's dominant strategy. This is based on the assumption that each medical condition has an 'optimal' reasoning strategy that should be used to maximise accuracy. We first categorise each of the 6 cases as having a ‘dominant’ reasoning strategy based on which was utilised the most across participants. Through this process, we categorise three conditions as HD (AD, GBS, UC), three conditions as PR (MTB, TTP, TA, we note that there was an equal number of PR and SI cases for TA condition, but we use PR as its dominant strategy to easily compare HD and PR directly). HD was assigned to `r round(stratBreakdown[stratBreakdown$Condition=="AD"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of AD cases, `r round(stratBreakdown[stratBreakdown$Condition=="GBS"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of GBS cases, and `r round(stratBreakdown[stratBreakdown$Condition=="UC"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of UC cases. PR was assigned to `r round(stratBreakdown[stratBreakdown$Condition=="MTB"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of MTB cases, `r round(stratBreakdown[stratBreakdown$Condition=="TTP"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of TTP cases, and `r round(stratBreakdown[stratBreakdown$Condition=="TA"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of TA cases. Overall, participants matched the dominant strategy on `r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$N` cases (`r round(dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$N/sum(dominantStratCompare$N)*100,1)`% of cases, excluding those cases without a clear reasoning strategy). Accuracy was found to be higher for cases when participants matched the condition's dominant strategy (`r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$Accuracy`) compared to when they did not (`r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==0,]$Accuracy`). However, this difference was not found to be significant via a mixed effects logistic regression (on accuracy as a binary outcome measure with both condition and participant as random effects) ($\beta$ = `r round(model$coefficients[2],2)`, SE = `r round(model$coefficients[,"Std. Error"][2],2)` t = `r abs(round(model$coefficients[,"z value"][2],2))`, p = `r round(model$coefficients[,"Pr(>|z|)"][2],2)`). 

```{r dominantstratstable, include = TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(dominantStrat)[1] <- "Dominant Strategy"
colnames(dominantStrat)[2] <- "Matching Dominant Strategy"

dominantStrat[,2] <- as.character(dominantStrat[,2])
dominantStrat[1,2] <- "No"
dominantStrat[2,2] <- "Yes"
dominantStrat[3,2] <- "No"
dominantStrat[4,2] <- "Yes"

knitr::kable(dominantStrat) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))


```
_Table 6: Table showing average accuracy values by cases where the participants used or did not use the dominant reasoning strategy for that case. Dominant strategies are decided based on which of the reasoning strategies was utilised by the majority of participants in the think-aloud study. Cases without a coded reasoning strategy are excluded from this table. The first column refers to the dominant strategy for that condition, whilst the second column refers to whether the cases' coded strategy matches the condition's dominant strategy._

#### Subjective Reasoning Strategies {.unnumbered}

```{r subjstrategies, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

subjstrategies <- as.data.frame(read.csv("./assets/TASubjectiveCodes.csv",header=TRUE))

colnames(subjstrategies)[1] <- "ID"

colnames(subjstrategies)[3] <- "SubjectiveStrategy"

TAData <- merge(TAData, subjstrategies[, c("ID","SubjectiveStrategy")], by = "ID", all.x = TRUE)

TAData$SubjectiveStrategyMatched <- ifelse(TAData$SubjectiveStrategy==TAData$InterraterStrat,1,0)

subjectiveMatchedTable <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(SubjectiveStrategyMatched) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2),                     
              `Differential Evaluations` = round(mean(DEs),2), 
              `Information Seeking` = round(mean(proportionOfInfo),2),
              `Confidence Change` = round(mean(confidenceChange)/100,2))


model <- summary(lme4::glmer(Correct ~ SubjectiveStrategyMatched + (1|Condition) + (1|ID), data = TAData[TAData$InterraterStrat!="NONE",], family = binomial(link = "logit")))

```

In addition to reasoning strategies being coded based on the participants' think-aloud utterances, we also asked participants about their diagnostic process during the debrief interviews that can be used to infer the reasoning strategies participants think they use in their regular medical practice. We use this to determine if participants are more accurate when using their subjectively preferred reasoning strategy. In Table 7 below, we categorise participants based on their subjective reflection of their diagnostic process. Through this process, we categorise `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="HD",])` participants under a HD reasoning strategy, `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="PR",])` participants as PR and `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="SI",])` participants as SI. Given these categorisations of reasoning strategy based on subjective reflection by participants, we compare these participant-level strategies to the case-level strategies assigned by our independent coders. We find that there are `r sum(TAData$SubjectiveStrategyMatched)` cases (`r round((sum(TAData$SubjectiveStrategyMatched)/nrow(TAData[TAData$InterraterStrat!="NONE",]))*100,2)`%, excluding cases without a coded reasoning strategy) where participants match the reasoning strategy during the case to their subjectively defined strategy that they tend to use for diagnostic decisions. Accuracy was found to be higher for cases when participants matched their subjective strategy (`r subjectiveMatchedTable[subjectiveMatchedTable$SubjectiveStrategyMatched==1,]$Accuracy`) compared to when they did not (`r subjectiveMatchedTable[subjectiveMatchedTable$SubjectiveStrategyMatched==0,]$Accuracy`). However, this difference was not found to be significant via a mixed effects logistic regression (with accuracy as a binary outcome measure and both condition and participant as random effects) ($\beta$ = `r round(model$coefficients[2],2)`, SE = `r round(model$coefficients[,"Std. Error"][2],2)` t = `r abs(round(model$coefficients[,"z value"][2],2))`, p = `r round(model$coefficients[,"Pr(>|z|)"][2],2)`). 
\

```{r subjstrategiesmatched, include = TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(subjectiveMatchedTable)[1] <- "Matched to Subjective Strategy"

subjectiveMatchedTable[,1] <- as.character(subjectiveMatchedTable[,1])
subjectiveMatchedTable[1,1] <- "No"
subjectiveMatchedTable[2,1] <- "Yes"

knitr::kable(subjectiveMatchedTable) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 7: Dependent variables by cases where the reasoning strategy used (as categorised by the independent coders) matches the subjective strategy coded for that participant (as per responses to the debrief interview, see table 8 below)._

```{r subjstrategiestable, include=TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(subjstrategies) <- c("Participant","Full Quote","Coded Strategy","Condensate","Interpretation")

knitr::kable(subjstrategies, longtable = TRUE) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","repeat_header")) %>%
  column_spec(1, width = "5em") %>%
  column_spec(2, width = "13em") %>%
  column_spec(3, width = "4em") %>%
  column_spec(4, width = "6em") %>%
  column_spec(5, width = "6em") 

```
_Table 8: Categorisation of participants under one of three possible reasoning strategies based on their responses during the debrief interview. We capture here the subjective reasoning strategy for each participant based on how they reflect on how tend to make diagnostic decisions. In the second column are key highlighted quotes related to each of the participants' diagnostic decision making process. In the fourth column, we provide our summary of the quote and then in the fifth column, our interpretation of the quote that explains the choice of reasoning strategy for that participant._

\

```{r overallstratmatch, include=TRUE,warning=FALSE, message=FALSE, echo=FALSE}

overallMatchedTable <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(SubjectiveStrategyMatched,matchingCaseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2))   

model <- summary(lme4::glmer(Correct ~ SubjectiveStrategyMatched*matchingCaseDominantStrat + (1|Condition) + (1|ID), data = TAData[TAData$InterraterStrat!="NONE",], family = binomial(link = "logit")))

```

To combine the two previous sections, we consider whether participants are more accurate when in fact their reasoning strategy matches both the case's dominant strategy and the participant's subjectively preferred strategy. For briefness, we refer to this as the "fully matched strategy" going forward. Participants used a fully matched strategy on `r overallMatchedTable[overallMatchedTable$SubjectiveStrategyMatched==1&overallMatchedTable$matchingCaseDominantStrat==1,]$N` cases (`r round(overallMatchedTable[overallMatchedTable$SubjectiveStrategyMatched==1&overallMatchedTable$matchingCaseDominantStrat==1,]$N/sum(overallMatchedTable$N)*100,2)`%). On these cases, participants were more accurate (`r overallMatchedTable[overallMatchedTable$SubjectiveStrategyMatched==1&overallMatchedTable$matchingCaseDominantStrat==1,]$Accuracy`) than for cases when reasoning strategy matched neither the case's dominant strategy nor the individual's subjectively preferred strategy (`r overallMatchedTable[overallMatchedTable$SubjectiveStrategyMatched==0&overallMatchedTable$matchingCaseDominantStrat==0,]$Accuracy`). We do not find evidence of an interaction via a mixed effects logistic regression (with accuracy as a binary outcome measure and both condition and participant as random effects) ($\beta$ = `r round(model$coefficients[4],2)`, SE = `r round(model$coefficients[,"Std. Error"][4],2)` t = `r abs(round(model$coefficients[,"z value"][4],2))`, p = `r round(model$coefficients[,"Pr(>|z|)"][4],2)`). 

### Thematic Analysis from Debrief Questionnaire {.unnumbered}

In this section, we present key themes from the thematic analysis of participant responses to the debrief questionnaire. The questionnaire was designed to ask participants how they think they tend to make diagnostic decisions and what their main considerations are during the decisional process. We provide quotes from participants belonging to each of these themes. Participants are referred to by their anonymised identifiers. 

#### Avoidance of Anchoring {.unnumbered}

A key consideration, as mentioned by six participants, was the concept of anchor bias. This was explained by one of these participants as follows:
\

_"I'm quite aware that there's, I've tried to remember what it's called, I think it's called anchor bias where you have, you can leap onto one thing early on, and then you want other things to fit that. I think we are all vulnerable to it to an extent. And we will look for things that support our initial idea, but I try and keep an open mind." (k5376h)_
\

These participants showed awareness of this phenomenon, whereby clinicians may focus too early on a particular diagnosis and then seek information to confirm this existing belief (a form of confirmation bias). This can then prevent participants from considerating alternative diagnoses early on in their decisional process. Given their awareness of this bias and its pitfalls, we can then infer that participants approach their diagnoses in such a way as to avoid this bias. Other participants cited this as a consideration of theirs when making diagnoses:
\

_"I'm quite rubbish, I often get fixated like 'I think this is this'’'...but I'm not that good at thinking, 'oh, what else could it be' into like, 'I've got something that's proved to me it's not.'" (4khzxs)_

_"I try to, but I think my brain can sometimes get stuck on an idea. And it's difficult to pull away from that." (gdq7tc)_

_"I think I probably do think about that the whole way through, which probably can be beneficial, but can also sometimes hold me back from looking at other options" (593ybw)_
\

One reason cited for such a bias to occur is that medical students are relatively early into their medical experience. As a result, they have not developed as much medical knowledge as experienced clinicians and may focus on diagnoses that they have more familiarity with. Medical students seem to also report making a conscious effort to keep an open mind with regards to alternative differentials:
\

_"But then I do think I, at this stage, I'm quite kind of biased towards what I know more about, if that makes sense. So, I think the things which I don't know about, I'm just hoping it’s not that." (3lkzjq)_

_"I think I try to keep an open mind perhaps because I'm just like, the student and I don't have as much knowledge, as someone who's been training for a long time." (dcjymb)_

_"My knowledge isn't broad enough...to remember all the differentials for everything." (rslkq8)_

\
This is important to note for three reasons. Firstly, medical students take their relative inexperience into account as a factor when making diagnoses. This could then mean that as medical students become intermediate/experienced clinicians, their decision making style may change to reflect their increased medical knowledge. Secondly, medical students may be more likely to express uncertainty if there are more diagnoses/conditions that they are unfamiliar with due to their lack of knowledge. Thirdly, the awareness shown for the relative inexperience of medical students indicates that students would be less likely to tend toward overconfidence, given this sense of 'humility' about what knowledge they have and do not have. Taken together, medical students are likely to approach medical decisions very differently from experienced clinicians mainly because they have different perceptions of their own medical knowledge. 

#### Standard Tendencies {.unnumbered}

Participants reflected a few general tendencies (or rules of thumb) when making diagnoses. Firstly, seven participants mentioned that they prioritised any serious/emergency differentials early on when making diagnoses, which would affect the urgency with which they would approach ruling these differentials out. This suggests that some focus would be taken away from determining likely diagnoses and instead ruling out more serious diagnoses that would require more immediate medical attention. This also indicates that the manner in which medical students approach diagnoses is dependent on the nature of the patient being treated and whether serious diagnoses are being considered. The need to consider serious diagnoses offers a potential reason for why participants do not simply utilise their preferred reasoning strategy on all cases, as some patients may display symptoms that prompt consideration of diagnoses that are harmful if missed. 
\

_"I do have an approach, the first (thing) I always want to think is if I miss something, is this patient gonna be bad? So, like, thinking about emergency stuff." (4khzxs)_

_"I think probably, especially as a medical student, we get taught to rule out red flag stuff...a lot of my thinking is like, what really worrying thing could this be that we need to rule out? And what tests do I need to rule it out?" (5lvg8j)_

_"If it's...an acute versus a non-acute thing, I think that would change the pace I approach it." (dcjymb)_

_"Like, if someone's coming in with a presentation that could be quite urgent and serious, then obviously you want to rule out like a stroke, you want to rule that out quite quickly." (gdq7tc)_

_"If I think something's remotely possible, that's really like say, so like for GBS, I'm even thinking about Cauda Equina syndrome, like, regardless of how high my index of suspicion for it is, even if it's pretty low. If it's an urgent diagnosis, I'll just do it anyway." (gs6zbl)_

_"I'm trying to rule out the most serious things." (rslkq8)_

_"And essentially if there’s any serious conditions, I make sure to rule those out...and then go from there. I probably should go through each one and weigh each one individually. Because that would avoid being as biased. But it’s not something I do as much as I should...The other big things, are there any of the red flag symptoms that are really important that should influence what I’m thinking? Like fevers especially, that sort of thing...So generally, acute situations are where I narrow a little bit." (y86m2n)_

\
Another tendency was for participants to report a form of progressive investigation that stems from the patient's history. In this sense, participants report a decision process that quite closely matches our experimental procedure of gradually seeking information based on patient's medical history to build up a picture of them. This illustrates the importance of a comprehensive medical history for the patient being available and how much it guides medical students' decisional process. We can also ascertain from this theme that the initial diagnostic differentials generated from the patient's history has a large influence on the subsequent diagnostic process: 
\

_"Definitely start like history...I think to go from there and like, kind of think about that in the context of the patient. Yeah, I feel I've definitely been taught in terms like that methodical, like do it in that order." (593ybw)_

_"I guess going through like a system of starting with the history and sort of gathering as much information as I can there and thinking already what I think might be happening. And then examining them and seeing if that sort of changed anything, but then sort of getting investigations." (5lvg8j)_

_"You can get a lot from the history. So I think sort of, I guess my general approach is like, take your history, and then from the history, have a little, it's not like, if you wrote it down, it'd be like a little bubble, like brainstorming thing as...the key big differentials I'm considering." (clhtyq)_

_"But if it was a patient...who had sort of not very clear symptoms, but wanted to be a bit more thorough, like take a history first and then looking at any test they've had, starting with like more basic tests like observations blood tests, and then and then, depending on the cause, or the symptoms, doing more invasive tests, perhaps." (dcjymb)_

_"Take a history, like detailed history, formulate my top differentials. And then basically, look at investigations and examinations to confirm or rule out these differentials." (l3jd8r)_ 

_"I think, start, think systematically. So start with a thorough history, asking kind of about what's happening currently, and then going through the kind of past medical history and focusing on that, asking what the patient thinks might be going on. And then focus on a thorough examination which sometimes for the interest of time is focused on the, the kind of symptom at hand, but you should do a kind of formal full checkthrough as well...see if there's anything that points you towards a diagnosis. I think it (the experiment) was set up in the way that I go about things in the way that you do the history first, you do the examination, you do the investigations." (ly9kzg)_

_"Just sort of work through from the most like, basic things like history and examination, least invasive tests, and then try to work up from there." (rslkq8)_

_"So probably getting history, I look at initial observations first...And then I look at ECG as well, then I want to get initial bloods being guided by what I think could be going on and then think about potential images." (ytpshg)_
\

Within this process of progressive investigation, six participants (including the quote above from participant ytpshg) noted that there are pieces of information or tests that they would seek for all patients (regardless of their condition) as part of a routine diagnostic approach. This indicates that some aspects of the diagnostic process are seen as fairly standardised by medical students:
\

_"I would always want to do like full blood count, VBG… Probably, as I said, I think like most people in the emergency department get a chest X ray" (4khzxs)_

_"With the examination, I think, normally, I would like...auscultating the heart and feeling the heart, abdomen, etc. These are things I think I would do in any patient, irrespective." (5lvg8j)_

_"And then for investigations...I'll take all the bloods, do an ECG, chest X Ray, just in case. Yeah, yeah. So I am a bit more like, on the side of caution." (d9b1qf)_

_"I think if you went to your senior and you said I'm really concerned about this patient, but I've not done an FBC, a U&E, an ECG, VBG...They'd be like, what are you on about? So there's a few that you would do anyway, that are largely non invasive, in terms of...higher degree investigations, very much depends on anatomically, what you're seeing, what your differential is." (k5376h)_

_"I wasn't sure (during the experiment) whether I should try and be very focused to the presentation at hand...because for example, when I was going through the examinations, in reality, I would do a full exam on someone, even if they presented with something that was very specific, like a very specific symptom just so that you can have a full kind of clerking assessment." (ly9kzg)_

#### Challenges of the Diagnostic Process {.unnumbered}

Participants cited a number of challenges related to our diagnostic task. These were related to ways in which our study did not emulate real-world aspects of the diagnostic process. Firstly, three participants noted that it was difficult to retain all of the information they needed during the task:
\

_"I kept thinking that I couldn't like hold onto all of the information." (4khzxs)_

_"I'm quite a visual person. So...reading on a screen is quite different to I don't know, actually having seen a patient, seeing the exam findings, or even looking at the scans myself. I feel like I find that easier to retain the information. Whereas when it's, I find it, it's kind of hard to take it in when it's like just written down" (593ybw)_

_"I think just thinking on the spot and coming up with the diagnosis quite quickly, it's quite hard remembering the management afterwards as well." (rslkq8)_
\

Four participants noted that, in their real medical practice, they would be consulting other doctors, frameworks or online resources, which made our task difficult given that these were not available to participants:
\

_"I'd find as much information as possible and to ask for help...I think I'm someone who looks up stuff a lot. Like I rely a lot on looking up things. And that gives me a lot of comfort. I feel like when I don't have those tools, yeah, I feel a bit shaky...And I often kind of just take my phone out and look at that, and even just glancing at them kind of helps me structure my thoughts. So, yeah, I would wish it would be kind of more, more of an organic process. But at the moment, I think I rely quite a lot on prompts and things like that, or guidelines, even if I don't read them thoroughly, I'd need kind of reminders, especially when I feel like there's so much that it could be and I lose myself a little bit in the possibilities." (3lkzjq)_

_"I guess in real life, I also have, like, Google. So at points where I forgot the disease, or like, what is the first line, I would have like checked before, before typing up my management plan...when I'm confused, I’m definitely going to approach the senior. So I wouldn't be the one making the diagnostic decision. So I think it's a bit harder to do it alone." (d9b1qf)_

_"When I'm not sure, I'll definitely running my my train of thought past my consultants." (l3jd8r)_

_"I think it's weird doing it in isolation. Because I guess in an actual clinical setting, you kind of bounce ideas off someone else." (ytpshg)_

### Reasoning Strategies in Study 2 {.unnumbered}

```{r stratClassifier, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE, fig.height=7}

trainingDataStrats <- infoSeekingMatrixTA
testingDataStrats <- infoSeekingFullMatrix[infoSeekingFullMatrix$ParticipantType=="p",]

testingDataStrats$initialDiagnoses <- studentCaseDf$initialDifferentials
testingDataStrats$confidenceChange <- studentCaseDf$confidenceChange
testingDataStrats$differentialChange <- studentCaseDf$differentialChange
testingDataStrats$likelihoodAcc <- studentCaseDf$likelihoodOfCorrectDiagnosis

trainingDataStrats <- trainingDataStrats[trainingDataStrats$Strat!="NONE",]

colnames(trainingDataStrats)[1:29] <- c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7",  
                                      "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                      "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")

colnames(testingDataStrats)[1:29] <- c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7",  
                                        "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                        "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")


# Convert all columns to integers
trainingDataStrats[c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")] <- 
  lapply(trainingDataStrats[c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")], as.integer)

trainingDataStrats$stratLabel <- as.factor(trainingDataStrats$Strat)

thresh<-seq(0,1,0.001)

set.seed(101)
#specify the cross-validation method
ctrl <- trainControl(method = "LOOCV", savePredictions = "final", sampling="down")
# T1-4 always sought, and T7
regModelLambda <- caret::train(stratLabel ~ T5 + T6 + T8 + T9 + T10 +
                    T11 + T12 + T13 + T14 + T15 + T16 + T17 +  T18 + T19 + T20 + T21 + T22 + T23 + T24 + T25 + T26 + T27 + T28 + T29, method = "glmnet", family="multinomial", data = trainingDataStrats, trControl = ctrl, tuneGrid = expand.grid(alpha = seq(0, 1, by = 0.1), lambda = seq(0, 0.05, by = 0.01)))

predictions <- regModelLambda$pred
confusionReg <- confusionMatrix(predictions$pred,predictions$obs)
confusionRegTable <- as.data.frame(confusionReg$byClass)

#########################
#coef_list <- coef(regModelLambda$finalModel, s = regModelLambda$bestTune$lambda)
#coef_combined <- list()

#for (i in 1:length(coef_list)) {
#  coef_combined[[i]] <- as.matrix(coef_list[[i]])
#}
#coef_df <- do.call(cbind, coef_combined)
##########################

##############################
# Make predictions
predictions <- predict(regModelLambda, newdata = testingDataStrats, type="raw")

testingDataStrats$classifiedStrat <- predictions
testingDataStrats$classifiedStrat <- as.factor(testingDataStrats$classifiedStrat)

###########################

testingDataStrats$infoAmount <- rowSums(testingDataStrats[,c(1:29)])/29

testingDataStrats$caseDominantStrat <- ifelse(testingDataStrats$Condition %in% c("AD","GBS","UC"),"HD","PR")

testingDataStrats$usingCaseDominantStrat <- ifelse(testingDataStrats$caseDominantStrat==testingDataStrats$classifiedStrat,1,0)

########################
# Add value

infoValueDf <- infoSeekingFullMatrix[,c(1:29)]
colnames(infoValueDf)[1:29] <- c("T1","T2","T3","T4","T5","T6","T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                              "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22", 
                                              "T23", "T24", "T25", "T26", "T27", "T28", "T29")

infoValueDf$Correct <- infoSeekingFullMatrix$Correct
infoValueDf$Condition <- infoSeekingFullMatrix$Condition
infoValueDf$ID <- infoSeekingFullMatrix$ID


temp <- infoSeekingFullMatrix[,c(1:29)]
colnames(temp)[1:29] <- c("T1","T2","T3","T4","T5","T6","T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                          "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22", 
                          "T23", "T24", "T25", "T26", "T27", "T28", "T29")

temp$Condition <- infoSeekingFullMatrix$Condition
temp$ID <- infoSeekingFullMatrix$ID

temp <- temp[!grepl("e1|e2|e3|e4|e5|e6|e7", rownames(temp)),]

standard <- "student" #student/expert
if (standard == "student")
{
  infoValueDf <- infoValueDf[!grepl("e1|e2|e3|e4|e5|e6|e7", rownames(infoValueDf)),]
} else
{
  infoValueDf <- infoValueDf[grepl("e1|e2|e3|e4|e5|e6|e7", rownames(infoValueDf)),]
}


for (n in 1:nrow(temp)) #row
{
  for (m in 1:29) #column
  {
    accSet <- c()
    currentID <- temp[n,]$ID # cross validation
    infoSelectCase <- infoValueDf[infoValueDf$Condition==temp[n,]$Condition,]
    infoSelect <- infoSelectCase[,m]
    infoSelect <- as.data.frame(infoSelect)
    infoSelect <- cbind(infoSelect,infoSelectCase$ID)
    infoSelect <- cbind(infoSelect,infoSelectCase$Correct)
    colnames(infoSelect) <- c("Info","ID","Correct")
    infoSelect <- infoSelect[infoSelect$ID!=currentID,]
    infoSelect <- infoSelect[, !(colnames(infoSelect) %in% c("ID"))] 
    accPresent <- mean(infoSelect[infoSelect$Info==1,]$Correct,na.rm=TRUE)
    accNotPresent <- mean(infoSelect[infoSelect$Info==0,]$Correct,na.rm=TRUE)
    if (nrow(infoSelect[infoSelect$Info==0,]) > 1)
    {
      temp[n,m] <- ifelse(temp[n,m]==1,accPresent-accNotPresent,NA)
      if (is.nan(temp[n,m]))
      {
        temp[n,m] <- 0
      }
    }
  }
}
temp = subset(temp, select = -c(Condition,ID))

temp$infoValue <- rowSums(temp,na.rm = TRUE)


testingDataStrats$value <- temp$infoValue

```

```{r stratClassifierNullShuffle, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE}

nShuffles <- 10

set.seed(1000)

trainingDataStratsShuffle <- trainingDataStrats
trainingDataStratsShuffle$nullStrat <- sample(trainingDataStratsShuffle$stratLabel)

accs <- c()

preds <- c()
obs <- c()

for (i in 1:nShuffles)
{
  trainingDataStratsShuffle$nullStrat <- sample(trainingDataStratsShuffle$nullStrat)
  
  model <- caret::train(nullStrat ~ T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 + T10 +
                    T11 + T12 + T13 + T14 + T15 + T16 + T17 +  T18 + T19 + T20 +
                    T21 + T22 + T23 + T24 + T25 + T26 + T27 + T28 + T29, method="multinom", data = trainingDataStratsShuffle, trControl = trainControl(savePredictions = "final"))

  predictions <- model$pred
  
  #predictions <- predict(model, newdata = trainingDataStratsShuffle)
  #trainingDataStratsShuffle$predictedStratNull <- predictions
  #success <- sum(trainingDataStratsShuffle$predictedStratNull==trainingDataStratsShuffle$nullStrat)/nrow(trainingDataStratsShuffle)
  
  success <- sum(predictions$pred==predictions$obs)/nrow(predictions)
  
  accs[i] <- success
  
  preds <- c(preds,predictions$pred)
  obs <- c(obs,predictions$obs)
  
}

nullPreds <- data.frame(preds,obs)

nullPreds$preds <- as.factor(nullPreds$preds)
nullPreds$obs <- as.factor(nullPreds$obs)

nullConfusion <- confusionMatrix(nullPreds$preds, nullPreds$obs)

nullConfusionTable <- as.data.frame(nullConfusion$byClass)


```

We next turn to the coding of reasoning strategies in Study 2. We train a multinomial logistic classifier to identify reasoning strategy in this think-aloud study, with each of the information requests as binary predictors (i.e. whether they were sought or not on each case). We then compare the predicted strategies from the classifier to the objective 'ground truth' strategies in order to assess the model's accuracy. In order to train the classifier, we account for imbalance in the training data (due to the larger number of HD cases and the lower number of SI cases) using downsampling and limited regularisation. In summary, we are not able to train a classifier that performs better than chance at predicting reasoning strategy. The accuracy of our classifier is `r round(confusionReg$overall["Accuracy"],2)`, which is significantly lower than the No Information Rate of `r round(sum(trainingDataStrats$stratLabel=="HD")/length(trainingDataStrats$stratLabel),2)` (i.e. the accuracy of the classifier if HD was predicted on all cases). We compare classifier accuracy to a bootstrapped null distribution where the ground truth labels are repeatedly shuffled. Based on `r nShuffles` shuffles, we find an average accuracy of `r round(nullConfusion$overall["Accuracy"],2)`. We also compare Balanced Accuracy values (i.e. the accuracy within each reasoning strategy, in order to account for the data imbalance) to those of the null classifier. We find that the Balanced Accuracy of our trained classifier (HD = `r round(confusionRegTable$"Balanced Accuracy"[1],2)`, PR = `r round(confusionRegTable$"Balanced Accuracy"[2],2)`, SI = `r round(confusionRegTable$"Balanced Accuracy"[3],2)`) does not exceed that of the null classifier (HD = `r round(nullConfusionTable$"Balanced Accuracy"[1],2)`, PR = `r round(nullConfusionTable$"Balanced Accuracy"[2],2)`, SI = `r round(nullConfusionTable$"Balanced Accuracy"[3],2)`). To summarise, we are not able to predict reasoning strategy reliably based on information seeking alone using on our think-aloud dataset. Hence, we do not perform further analysis based on the predicted reasoning strategies in Study 2's dataset.

\
```{r dominantstratsonline, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

dominantStratOnline <- testingDataStrats %>%
              group_by(caseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round((mean(likelihoodAcc)/10),2),                 
              `Initial Diagnoses` = round(mean(initialDiagnoses),2), 
              `Information Seeking` = round(mean(infoAmount),2),
              `Confidence Change` = round(mean(confidenceChange)/100,2),
              `Information Value` = round(mean(value),2))

dominantStratOnlineAnalysis <- testingDataStrats %>%
              group_by(ID,caseDominantStrat) %>%
              dplyr::summarise(Accuracy = round((mean(likelihoodAcc)/10),2),
              `InitialDiagnoses` = round(mean(initialDiagnoses),2), 
              `InformationSeeking` = round(mean(infoAmount),2),
              `ConfidenceChange` = round(mean(confidenceChange)/100,2),
              `InformationValue` = round(mean(value),2))

# Accuracy

accTest <- wilcox.test(Accuracy ~ caseDominantStrat, data = dominantStratOnlineAnalysis,paired = T, conf.int = T)

# Confidence

conTest <- wilcox.test(ConfidenceChange ~ caseDominantStrat, data = dominantStratOnlineAnalysis, paired = T, conf.int = T)

# Initial Diagnoses

diagTest <- wilcox.test(InitialDiagnoses ~ caseDominantStrat, data = dominantStratOnlineAnalysis, paired = T, conf.int = T)

# Info Seeking

infoTest <- wilcox.test(InformationSeeking ~ caseDominantStrat,data=dominantStratOnlineAnalysis,paired = T, conf.int = T)

# Info Value

valTest <- wilcox.test(InformationValue ~ caseDominantStrat,data=dominantStratOnlineAnalysis,paired = T, conf.int = T)


# Interaction: Accuracy * Diagnoses 

accModel <- lmer(Accuracy ~ InitialDiagnoses*caseDominantStrat + (1 | ID), data = dominantStratOnlineAnalysis)
accModelAnova <- joint_tests(accModel)

```

We are not able to reliably predict reasoning strategy on a case-by-case basis. We instead look at behaviour as a function of the dominant reasoning strategy for the patient conditions based on the coded reasoning strategies in the think-aloud study. Our aim to is to look at whether cases result in different diagnostic behaviour assuming that each patient condition is associated with a particular reasoning strategy. We find that three of the six conditions (UC, GBS, AD) were approached using a HD strategy by a majority of participants. The remaining cases tended to be performed using a PR strategy (except for the TA case, where there were equal numbers of PR and SI coded participants). We consider these cases (TTP, TA, MTB) as PR-dominant cases for ease of comparison with HD-dominant cases. As our observations are not independent, we calculate the mean values across conditions within each group of cases (HD-dominant or PR-dominant) in the online dataset from Study 2 such that there is a single observation per participant. This allows us to investigate broadly if reasoning strategy on a case-wise level affects diagnostic behaviour. 

\
First, we look at whether our key variables vary as a function of the dominant strategy for cases in order to determine if strategy is associated with a difference in behaviour. As we have two groups of cases, we use paired Wilcoxon Signed Rank to compare median values between these groups (averaged across conditions), as these variables are not normally distributed. We observe higher accuracy for HD-dominant cases (Mdn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="HD",]$Accuracy)`) when compared to PR-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="PR",]$Accuracy)`) (V = `r accTest$statistic`, pseudomedian difference = `r round(accTest$estimate,2)`, 95% CI = [`r round(accTest$conf.int[1],2)`, `r round(accTest$conf.int[2],2)`], p < .001). We also observe that more initial diagnoses being considered during HD-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="HD",]$InitialDiagnoses)`) when compared to PR-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="PR",]$InitialDiagnoses)`) (V = `r diagTest$statistic`, pseudomedian difference = `r round(diagTest$estimate,2)`, 95% CI = [`r round(diagTest$conf.int[1],2)`, `r round(diagTest$conf.int[2],2)`], p < .001). When looking at differences in information seeking, we observed that more information was sought on PR-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="PR",]$InformationSeeking)`) when compared to HD-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="HD",]$InformationSeeking)`) (V = `r infoTest$statistic`, pseudomedian difference = `r abs(round(infoTest$estimate,2))`, 95% CI = [`r abs(round(infoTest$conf.int[1],2))`, `r abs(round(infoTest$conf.int[2],2))`], p < .001). We also observed that informational value was higher for PR-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="PR",]$InformationValue)`) when compared to HD-dominant cases (MDn = `r median(dominantStratOnlineAnalysis[dominantStratOnlineAnalysis$caseDominantStrat=="HD",]$InformationValue)`) (V = `r valTest$statistic`, pseudomedian difference = `r abs(round(valTest$estimate,2))`, 95% CI = [`r abs(round(valTest$conf.int[1],2))`, `r abs(round(valTest$conf.int[2],2))`], p < .001). We did not observe a significant difference between groups of cases in terms of changes in confidence.

\
Finally, we look at predictors of diagnostic accuracy, hypothesising that the dominant reasoning strategy for a case interacts with the number of initial diagnoses when predicting accuracy. Given that HD has been previously associated with more diagnoses being considered early on, we expect that diagnostic accuracy is determined by using the optimal reasoning strategy given the initial diagnostic breadth. In order to investigate this hypothesis, we fit a linear mixed effects model that predicts diagnostic accuracy with an interaction between the number of initial diagnoses and case-dominant strategy, with participant as a random effect. As in the previous analysis, variables are averaged per participant across all conditions within each case group (i.e. for HD-dominant cases, accuracy and initial diagnoses are averaged across the UC, GBS and AD cases for each participant). We find evidence for an interaction between the number of initial diagnoses and case-dominant reasoning strategy (F(`r accModelAnova$df1[3]`,`r accModelAnova$df2[3]`) = `r round(accModelAnova$F.ratio[3],2)`, p = .001). As depicted in Figure 3 below, we observe lower accuracy for PR-dominant cases compared to HD-dominant cases with lower initial diagnostic breadth, but accuracy is highest for PR-dominant cases with higher initial diagnostic breadth.    

```{r accmodelplot, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

intplot <- interact_plot(accModel, pred = InitialDiagnoses, modx = caseDominantStrat) +
  labs(y="Accuracy", x = "Initial Diagnoses", colour = "Reasoning Strategy") +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        legend.title=element_text(size=16),
        legend.text=element_text(size=16))
print(intplot)

```
_Figure 3: Fitted regression line for a linear mixed effects model that predicts accuracy (y-axis) with an interaction between the number of initial diagnoses/differentials (x-axis) and dominant reasoning strategy (HD in blue averaged across the UC, GBS and AD cases, PR in orange averaged across the MTB, TA and TTP cases). Data shown here is from the online dataset of Study 2._

## Discussion {.unnumbered}

This study of 16 medical students explored the usage of a think-aloud methodology to understand thought processes during medical diagnoses. Using our online interface and recorded verbalisations by students, we aimed to detect clinical reasoning strategies based on criteria adapted from Coderre et al. (2003). The strength of this paradigm is in qualitatively recording medical students' thought process as it involves with information as per our flexible, evolving vignette-based interface design. By recording how participants consider different diagnoses in real time, we are able to understand the reasoning approach students are applying for each case and how this affects their information seeking and confidence behaviour. We are also able to investigate if these reasoning strategies affect diagnostic accuracy, both in the context of this current study and in the previous online study. 
\

In terms of performance and calibration, we found that participants' confidence judgements were well calibrated to their objective accuracy, similar to the previous study. The measure of accuracy was different in this study by necessity, in that a case was considered 'correct' with the mention of a correct differential at some point during the case. This measure is most similar to the lenient measure of accuracy from our previous study, in that participants were marked as correct if they considered a correct differential without taking into account its relative likelihood compared to other differentials. However, given that accuracy and our test of calibration being different in this study, it is difficult to compare these findings directly. Our finding of calibrated confidence in both studies however is an indication that medical students express uncertainty appropriately. Similar to the previous study, we also find that medical students are reticent to remove differentials from consideration. In this study, participants report low occurrences of disregarding differentials. This would correspond with our assumption that medical students attempt to remain open minded in their diagnoses. By rarely removing differentials from consideration, students are then observed to broaden their differentials with more information (as the number of differentials being considered at once only increases). 

\
In terms of differential evaluations, we observed via think-aloud utterances a similar tendency for medical students to very rarely remove differentials from their consideration. This would support the finding from the previous study that students tend to broaden the differentials they are considering as they receive more information. We find in this study that a HD reasoning strategy is the most common among students, which is similar to Coderre et al. finding that novice clinicians tended to use a HD reasoning strategy over PR). The overall pattern of broadening differentials in Study 2 can explained by students tending to use a HD strategy but rarely removing differentials from consideration (assuming that there was a majority of students using a HD strategy in Study 2). Prior to conducting both of these studies, we may have expected a 'process of elimination' to be used by students but this does not appear to be the case across both of these studies. Removing differentials seems to be a clear tendency for medical students, rather than a quirk of our interface from the previous study. This has implications for medical education in terms of whether students should be taught situations where it makes sense to disregard differentials or if remaining open-minded at all times is useful all patients. 

\
On reasoning strategies, we were able in this study to use think-aloud utterances to detect reasoning strategies on the part of the medical students. We considered three different strategies: Hypothetico-Deductive (HD), Pattern Recognition (PR) and Scheme-Inductive (SI). These strategies represent different approaches to diagnosis, either seeking to be comprehensive in both the information sought and differentials considered or focusing in on a single diagnosis. We found that these choices in reasoning strategies were not solely determined by either an individual's general decision making approach or by specific patient conditions. Whilst these reasoning strategies carry some differences qualitatively, our study was used to investigate how these strategies actually manifest in differences to information seeking and confidence. We found that HD reasoning was associated with reevaluting the diagnoses considered more often when compared to the other approaches, as well as higher diagnostic accuracy. This is different to the results to Coderre et al. (2003), who found PR was associated with accuracy. We would interpret our findings as HD being a 'better' approach for medical students and PR would be more suitable for experienced clinicians who have more cases to draw from. Our findings suggest that the reasoning strategy used by a medical decision maker should not only be appropriate for the case at hand, but also appropriate for the decision maker's experience/expertise (either for cases of that nature or in terms of general medical experience). This implies pattern recognition should be employed when possessing enough experience to use it accurately.

\
Based on our qualitative findings, we provide support for findings from our previous study. Firstly, several participants reported progressively investigating patient symptoms based on the patient's history and their initial set of diagnostic differentials. This corresponds with our finding in the previous study that the number of initial differentials considered based on the patient history was predictive of information seeking and changes in confidence. This supports evidence for the large weighting on early information received by clinicians, especially to do with history taking, because early information is responsible for the initial set of diagnoses that then guide subsequent information seeking. Secondly, we find a qualitative theme that participants report certain information being standard to seek regardless of the patient case. This corresponds with the finding from our previous study that lower information seeking variability was associated with higher accuracy, with certain information requests in our task being associated with higher accuracy when sought across cases. This further corroborates our evidence for a degree of standardisation in information seeking being useful for maximising diagnostic accuracy. In particular, we had found in Study 2 that lower variability in information seeking was associated with higher accuracy, with specific pieces of information (e.g. full blood count, assessing extremities) being useful to seek regardless of the patient's condition. We can then find an agreement between these studies that diagnosis is a decision process where there is considered to be 'optimal' information to seek for any given patient. Future work from medical professionals should focus on designing cognitive aids for prompting information seeking, such as providing a 'checklist' of what information should be needed for particular conditions. Whilst this is not feasible for all possible patient conditions, some standardisation in information seeking would be beneficial for diagnostic accuracy. 

\
Finally, we identified the dominant reasoning strategy for each of the patient conditions used in our vignettes, and then investigated their effect on accuracy within our previous study's (larger) dataset. We sorted cases into two groups: HD-dominant (where HD was used by a majority of students in the think-aloud study) and PR-dominant (where PR was used by a majority of students in the think-aloud study). Whilst we observed higher accuracy and higher initial diagnostic breadth for HD-dominant cases, we observed higher information seeking and informational value for PR-dominant cases. It seems likely that as a result of our task design, naming more diagnoses increases the chance that a correct diagnosis is mentioned. A HD reasoning strategy being associated with greater diagnostic breadth corresponds with the nature of HD being that of considering a broad set of differentials to either add to or subtract from. Higher information seeking for PR-dominant cases is surprising however, as we may have expected a PR strategy to result in selective information seeking in order to confirm a focal diagnosis. An alternative explanation could be participants seek more information because they are less able to generate a broad set of differentials. However, we find through modelling that accuracy was highest when participants had initial diagnostic breadth on a PR-dominant cases (whilst increasing diagnostic breadth had less of impact on accuracy). Whilst reasoning strategy does seem to vary as a function of the patient condition, an optimal strategy for diagnostic accuracy seems to be generating a larger set of initial differentials and then selectively choosing from this set the diagnosis that closely resembles the patient's symptoms and observations. In other words, medical students performed best by starting broad and then narrowing their differentials. This is predicated on students and clinicians being able to identify plausible diagnoses early on, whilst remaining open minded to other possibilities. An assumption we make from this result however is that the dominant strategy for each patient conditions is the same in the think-aloud study as it was in the online-study. As our multinomial classifier was unable to accurately predict strategies for individual cases in the online study, we are unable to verify this assumption. This is likely because of the relatively small and unbalanced think-aloud dataset used to train the classifier. This could be rectified by future work that uses think-aloud methodologies for larger scale data collection for quantitative analysis such as this.

\
These two studies together provide a nuanced and in-depth look of the diagnostic process as demonstrated with our vignette-based task. We should consider however the generalisability and ecological validity of these studies. By using a vignette-based paradigm, participants do not actually interact with, observe and treat a patient. We are also limited in terms of the information that is available for clinicians to seek. In addition, participants completed the studies in relatively controlled environments, outside of their usual medical context. Our next study hence aims to study the link between information seeking and confidence, but with a more naturalistic paradigm. To alleviate these concerns of generalisability, we require a paradigm that allows for more open-ended information seeking, observation of a patient that can be treated during cases and the use of a clinical environment akin to the one in which clinicians operate. As previously explored in our systematic review, the use of in-situ research lacks objective markers of accuracy that we utilise. To this end, we use virtual reality (VR) in our next study to simulate a realistic medical environment, as well as the patients themselves. This allows for a realistic, interactive paradigm where participants observe a (virtual) patient in real-time and can administer treatment (and observe reactions to this treatment in the patient). There is also more openness in terms of the information that can be sought and clinical actions taken, making its use more analogous to real medical contexts. 
