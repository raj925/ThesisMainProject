---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{r, echo=FALSE}

# Colour coding for figures
confidenceColour <- "#03c200"
difficultyColour <- "#bf00c2"
infoSeekingColour <- "#ca0600"
differentialColour <- "skyblue"
likelihoodColour <- "orange"
accuracyColour <- "black"
resolutionColour <- "yellow"
```


# Study 3 - Diagnostic Reasoning Strategies via a Think-Aloud Paradigm {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Think Aloud}{}
```
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

We aimed to replicate the finding of considered differentials increasing with more information when the method by which these differentials were reported. Are students seeking information to confirm their existing set of differentials, to rule out differentials or to expand their set of considered possibilities? And are these different approaches interleaving or are they more dependent on individual diagnostic decision making styles? In order to provide more context to the results from study 1, we conducted a follow-up study that utilised a very similar experimental procedure, but instead prompted students to think out loud as they were performing the task. and the transcripts were coded to conduct both quantitative and qualitative analysis. 

Think-aloud methodologies are useful for directly accessing ongoing thought processes during decisions (van Someren, Barnard & Sandberg, 1994). The use of thinking aloud (or verbal protocols) in research is useful for being able to access the information attended to participants in short term memory (Payne, 1994) and can be treated as the ongoing behavioural state of a participant's knowledge (Newell & Simon, 1972). Think-aloud protocols have historically been used to study problem solving, particularly for comparing how novices and experts solve problems such as finding the best move in chess (de Groot, 1946, Bilalić, McLeod & Gobet, 2008). Diagnosis is a decisional process that develops over time and allowing participants to think aloud reflects this by providing a time-ordered sequence of how thought processes develop (Payne, 1994). This is especially well-suited to our task where the information available to participants is controlled with time, allowing us to investigate how diagnostic thinking develops with more information. A think-aloud methodology has previously been used to study the differences between novice and expert clinicians during diagnostic reasoning (Coderre et al., 2003). This study found a general trend that experts tend to use a ‘pattern recognition' approach to diagnosis more than novices, who tended to use a ‘hypothetico-deductive' process (which is aforementioned to be the ‘textbook' description of the diagnostic process), but this was highly dependent on the case presented. We build on the work of Coderre et al. (2003) here to further investigate how reasoning strategies contribute to accuracy and why certain cases result in differing strategies. 

## Methods {.unnumbered}
### Participants {.unnumbered}

16 participants were recruited for this study. Participants were 5th or 6th year medical students at Oxford University (including 2nd year Oxford University Graduate Entry Medical students) recruited via posters in the John Radcliffe Hospital in Oxford and via a mailing list for students managed by the Medical Sciences Division at the University of Oxford. The study was conducted onsite at John Radcliffe hospital. Participants were recruited between July 5th 2023 and December 1st 2023. Data was reviewed on an ongoing basis to cease recruitment when emerging themes were exhausted. This study was reviewed and granted ethical approval as an amendment to our existing protocol to allow for audio recordings by the Oxford Medical Sciences Interdivisional Research Ethics Committee under reference R81158/RE004.

### Materials {.unnumbered}

The same set of cases and a similar computer interface from Study 1 were used for this study, with the exception that participants no longer recorded their differentials in a specific screen at the end of each information gathering stage. Instead, participants' differentials were recorded as a more naturalistic part of their diagnostic process as they reported aloud their thoughts as they worked through each diagnostic case. The study was conducted onsite using a laptop, with actions on screen recorded on video and the audio of participants' thinking aloud recorded via a microphone. Informed consent was obtained anonymously using an online electronic information sheet and consent form. Information, including experimental data and audio recordings, collected during the study were stored under anonymised IDs with no linkages to participants. Data was kept on a password-protected computer and hard drive.

### Procedure {.unnumbered}   
The general procedure was very similar to that of Study 1, except that participants were given the following instructions at the start of the study:

"Whilst you are doing the task, you will be asked to think aloud. This means that you verbalise what you are thinking about, especially how you interpret the information you receive and what conditions or diagnoses you are considering or are concerned about for each patient case. If you have nothing to say or nothing on your mind, there's no need to say anything but do say whatever is on your mind once it pops up. If you are unsure about anything you see or do not know about what something means, you will not receive any help but verbalise when you are unsure about anything during the task. Please make sure that you speak clearly 'to the room'."

The experimenter occasionally prompted participants with content-neutral probes: "can you tell me what you are thinking?" in cases of periods of long silence, and "can you tell me more?" when the participant said something vague that may warrant further detail. We emphasise that these are non-leading questions. The audio of the participants' verbalisations was recorded and then transcribed. An initial transcript was generated using Microsoft Office's transcription feature, but the transcript was checked and modified for accuracy by listening through the audio recordings again. The screen of the experimental interface was also recorded, such that the audio could be linked to specific actions within the task. The focus of this study is on verbal utterances rather than any non-verbal or inferential aspects of the participants' qualitative data. At the end of the experiment, the researcher administered a semi-structured interview to better understand what the participants feel their diagnostic reasoning approach tends to be. These questions are provided in the Supplemental Materials.

### Data Analysis {.unnumbered}

We conducted a theory-driven semantic thematic analysis (as per definitions detailed by Braun and Clarke, 2006) to code utterances under specific categories. This kind of thematic analysis is suitable given that our qualitative data is from a structured experiment, rather than a dataset with a looser structure (e.g. interview recordings). As a result, we apply deductive analysis using predetermined codes for think-aloud utterances and for a debrief interview where we administer a semi-structured interview with specific questions of interest.  

Firstly, we code all utterances related to the main research areas of interest in this project, namely information seeking, confidence and differential/hypothesis generation. Respectively, we define the following codes:

* Differential Evaluation: any time that the participant (each of the following is considered a separate subcode):
  * Differential Added - Mentions a new condition that they are considering 
  * Differential Removed - Rules out or eliminates a condition from consideration
  * Likelihood Increased - Mention of increased likelihood of a previously mentioned condition, or that information seems to correspond with a condition
  * Likelihood Decreased - Mention of decreased likelihood of a previously mentioned condition, or that information seems to contradict with a condition

We also define a group of codes that indicate three different diagnostic reasoning strategies: hypothetico-deductive reasoning, scheme-inductive reasoning and pattern recognition (Coderre et al.., 2003). These were defined as follows:

* Hypothetico-Deductive Reasoning - prior to selecting the most likely diagnosis, the participant analysed any alternative differentials one by one through something akin to a process of elimination.
* Scheme Inductive Reasoning - participant structures their diagnosis by pathophysiological systems or categories of conditions (e.g., infective vs cardiovascular causes) to determine root causes of patient symptoms rather than focusing on specific conditions.
* Pattern Recognition - participant considers only a single diagnosis with only perfunctory attention to the alternatives, or makes reference to pattern matching when using a prototypical condition to match its symptoms against the current observed symptoms for the patient (e.g., "these symptoms sound like X" or "this fits with a picture of Y").

We first code specific statements within each case that suggested one of these strategies, and then determined which strategy was most prevalent or influential for cases as a whole such that each case was categorised under one of these strategies. In addition to coding each case under one of these strategies, we also code participants on an overall level based on their subjective perception of how they make diagnostic decisions. This is based on responses provided during the debrief interview (as described in the Procedure section). Hence, reasoning strategy codes are at the case level and also at the participant level. 

Coding of utterances and case-wise reasoning strategies were conducted with a second independent coder. For reasoning strategies, initial interrater reliability was low, with both coders agreeing on 58.3% of cases. Conflict resolution led to changes made to the coding criteria by prioritising strategies used early in a case, as some participants were noted to utilise multiple strategies within a single case, as well as allowing some cases to be coded as not having a clear strategy due to a lack of utterances. Conflicts were then resolved with these updated criteria. Both coders agreed on 78% of cases when coding for correctness, with conflicts resolved in consultation with a member of expert panel used to develop the vignettes (as mentioned in Study 1).    

Although we do not record differentials in the same way as in Study 1 (in a list with corresponding likelihood and severity ratings), we do obtain the other variables from Study 1. Namely, we record confidence at each stage of information seeking and data around the information sought by participants. As we do not explicitly record differentials in the same manner as in Study 1, accuracy is operationalised differently. We code each case as ‘correct' if a correct differential is mentioned at some point by the participant (using the same marking scheme in table S1).

## Results {.unnumbered}

First, we look at overall quantitative characteristics of the think aloud statements. When looking at accuracy (the proportion of cases where a correct differential was mentioned by the participant), accuracy was 0.57 across all cases. This varied considerably by condition however, with accuracy across participants for each condition being as follows: AD = 0.63, GBS = 0.88, MTB = 0.19, TA = 0.44, TTP = 0.69, UC = 0.63. For utterances coded as Differential Evaluations, participants on average made 5.21 such utterances per case (SD = 2.80). The mean number of Differential Evaluations was relatively constant by condition except for the AD case: AD = 8.18, GBS = 4.63, MTB = 4.81, TA = 4.75, TTP = 4.25, UC = 4.63. Participants varied in how much they spoke during the study, uttering 1038-7730 words (M = 4194) across the scenarios. Part of this range is driven by participants repeating information they see during the task, but participants also varied in terms of how much they externalised their thought process. 

As previously mentioned, Differential Evaluations can be further categorised into one of four subcodes: Differential Added, Differential Removed, Likelihood Increased and Likelihood Decreased. As found in the previous study, there is a general reticence to disregard differentials completely. Participants expressed significantly more statements adding differentials (M = 3.14, SD = 0.89) than removing differentials (M = 0.27, SD = 0.28) (t(15) = 14.14, MDiff = 2.86, p < .001). Participants expressed more statements of decreasing likelihoods (M = 0.99, SD = 0.62) rather than increasing likelihoods (M = 0.93, SD = 0.46) but we did not find evidence of a significant difference (t(15) = 0.34, MDiff = 0.06, p = .73).  

### Reasoning Strategies {.unnumbered}
Next we look at our coding of reasoning strategies at a case level. As mentioned, our criteria for each code was applied to each individual case based on the transcribed utterances. When looking at reasoning strategies by case, 43 cases were coded as Hypothetico-Deductive, 29 were coded as Pattern Recognition and 18 were coded as Scheme Inductive (the remainder of cases did not contain enough clear utterances to classify under one of these strategies). Accuracy was higher for cases coded as Hypothetico-Deductive (71%) compared to both Pattern Recognition cases (64%) and Scheme Inductive (39%). It is worth noting here that accuracy was solely based on participants mentioning differentials during their thinking aloud, which is naturally not facilitated by Scheme Inductive reasoning due to its focus on identifying pathophysiological systems acting as sources of patient symptoms rather than specific conditions. This can hence explain the lower ‘accuracy' for Scheme Inductive cases. We also note that the types of reasoning strategy used varies by condition (see Figure 13 below), with the MTB and TTP cases in particular exhibiting higher usage of Pattern Recognition than others. This could be because this case was considered harder than others and hence participants could not generate a larger set of candidate differentials due to its difficulty.

We note, rather unsurprisingly, that we observe a higher number of average Differential Evaluations when cases are correct (M = 5.85, SD = 0.38) compared to when they are incorrect (M = 4.34, SD = 0.39). Given our methodology for defining accuracy, participants are more likely to mention a correct differential if they mention more differentials. The procedure used in the previous study for collecting data on which differentials participants were considering at each information stage was not present here and hence we are not able to operationalise accuracy in the same manner as before. While we look at which differentials are mentioned, we cannot observe how participants weigh up differentials against each other in the same way as in the first study. 

To connect the results of this study to those of Study 1, we break down the same dependent variables (as operationalised in that study) by reasoning strategy. We do not apply statistics to this study due to the lower sample size. We first categorise each of the 6 cases as having a ‘dominant' reasoning strategy based on which was utilised the most across participants. Through this process, we categorise three conditions as HD (AD, UC, GBS), three conditions as PR (MTB, TTP, TA). The proportions of participants who use each reasoning strategy for each condition can be viewed in Figure 10. We then compare the individual case classifications of strategy to this reasoning strategy that is most commonly used for that medical condition. Table 2 shows how dependent variables are affected by reasoning strategy. We find that the amount of information seeking was fairly consistent across reasoning strategy, but that PR cases were associated with higher value in information seeking. In order to derive informational value, we used the same values of each piece of information for each case that were derived in Study 1. This higher informational value does not translate into higher accuracy for PR cases, though we should note that the manner in which accuracy was defined for this study limits the analysis only to statements made out loud of specific conditions rather than formally recorded differentials as we did in Study 1. In order to formally replicate this finding with the larger dataset, we use the cases from this study and the coding of strategies to apply the same coding to our online dataset from Study 1. 

### Reasoning Strategies in Study 1 Dataset {.unnumbered}

In order to apply reasoning strategies to the data from Study 1, we train a classifier using penalised multinomial regression to classify cases as HD, PR or SI using the cases from the think aloud study (with Leave One Out Cross Validation). The input parameters for the classifier are the 29 pieces of information as binary predictors (similar to the approach depicted in Figure 7) and the cases' condition. In other words, the cases from the think-aloud study make up the training data for the classifier whilst the cases from the larger online study is the test dataset. The classifier was implemented using R's nnet package (version 7.3-19). The testing data is then labelled with predicted testing strategies using R's predict function. We note that the training data was initially labelled with reasoning strategies using the think-aloud utterances and thus is separated from the information sought during the case. 

We show a breakdown of cases by their coded reasoning strategy in Table 4. We now look to compare our key dependent variables by strategy, in particular comparing PR and HD cases. In line with our expectations based on the definitions of HD and PR reasoning approaches, we find that HD cases are associated with more differentials being considered (M = 3.37, SD = 1.64) average when compared to PR cases (M = 2.84, SD = 1.58) and find evidence of a difference between the two via a Welch Two Sample t-test (t = 2.89, MDiff = 0.53, p = .004). We find that PR cases are associated with higher informational value (M =  2.35, SD = 1.07) when compared to HD cases (M =  2.15, SD = 1.32) (t = 1.48, MDiff = 0.20, p = .14). However we do find evidence of higher amounts of information seeking for HD cases (M = 0.63, SD = 0.21) when compared to PR cases (M = 0.50, SD = 0.21), (t = 5.28, MDiff = 0.13, p < .001). Overall, this indicates that PR reasoning were associated with lower but more selective information seeking when compared to HD reasoning. 

We hypothesised that an interaction with reasoning strategy is associated with accuracy on the task. This is because a single reasoning strategy is considered unlikely to be more accurate for all cases. As indicated by Figure 10, different patient conditions seem to result in varying reasoning strategies being utilised, which begs the question of what properties of a condition contribute to changes in strategy and in accuracy. One possibility is that reasoning strategy interacts with the diagnostic uncertainty of a case (i.e. the breadth of conditions that a patient could have given their current symptoms and history, with some conditions presenting in a more apparent way than others), as captured by the number of initial differentials reported by participants. To test this hypothesis, we fit a linear model to predict accuracy with an interaction between the number of initial diagnoses and reasoning strategy. The interaction regression lines are plotted below in Figure 11.

## Discussion {.unnumbered}
In the quantitative results shown in Table 4, we firstly find that in our think-aloud study, PR cases were associated with the highest informational value (despite fairly constant amounts of information seeking across reasoning strategies). SI cases were associated with the highest increase in confidence across the information stages. Whilst the latter finding was similar in the online study, PR cases saw higher informational value than HD cases in this dataset. In accordance with our definitions of reasoning strategies, we find that PR cases were associated the fewest differentials of the three strategies. These findings indicate that different reasoning strategies result in behavioural differences when performing diagnoses, both in terms of information seeking and weighing up differentials. 

We also find evidence of an interaction effect between reasoning strategy and the number of initial diagnoses on accuracy. Intuitively, it makes sense to broaden or narrow differentials based on the number of differentials being considered. Given that reasoning strategies differ by a function of the case/condition, it might be that the case-level factor affected reasoning strategy is how ‘apparent' the underlying condition of the patient is based on the initial patient presentation/history. We operationalise this as the number of initial differentials, which captures how clear the patient's condition for a given participant based on the patient history. In the case of the interaction depicted in Figure 14, we find that reasoning strategy and the number of initial differentials interact. With a lower number of initial differentials, participants exhibited increased diagnostic accuracy by broadening their consideration of differentials via a hypothetico-deductive process (i.e. "what else could be causing these symptoms?"). With a higher number of initial differentials, higher diagnostic accuracy was found by narrowing differentials via a pattern recognition process (i.e. "which of these differentials does this patient most resemble?"). Whilst past work has tended on focusing on designing cognitive interventions that aim to fit all diagnostic scenarios, this result indicates that a flexibility in reasoning strategy based on the patient case is key for increased accuracy. Future research should hence focus on prompting the right reasoning strategy based on the initial patient presentation.

We can consider both studies together to provide a nuanced discussion of the diagnostic process among medical students. We find that information seeking patterns and evaluation of differentials during the diagnostic process contribute to diagnostic accuracy. When students generated a greater number of differentials from a patient history, they sought a greater amount of information. We then observe an association between information seeking and confidence, but not with accuracy. Instead, accuracy was characterised by more selective information seeking during the diagnostic process. This is important to note as it demonstrates that being selective in information seeking is a better marker of performance and giving a lower ability participant all available information does not necessarily translate into accurate diagnoses even though it increases diagnostic confidence (Gruppen, Wolf & Billi, 1991). 

This has interesting implications for medical practice, as the ordering of unneeded tests or patient examinations may not contribute to better decisions and is not cost effective. Given the constraints within most hospitals and healthcare to obtain certain tests, being selective with information seeking is already a necessity and results from this study seem to show evidence that it is also a good marker of diagnostic performance. There has been increased research on overtesting, such as requesting costly imaging scans when they may not be medically necessary (Carpenter, Raja & Brown, 2015). ‘Overtreatment' has been estimated to cost the US healthcare system between 158 and 226 billion dollars in 2011 (Berwick & Hackbarth, 2012). Seeking more information during the task made students more confident but not more accurate, which is important to note as it corresponds with previous findings from the cognitive psychology literature (Ko, Feuerriegel, et al., 2022).

The finding of evidence for an interaction between strategy and the number of diagnoses with regards to accuracy is an interesting one for future medical education. Past work that aim to teach diagnostic reasoning or administer cognitive interventions/aids has tended to assume that a single aid can be optimal for all types of patient cases. However, this results hints at the fact that reasoning strategies' effects on accuracy depend on the initial diagnostic uncertainty associated with the case. In particular, PR seems to result in lower accuracy for fewer initial diagnoses and higher accuracy for more initial diagnoses. This makes intuitive sense when considering how reasoning strategies relate to reducing or expanding the space of possible diagnoses. For instance, if a clinician has a large set of possible differentials in mind from the initial patient presentation, they should narrow their range of possibilities using a pattern recognition approach ("which of these conditions does this most match?"). Conversely, if a clinician is struggling to bring multiple differentials to mind, they should broaden their thinking to consider more conditions using a hypothetico-deductive approach ("what other conditions should I be concerned about?"). This account of the results is bolstered by our operationalisation of accuracy, whereby participants are more accurate by not only considering the correct condition but also considering it as highly likely amongst the considered differentials. 

Coderre et al. (2003) found the pattern recognition was utilised more as clinicians increased in experience. On the one hand, this makes sense given that having more experience of disease presentations would improve a diagnostician's ability to match symptoms to a condition. However, as alluded to by students in this study, knowledge and experience brings with it the ability to generate more differentials than a less experienced clinician. One cannot adopt a hypothetico-deductive reasoning process, whereby multiple differentials are considered and then eliminated, if the clinician lacks sufficient knowledge to generate a set of differentials based on the observed patient. This may be where the complexity/difficulty of the case has a bearing on reasoning process too, whereby harder cases are harder because one cannot easily generate differentials for them. However, the inverse could also be true, whereby a set of conflicting symptoms may cast a wider net of potential differentials that are more challenging to narrow down. As we noted in the online study, the number of initial differentials has an impact on information seeking behaviour, but as we explain here, differentials are themselves a result of a particular reasoning strategy. Ascertaining the exact interaction between reasoning strategy, case difficulty and differential evaluation is hence important for us to focus on in the following study, as it informs how diagnosis is characterised as a cognitive process and how cognitive interventions are designed to aid the process. 

We can also observe that reasoning strategies may in turn bring with them differences in information seeking patterns. The choice of information or tests within the diagnostic has been understudied to date given its role in real-world clinical settings. Our results in Study 1 indicate that information seeking patterns are associated with accuracy, specifically around greater selectivity and less variability in information seeking. This relates well to real-world clinical decisions where information and tests can require sizeable time and even other staff or technology to request, as mentioned earlier. Hence, in a setting where all possible information is not always readily available to clinicians, being selective is advantageous. In addition, being more standardised in information seeking can also make comparisons between patients easier given that the information being compared is more similar. We can assume that a big part of gaining medical experience is by using past patient cases personally experienced by the clinician and then drawing upon that experience for a new  incoming patient. This may explain the findings of Coderre et al. (2003) that pattern recognition was utilised more with experience: because experienced clinicians have more past patients to draw patterns from. 

### Study Strengths {.unnumbered}
We note a number of strengths of both of these studies. To our knowledge, this is the first research of this kind to use both a mixed-methods approach to understand the cognitive underpinnings of diagnosis as a decisional process. Our paradigm also emulates diagnosis in a manner that is not simply a single decision.  When creating a task that emulates diagnosis, we in a sense conceptualise what diagnosis looks like in a fairly static manner, when really diagnosis is a more fluid and nebulous process in medicine. In our study, diagnosis is modelled as a process that develops over time with more information and is constantly shifting doctors' thought processes. In particular, we note linking diagnostic accuracy, confidence, information seeking and differential generation has not been attempted in prior work and should be considered for future work to consider a more complete study of diagnostic decisions. Future work could build upon on this work to investigate more flexible and open information seeking by emulating naturalistic decision making processes. This can be used to investigate how contextual limitations on information seeking impact confidence and accuracy (such as time pressures or testing being unavailable). The use of a think-aloud paradigm also brings a number of strengths by allowing for an ongoing recording of medical students' thought processes, again showing the dynamic, evolving nature of the diagnoses. While participants varied in terms of their ability to verbalise their thoughts, it provides a clear access to how they approaching their decisions that would otherwise be difficult to determine in real-time. We therefore encourage future researchers to consider think-aloud methodologies in their work.

### Limitations {.unnumbered}
We note that the use of a think-aloud methodology brings with it a couple of limitations. Firstly, participants may behave differently to how they otherwise would, given that they are being observed and recorded by a researcher. Hence, there may be a tendency toward medical students behaving in a manner that they believe to be judged as better by others, such as being thorough in their information seeking and differential evaluation. Relatedly, we found that medical students naturally differed from one another in terms of the amount of verbalising they did during the task, which could be related to differences in verbalisation skills (van Somersen, Barnard & Sandberg, 1994). By not explicitly asking students for their diagnostic differentials as we did in Study 1 (and minimising the amount of input that the researcher had during the task), we are constrained to analysing only what students say out loud. Given that some students do not verbalise their thoughts as naturally, we may not aware of the aspects of their thought process that they did not verbalise. For example, participants may not explicitly say out loud that a differential is no longer under consideration when in actual fact it has been dropped from their thought process, leading to a lower number of removed differentials as we observed in the data. Similarly, participants may have multiple differentials in mind but some may be subconsciously considered too unlikely to be even worth mentioning. This would then contribute to fewer overt instances of a hypotethico-deductive reasoning process as differentials are underreported. Future work should utilise more structured methods for eliciting clinicians' thought process during diagnoses in order to ensure accurate reporting of differentials in a more naturalistic, evolving manner. We also could have recruited a larger sample in order to gain a better range of participants and reasoning strategies, increasing the power of our analyses, as well as participants from differing experience levels.

### Implications for Medical Practice and Education {.unnumbered}
There is real value in teaching metacognition and uncertainty within medical education (Royce, Hayes, & Schwartzstein, 2019), such as with the use of cognitive aids (Chew, Durning & Van Merriënboer, 2016, Ely, Graber & Croskerry, 2011), especially given that doctors can be reticent to express their uncertainty (Katz, 1984). A more structured aid is needed, as simply looking at a case for a second time may not be sufficient to improve diagnostic accuracy (Monteiro et al., 2015) and current cognitive forcing strategies have not been found to be effective enough (Sherbino et al., 2014). The reason for this might that past distinction between System 1 (automatic, quick) and System 2 (deliberate, slow) thinking for prompting diagnostic reasoning may be overly simplistic, in that one solution may not fit all possible cases and all clinicians. Future work should focus on understanding when certain reasoning styles and which cognitive aids may be more useful for a given clinical situation. In particular, our findings indicate that cognitive aids should prompt reasoning strategies based on how clear a patient presentation is. This can be thought of as whether the patient's initial symptoms suggest a narrow or wide range of differentials, which is likely a combination of the clinician's knowledge and case's complexity. 

Past work on cognitive interventions have not tended to focus on prompting appropriate information seeking, and we show here that different facets of information seeking contribute uniquely to both confidence and accuracy. While the most relevant information that should be afforded to clinicians will differ depending on the medical discipline, interventions can focus on standardising which is the most valuable information to be presented to clinicians in the first place. This could not only improve diagnostic accuracy but ensure more appropriate expressions of confidence and uncertainty by reducing a tendency toward overtesting. We emphasise that such recommendations are highly dependent and variable depending on the specific medical context, but this acts an important facet for medical education to consider around how seeking information relates to reasoning styles and how important these non-technical skills are to integrate into the educational context of medicine. 

