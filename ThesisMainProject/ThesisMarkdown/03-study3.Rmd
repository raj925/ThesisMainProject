---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

```{r, echo=FALSE}

# Colour coding for figures
confidenceColour <- "#03c200"
difficultyColour <- "#bf00c2"
infoSeekingColour <- "#ca0600"
differentialColour <- "skyblue"
likelihoodColour <- "orange"
accuracyColour <- "black"
resolutionColour <- "yellow"
```


# Study 3 - Diagnostic Reasoning Strategies via a Think-Aloud Paradigm {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Think Aloud}{}
```
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

```{r install_packages, include=FALSE}
source('scripts_and_filters/install_packages_if_missing.R')

binarysimilarityMean <- function(m){
  mat <- binarysimilarityMat(m)
  values <- mat[upper.tri(mat)]
  return(c(mean(values),sd(values)^2))
}
```

```{r AggregateData2, include=FALSE, eval=knitr::is_latex_output()}

df <- as.data.frame(read.csv("./study2data.csv",header=TRUE))
source('scripts_and_filters/Study2/AggregateData.R')

TAData <- as.data.frame(read.csv("./study3data.csv",header=TRUE))

###### Info Seeking Variance and Value

infoSeekingMatrixTA <- data.frame()
studentInfoDf <- as.data.frame(read.csv("./studentInfoDf.csv",row.names = 1,header=TRUE))

for (p in 1:nrow(TAData))
{
  infoVector <- TAData$reqTestArray[p]
  infoVector <- str_split(infoVector,", ")[[1]]
  infoVector <- as.numeric(infoVector)
  condition <- TAData$Condition[p]
  
  infoValues <- studentInfoDf[rownames(studentInfoDf) == condition,]

  valueVector <- infoVector
  # Iterate through each element in infoVector
  for (i in 1:length(valueVector)) {
   # Check if the current element is 1
    if (valueVector[i] == 1) {
      # If it is, replace it with the corresponding value from infoValues
      valueVector[i] <- as.double(infoValues[i])
    }
  }
  
  TAData$infoValue[p] <- sum(valueVector)
  
  infoSeekingMatrixTA <- rbind(infoSeekingMatrixTA, infoVector)
  rownames(infoSeekingMatrixTA)[p] <- paste(TAData$ID[p],
                                            "-OverallStrat",
                                            TAData$InterraterStrat[p],
                                            "-CaseDominantStrat",
                                            TAData$caseDominantStrat[p],
                                            "-usingCaseDominantStrat",
                                            TAData$usingCaseDominantStrat[p],
                                            "-matchingIndividualDominantStrat",
                                            TAData$matchingIndividualDominantStrat[p],
                                            "-",
                                            TAData$Condition[p],
                                            "-Correct",
                                            TAData$Correct[p],
                                            sep="")
  
  
  
}

colnames(infoSeekingMatrixTA) <- c(1:29)
infoSeekingMatrixTA$Condition <- TAData$Condition
infoSeekingMatrixTA$ID <- TAData$ID
infoSeekingMatrixTA$Correct <- TAData$Correct
infoSeekingMatrixTA$Strat <- TAData$InterraterStrat
infoSeekingMatrixTA$caseDominantStrat <- TAData$caseDominantStrat
infoSeekingMatrixTA$usingCaseDominantStrat <- TAData$usingCaseDominantStrat
infoSeekingMatrixTA$matchingIndividualDominantStrat <- TAData$matchingIndividualDominantStrat

distancesTA <- infoSeekingMatrixTA[,1:29] %>% proxy::dist(method = "Dice") %>% as.matrix()
colnames(distancesTA) <- rownames(distancesTA)

```

## Introduction {.unnumbered}

In this mixed-methods study, we aim to gain insight on the types of reasoning strategies used by medical students and how these strategies influence their information seeking patterns and changes in confidence over the course of their diagnostic decisions. We also investigate why reasoning strategies may vary on a case-by-case basis. We utilise a very similar experimental procedure to our previous online study (using the same patient vignettes), but instead prompted students to think out loud as they were performing the task. Everything said by participants was audio-recorded, transcribed and then coded for both quantitative and qualitative analysis. 
\

In our previous study, we observed a general tendency for participants to broaden the set of differentials they considered as they received more information. This was reflected in the average number of differentials reported by the end of the case being higher than the  average number of initial differentials (based on the patient history). This effect was driven by participants (74 out of 85 participants, 87%) never reporting fewer differentials at the Testing stage compared to the Patient History stage. This is a surprising result, as we may have expected participants to use a 'process of elimination', which would manifest in decreasing the number of differentials considered as participants receive more information. What this speaks to however is a general reticence to remove differentials from consideration. One of our focuses for this study then is to replicate this finding by examining if students' thought processes reflect this tendency to focus on broadening rather than narrowing differentials being considered. This is to ensure that the finding in our previous study is not merely a quirk of our experimental interface and design, as it is possible that participants are not sufficiently encouraged to disregard differentials within our paradigm.
\

Another focus of this study is to understand how differences in information seeking patterns arise. One possibility is that these differences stem from reasoning strategies adopted when making diagnoses. Clinical reasoning is a key skill that is taught, either explicitly or implicitly, within medical education. Medical decisions are frequently made in uncertainty, with clinical reasoning taught as a skill to navigate this uncertainty. However, clinical reasoning has a broad remit and covers multiple different approaches to making medical decisions. Doctors may have different considerations when diagnosing a patient. As we noted in the design of our previous study, doctors may have consider what conditions are likely for a patient and what conditions are too severe to not miss. A doctor's reasoning strategy may then reflect this dichotomy, such that the focus is either on determining what is most likely or being thorough such as to consider all possible diagnoses. Diagnostic decisions have traditionally been thought of as ‘ideal’ when using the hypothetico-deductive process (Kuipers & Kassirer, 1984), whereby hypotheses are formulated based on patient symptoms and established criteria for a diagnosis. Further information is then gathered to test these hypotheses (Higgs et al., 2008) or eliminate others. This theory was challenged by the results of Coderre et al. (2003), who found that reasoning strategies differed between novice and expert clinicians and that, crucially, a pattern recognition approach (rather than hypothethico-deductive) was associated with higher diagnostic accuracy. A pattern recognition approach would involve considering fewer diagnostic hypotheses and instead matching the symptoms to prototypical cases of a patient condition. Hypothetico-deductive reasoning represents a more structured, systematic approach to make diagnoses, in which clinicians maintain a more open mind to different diagnostic possibilities, whilst pattern recognition is more directed and driven by intuition. 

\
In their paper, Coderre et al. (2003) asked novice and expert clinicians to provide diagnoses based on patient vignettes and asked them to verbalise their thought process. Using these verbalisations, the researchers categorised the clinician's reasoning strategy on each case. The researchers defined three reasoning strategies as follows (paraphrased from their paper):

* *Hypothetico-deductive (HD) strategy*: prior to selecting the most likely diagnosis, the clinician analysed, one by one, each alternative diagnosis. 

* *Scheme-inductive (SI) strategy*: This strategy consisted of key predetermined
propositions that linked categories and thus provided evidence for chunking (i.e. scheme use). These propositions were presented as structuring diagnoses by different pathophysiological systems/processes (e.g. Small Bowel vs. Large Bowel, Gastrointestinal vs. non-GI causes).

* *Pattern Recognition (PR) strategy*: The clinician directly reached a single
diagnosis with only perfunctory attention to alternative diagnoses.

These reasoning strategies may have been explicitly taught during a clinician's medical education or implicitly developed with experience. With these differing strategies, we can consider whether there are normatively 'better' strategies to use in certain clinical situations. As highlighted with the previous set of conflicting literature, there is currently not a consensus within medicine as to which strategy is ideal for diagnostic accuracy. In their study, Coderre et al. found that novice clinicians tended to adopt a HD strategy more often, whilst experts tended to use a PR strategy more. In addition, using a PR strategy was associated with higher diagnostic accuracy, which in turn was used to explain why experts were more accurate than novices. In addition to explaining differences in accuracy, these strategies point to different approaches in how diagnostic differentials are generated and considered. A PR strategy, by definition, considers a clinician as seeking to identify the correct condition whilst SI and HD strategies reflect a more thorough, systematic consideration of possible differentials. The work of Coderre et al. did not reveal whether these strategies result in differences in information seeking and confidence. We would expect that with a more thorough SI or HD reasoning strategy, participants would seek more information in order to consider a larger number of diagnostic possibilities. By contrast, we would expect that a PR strategy would result in selective (but less) information seeking in order to gather evidence in support of the single diagnosis being considered. 
\

In order to pick apart these different reasoning strategies within our vignette-based paradigm, we adopted a think-aloud methodology similar to Coderre et al. (2003), whereby participants verbalise their thought process as they are doing the vignette-based diagnosis task. Think-aloud methodologies are useful for directly accessing ongoing thought processes during decisions (van Someren, Barnard & Sandberg, 1994). The use of thinking aloud (or 'verbal protocols') in research is useful for being able to access the information attended to participants in short term memory (Payne, 1994) and can be treated as the ongoing behavioural state of a participant’s knowledge (Newell & Simon, 1972). Think-aloud protocols have historically been used to study problem solving, particularly for comparing how novices and experts solve problems such as finding the best move in chess (de Groot, 1946, Bilalić, McLeod & Gobet, 2008). Diagnosis is a decisional process that develops over time and allowing participants to think aloud reflects this by providing a time-ordered sequence of how thought processes develop (Payne, 1994). This is especially well-suited to our task where the information available to participants is controlled at discrete time points, allowing us to investigate how diagnostic thinking develops with more information. A think-aloud methodology has previously been used to study the differences between novice and expert clinicians during diagnostic reasoning (Coderre et al., 2003). We build on the work of Coderre et al. here to further investigate how reasoning strategies contribute to differences in both accuracy and confidence, as well as understanding why certain cases result in differing strategies. 
\

In order to bolster our findings from this study, we aim to use the reasoning strategies determined from this study to reanalyse the cases in Study 2. Whilst we record information seeking, confidence and differential behaviour during the previous study, we do not have an understanding of how participants are approaching each case from a reasoning strategy perspective. One way in which we can to some extent infer this is via the information seeking patterns that participants adopt. If our hypothesis is correct that reasoning strategies result in different patterns of information seeking, we should then we able to predict what strategy a participant is using solely from their information seeking. If these predictions are reliable, we can then study the properties of these reasoning strategies with the larger sample size afforded to us in Study 2, as well as participants' behaviour with regard to consideration of differentials. This work would then improve our understanding of how these reasoning strategies not only affect diagnostic accuracy, but how they contribute to information seeking and confidence. Based on these findings, we could then draw some implications for medical education around clinical reasoning. 

### Research Questions {.unnumbered}

In this study, we investigate the following research questions:

* What is the thought process of students as they performing our diagnosis task? Do students report ruling out differentials as they seek information on patient during diagnoses?
* Can we define different reasoning strategies based on the think-aloud utterances of medical students?
* If so, what reasoning strategies are medical students using when making diagnoses and weighing up differentials?
* How do differences in reasoning strategy manifest in terms of information seeking, both in terms of the quality and quantity of information sought?
* Are differences in reasoning strategy related to the individual or are they dependent on the case at hand? Do better performing medical students utilise specific reasoning strategies?
* What considerations do medical students report having whilst they are making diagnoses? And how these differ from how medical students reflect on their thought process after performing the task?

## Methods {.unnumbered}

### Participants {.unnumbered}

16 participants were recruited for this study. Participants were 5th or 6th year medical students at the University of Oxford (including 2nd year Graduate Entry Medical students) recruited via physical posters at Oxford's John Radcliffe Hospital and via a mailing list for students managed by the Medical Sciences Division at the University of Oxford. The study was conducted onsite at the John Radcliffe hospital. Participants were recruited between July 5th 2023 and December 1st 2023. This study was reviewed and granted ethical approval as an amendment to our existing protocol to allow for audio recordings by the Oxford Medical Sciences Interdivisional Research Ethics Committee under reference R81158/RE004.

### Materials {.unnumbered}

The same set of vignettes and a similar computer interface to Study 2 was used for this study, with the exception that participants no longer explicitly recorded their differentials at the end of each information gathering stage. Instead, participants’ differentials were recorded in a more naturalistic way. Participants verbalised out loud  their thought process as they worked through each diagnostic case. The study was conducted onsite using a laptop, with actions on screen recorded on video and the audio of participants’ thinking aloud recorded via a microphone. Informed consent was obtained anonymously using an online electronic information sheet and consent form. Information, including experimental data and audio recordings, collected during the study were stored under anonymised IDs with no linkages to participants. Data was kept on a password-protected computer and hard drive.

### Procedure {.unnumbered}

The general procedure was very similar to that of Study 1, except that participants were given the following instructions at the start of the study:
\

*“Whilst you are doing the task, you will be asked to think aloud. This means that you verbalise what you are thinking about, especially how you interpret the information you receive and what conditions or diagnoses you are considering or are concerned about for each patient case. If you have nothing to say or nothing on your mind, there’s no need to say anything but do say whatever is on your mind once it pops up. If you are unsure about anything you see or do not know about what something means, you will not receive any help but verbalise when you are unsure about anything during the task. Please make sure that you speak clearly ‘to the room’.”*
\

The experimenter occasionally prompted participants with content-neutral probes: *“can you tell me what you are thinking?”* in cases of periods of long silence, and *“can you tell me more?”* when the participant said something vague that may warrant further detail. We emphasise that these are non-leading questions. The audio of the participants’ verbalisations was recorded and then transcribed. An initial transcript was generated using Microsoft Office’s transcription feature, but the transcript was checked and modified for accuracy by listening through the audio recordings again. The screen of the experimental interface was also recorded, such that the audio could be linked to specific actions within the task. The focus of this study is on verbal utterances rather than any non-verbal or inferential aspects of the participants’ qualitative data. Given that participants were encouraged to verbalise their considered differentials as they were performing the task, we did not show participants the screen where they explicitly listed the differentials they were considered. At the end of the experiment, the researcher administered a semi-structured interview to better understand what the participants feel their diagnostic reasoning approach tends to be. These questions are provided in the Appendices.
\

Aside from these differences, participants performed the same six patient vignettes (in a randomised order) from the first study using the same interface that allows them to seek information that they think is useful for that particular case.

### Data Analysis {.unnumbered}

Our data analysis process for this mixed-methods study is split into a few parts. We first describe the main quantitative variables and analysis for this study. We then detail our coding process for detecting reasoning strategies based on participants' think-aloud utterances, followed by quantitative analysis we perform based on these coded reasoning strategies. We then describe the qualitative analysis performed based on the recorded debrief interviews with participants. Finally, we detail the process by which we code for reasoning strategies in the previous study's (Study 2) dataset. 

#### Descriptive Quantitative Analysis {.unnumbered}

The variables defined for this study are similar to Study 2, as we utilised the same interface and vignettes. Specifically, the variables for confidence, subjective difficulty and information seeking are the same as in Study 2. We note some key differences however. Firstly, given that participants did not explicitly report the differentials they were considered at each information stage, we are not able to record the number of differentials at each stage. Secondly, we also define accuracy differently due to the lack of this differential reporting screen: 
\

* _Accuracy_: Each case is defined as correct if a differential that is considered correct (as per our marking criteria in the Appendices) is mentioned by the participant at some point during the case. 

We also code all utterances related to differential/hypothesis generation. We define instances of Differential Evaluation, which is a main code that comprises a number of subcodes that we apply to think-aloud utterances. These are defined as follows:
\

* **Differential Evaluation**: any time that the participant (each of the following is considered a separate subcode):
 + * _Differential Added:_ - Mentions a new condition that they are considering 
 + * _Differential Removed:_ - Rules out or eliminates a condition from consideration
 + * _Likelihood Increased:_ - Mention of increased likelihood of a previously mentioned condition, or that information seems to correspond with a condition
 + * _Likelihood Decreased:_ - Mention of decreased likelihood of a previously mentioned condition, or that information seems to contradict with a condition
\

Based on this, in lieu of the Number of Differentials variable from our study, we define a new variable to look at the number of instances in which participants evaluate or reevaluate the differentials they are considering:

* _Number of Differential Evaluations_: The number of instances of the above subcodes belonging to the main Differential Evaluation code. The number of such utterances are defined for each individual case. 

#### Coding of Reasoning Strategies {.unnumbered}

We aim to detect which reasoning strategies are used by students on each case. To code for reasoning strategies, we adopt a similar approach to Coderre et al. (2003). We define coding criteria that indicate three different diagnostic reasoning strategies: hypothetico-deductive reasoning, scheme-inductive reasoning and pattern recognition (Coderre et al.., 2003). These were defined as follows:
\

* **Hypothetico-Deductive Reasoning (HD)** - prior to selecting the most likely diagnosis, the participant analysed any alternative differentials one by one through something akin to a process of elimination.
* **Scheme Inductive Reasoning (SI)** - participant structures their diagnosis by pathophysiological systems or categories of conditions (e.g., infective vs cardiovascular causes) to determine root causes of patient symptoms rather than focusing on specific conditions.
* **Pattern Recognition (PR)** - participant considers only a single diagnosis with only perfunctory attention to the alternatives, or makes reference to pattern matching when using a prototypical condition to match its symptoms against the current observed symptoms for the patient (e.g., “these symptoms sound like X” or “this fits with a picture of Y”).
* **None** - cases are defined as not having a clear reasoning strategy if there are insufficient utterances to make an inference of strategy (as agreed by both coders).
\

We first code specific statements within each case that suggested one of these strategies, and then determined which strategy was most prevalent or influential for cases as a whole such that each case was categorised under one of these strategies. Coding of utterances and case-wise reasoning strategies were conducted with a second independent coder. For reasoning strategies, initial interrater reliability was low, with both coders agreeing on 58.3% of cases. Conflict resolution led to changes made to the coding criteria by prioritising strategies used early in a case, as some participants were noted to utilise multiple strategies within a single case. The coding criteria was also changed to allow cases to be coded as not having a clear strategy due to a lack of utterances. The coding criteria provided above is after these changes were made. Cases were then independently coded for a second time with these updated criteria. Both coders agreed on 78% of cases when coding for correctness, with conflicts resolved in consultation with a member of expert panel used to develop the vignettes (as mentioned in Study 2). 
\

We hypothesise not only that this think-aloud methodology can be used to detect different reasoning strategies but also that usage of these reasoning strategies will vary. There are two possible ways in which they vary: they may vary as per the individual medical student and/or they may vary as a function of the specific case/patient condition being treated. For the former, reasoning strategy would be more related to the individual medical student, in that each student will have their own strategy that they tend to use regardless of the patient. For the latter, there would be properties of specific cases that prompt usage of certain reasoning strategies. We investigate both of these competing theories within this study. To look at individual-level strategy, we ask participants about their diagnostic reasoning process during the debrief interview. This includes questions such as _"What's your general approach to making diagnoses?"_ and _"Do you tend to keep a broad set of differentials in mind?"_ (full list of questions available in the Appendices). Based on their responses, participants are each categorised as belonging to one of the three reasoning strategies. This is considered their 'subjective strategy'. To look at condition-level strategies, after classifying each case using independent coders, we find the most commonly used strategy for each condition. This is considered the condition's 'dominant strategy'. Once both of these are defined, we compare the strategies coded for each case against both the subjective and dominant strategies. By comparing the cases' reasoning strategies with both the subjective and dominant strategies, we can investigate whether it is the individual medical student or the patient's medical condition that is responsible for the choice of reasoning strategy on a given case. We then compare our dependent variables (Accuracy, Confidence, Information Seeking, Differential Evaluations) as a function of reasoning strategy. 

#### Qualitative Thematic Analysis {.unnumbered}

The aim of this thematic analysis was to identify the reasoning strategies that medical students in this study reflectively report using in their medical practice, as well as understanding considerations made by students when making diagnoses. Similar to the think-aloud utterances by participants during the vignette task, we record and transcribe the responses given by participants during the debrief interviews (administered after the vignette task). With these interviews, we aimed to understand how participants report making diagnostic decisions, including how they seek information and weigh up differentials against each other. Based on these transcribed responses, we conducted a theory-driven semantic thematic analysis (as per definitions detailed by Braun and Clarke, 2006) to code utterances under specific categories. This kind of thematic analysis is suitable given that our qualitative data is from a structured interview with predefined research questions of interest, rather than a dataset with a looser structure. 

#### Recoding of Reasoning Strategies in Online Vignette Study {.unnumbered}

We aim to determine, given medical students' reasoning strategies, whether these strategies are associated with differences in information seeking and confidence. To improve the robustness of our findings, we not only look at quantitative differences by reasoning strategy within this study, but also using the previous study's dataset (where we have a larger sample size, as well as access to explicitly recorded differentials). This is important given that participants may not naturally verablise everything they are thinking of naturally, including the differentials they are considering but consider too remote to even verablise. In order to apply reasoning strategies to the data from Study 2, we train a classifier using penalised multinomial regression to classify cases as HD, PR or SI using the cases from the think aloud study (with Leave One Out Cross Validation). The input parameters for the classifier are the 29 pieces of information as binary predictors (similar to the approach depicted in Figure 8 of the previous study) and the cases’ condition. In other words, the cases from the think-aloud study make up the training data for the classifier whilst the cases from the larger online study make up the test dataset. The classifier was implemented using R’s nnet package (version 7.3-19). The testing data is then labelled with predicted strategies using R’s predict function. 
\

Based on these predicted reasoning strategies, we first test for differences in information seeking, differentials and confidence between the HD and PR cases in particular, as they represent relatively opposite approaches for making diagnoses (i.e broad vs narrow differentials respectively). We test for these differences using linear mixed effects models that control for both condition and participant as random effects, given that the samples from each group of cases are not independent (as participants contribute multiple data points). We also fit linear mixed effects model to cases belonging to each strategy to predict changes in confidence (i.e. the difference in reported confidence at the Patient History and Testing stages). For linear mixed effects with multiple random effects, we perform a PCA of the variance-covariance estimates to detect model overfitting using the rePCA function (Bates et al., 2015). We hypothesise that based on the reasoning strategy employed, reported confidence would be a readout of different aspects of the diagnostic process, be it the differentials considered or the information sought. These models are defined as follows (variables are defined in the Data Analysis section of the previous study):
\

**Confidence Change = Number of Initial Differentials + Proportion of Information Seeking + Change in Differentials**

_Random Effects: Participant, Patient Condition_

\
We also aim to determine how reasoning strategies affect diagnostic accuracy (defined as the likelihood assigned to the correct differential if included). We hypothesise that the efficacy of a reasoning strategy is dependent on how many differentials are generated based on the patient's history, given that reasoning strategies have differing focuses on broadening or narrowing the differentials considered. With a broad set of differentials generated from the patient history, clinicians would be expected to focus on narrowing their set in order to determine a more definitive diagnosis (via a PR approach), whilst they would be expected to broaden their differentials given fewer initial differentials in order to cover more possibilities (via a HD approach). To this end, we fit a linear model that predicts Accuracy by an interaction between the Number of Intial Differentials and Reasoning Strategy. This is because the initial number of diagnoses should, in theory, dictate the type of reasoning strategy that clinicians should use. 

## Results {.unnumbered}

### Descriptive Quantiative Results {.unnumbered}

#### Overall Performance and Calibration {.unnumbered}

```{r casewiseStatsTable, include=TRUE, echo=FALSE}

caseBreakdown <- TAData %>%
  group_by(Condition) %>%
  dplyr::summarise(Accuracy = round(mean(Correct),2),
                   `Final Confidence` = round((mean(finalConfidence)/100),2),
                    Difficulty = round(mean(subjectiveDifficulty,na.rm=T),1),
                   `Information Seeking` = round(mean(proportionOfInfo),2))

colnames(caseBreakdown)[1] <- "Case"

knitr::kable(caseBreakdown) %>% 
  kableExtra::kable_styling(latex_options="HOLD_position")
```
_Table 1: Table showing, by case, from left to right, average values across participants for Accuracy (the proportion of participants who mentioned a correct differential during the case), Final Confidence (reported at the Testing stage), Difficulty (as rated by participants at the end of the case on a scale of 1-10) and Information Seeking (the proportion of available information sought)._

Participants varied in how much they spoke during the study, uttering 1038-7730 words (M = 4194) across the scenarios. Part of this range is driven by participants repeating information they see during the task, but participants also varied in terms of how much they externalised their thought process. 
\

```{r calibrationtest, include=TRUE, echo=FALSE}

calibrationtest <- lmerTest::lmer(finalConfidence ~ Correct + (1|ID),data=TAData)
calibrationtest <- summary(calibrationtest)

```
\

When looking at accuracy (the proportion of cases where a correct differential was mentioned by the participant), accuracy was `r round(mean(caseBreakdown$Accuracy),2)` across all cases. This varied by condition, as can be seen above in Table 1. Similar to the previous study, we ask whether participants provided confidence judgements that were, on the whole, calibration to their objective accuracy. In order to investigate this for this study, we compare the final confidence reported by participants on cases when they were objective correct and when they were objectively incorrect. Participants would be considered calibrated if we found evidence of higher confidence when correct. Participants did indeed report higher confidence when correct (M = `r round(mean(TAData[TAData$Correct==1,]$finalConfidence),2)`, SD = `r round(sd(TAData[TAData$Correct==1,]$finalConfidence),2)`) compared to when they were incorrect (M = `r round(mean(TAData[TAData$Correct==0,]$finalConfidence),2)`, SD = `r round(sd(TAData[TAData$Correct==0,]$finalConfidence),2)`). Given that the samples from each of these groups are not independent, we test for a difference between these groups using a mixed effects model that predicts final confidence using the correctness of the case as a fixed effect and the individual participant as a random effect. We find evidence that the case correctness was predictive of final confidence ($\beta$ = `r round(calibrationtest$coefficients[2],2)`, SE = `r round(calibrationtest$coefficients[,"Std. Error"][2],2)`, t = `r round(calibrationtest$coefficients[,"t value"][2],2)`, p = `r round(calibrationtest$coefficients[,"Pr(>|t|)"][2],2)`), indicating that participants provided confidence judgements that were well calibrated to their objective accuracy. 

#### Differential Evaluations {.unnumbered}

```{r diffStatsTable, include=TRUE, echo=FALSE}

diffBreakdown <- TAData %>%
  group_by(Condition) %>%
  dplyr::summarise(`Differential Added` = round(mean(DE.DAs),2),
                  `Differential Removed` = round(mean(DE.DRs),2),
                   `Increased Likelihood` = round(mean(DE.IL),2),
                   `Decreased Likelihood` = round(mean(DE.DL),2),
                  `Total Differential Evaluations` = round(mean(DEs),2))

colnames(diffBreakdown)[1] <- "Case"

diffBreakdownShort <- diffBreakdown
colnames(diffBreakdownShort) <- c("Case","DA","DR","IL","DL","TDE")

knitr::kable(diffBreakdown) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 2: Descriptive Statistics for subcodes within the Differential Evaluation main code as detailed above in the Data Analysis section. Shown above are mean values for the number of instances/utterances for each of the following subcodes (from left to right): a new differential being considered, a differential being removed from consideration, a differential being seen as more likely given a piece of information, a differential being seen as less likely given a piece of information, the average total of these subcodes. _


```{r diffbars, include=FALSE, message=FALSE, echo=FALSE, warning=FALSE}

pptData <- TAData %>%
  group_by(ID) %>%
  dplyr::summarise(meanDiffsAdded = mean(DE.DAs),
                   meanDiffsRemoved = mean(DE.DRs),
                   meanIncreasedLikelihoods = mean(DE.IL),
                   meanDecreasedLikelihoods = mean(DE.DL))

colnames(pptData) <- c("ID","meanDiffsAdded","meanDiffsRemoved","meanDecreasedLikelihoods","meanIncreasedLikelihoods")

ttest1 <- t.test(pptData$meanDiffsAdded,pptData$meanDiffsRemoved,paired=T)
ttest2 <- t.test(pptData$meanIncreasedLikelihoods,pptData$meanDecreasedLikelihoods,paired=T)

```
\

For utterances coded as Differential Evaluations, participants on average made `r round(mean(diffBreakdownShort$TDE),2)` such utterances per case. The mean number of Differential Evaluations was relatively constant by condition except for the AD case, for which we observed a higher amount of Differential Evaluations (see table 2 above). As previously mentioned, Differential Evaluations can be further categorised into one of four subcodes: Differential Added, Differential Removed, Likelihood Increased and Likelihood Decreased. As found in the previous study, there is a general reticence to disregard differentials completely. Participants expressed significantly more statements adding differentials (M = `r round(mean(TAData$DE.DAs),2)`, SD = `r round(sd(TAData$DE.DAs),2)`) than removing differentials (M = `r round(mean(TAData$DE.DRs),2)`, SD = `r round(sd(TAData$DE.DRs),2)`) (t(`r round(ttest1$parameter)`) = `r round(ttest1$statistic,2)`, MDiff = `r round(ttest1$estimate,2)`, p < .001). Participants expressed more statements of decreasing likelihoods (M = `r round(mean(TAData$DE.DL),2)`, SD = `r round(sd(TAData$DE.DL),2)`) rather than increasing likelihoods (M = `r round(mean(TAData$DE.IL),2)`, SD = `r round(sd(TAData$DE.IL),2)`) but we did not find evidence of a significant difference (t(`r round(ttest2$parameter)`) = `r round(ttest2$statistic,2)`, MDiff = `r round(ttest2$estimate,2)`, p = `r round(ttest2$p.value,2)`). 

\newpage

```{r diffbarsplot, include=TRUE, message=FALSE, echo=FALSE, warning=FALSE,out.width='100%', fig.align='center', fig.height=6}

acc <- c("DiffsAdded","DiffsRemoved")

means <- c(mean(pptData$meanDiffsAdded),
                mean(pptData$meanDiffsRemoved))
sds <- c(sd(pptData$meanDiffsAdded)/sqrt(nrow(pptData)),
              sd(pptData$meanDiffsRemoved)/sqrt(nrow(pptData)))

temp <-data.frame(acc, means,sds)

p <- ggplot(temp) +
  geom_bar(aes(x = acc, y = means,fill=acc),stat="identity",alpha=0.7) +
  scale_fill_manual(values=c("#69b3a2", "#404080")) +
  geom_errorbar(aes(x = acc,ymin=means-sds, ymax=means+sds), width=.2, position=position_dodge(0.05),color="orange") +
  labs(x="Strategy",y="Mean") +
  theme_classic() +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        line = element_blank())

acc <- c("Likelihood-","Likelihood+")

means <- c(mean(pptData$meanDecreasedLikelihoods),
                mean(pptData$meanIncreasedLikelihoods))
sds <- c(sd(pptData$meanDecreasedLikelihoods)/sqrt(nrow(pptData)),
              sd(pptData$meanIncreasedLikelihoods)/sqrt(nrow(pptData)))

temp <-data.frame(acc, means,sds)

p2 <- ggplot(temp) +
  geom_bar(aes(x = acc, y = means,fill=acc),stat="identity",alpha=0.7) +
  scale_fill_manual(values=c("#69b3e9", "#620a1a")) +
  geom_errorbar(aes(x = acc,ymin=means-sds, ymax=means+sds), width=.2, position=position_dodge(0.05),color="orange") +
  labs(x="Strategy",y="Mean") +
  theme_classic() +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        line = element_blank())

cow <- plot_grid(p,p2,ncol=2, align = "v", axis="1", labels=c('A','B'))
print(cow) #view the multi-panel figure  

```
_Figure 1: Bar graphs comparing incidences of each of the four subcodes within Differential Evaluations. We compare instances of differentials being added (green) and removed (purple) from consideration (Figure 1A) and compare instances of differentials decreasing (blue) and increasing (red) in likelihood (Figure 1B)._


### Reasoning Strategies {.unnumbered}

#### Incidence of Strategies {.unnumbered}

```{r stratppttable, warning=FALSE, message=FALSE, echo=FALSE}

stratppttable <- TAData %>%
  group_by(Condition, ID) %>%
  dplyr::summarise(Strategy = InterraterStrat) %>%
  pivot_wider(names_from = Condition, values_from = Strategy)

```

In Table 3 below, we show all `r nrow(TAData)` cases from the think-aloud strategy and the strategy coded to each after resolving all interrater conflicts. Of these cases, `r sum(stratppttable=="NONE")` were coded as not having a clear reasoning strategy due to both an insufficient amount of think-aloud utterances and no diagnostic differentials being mentioned. `r sum(stratppttable=="HD")` cases were coded as having a HD strategy, `r sum(stratppttable=="PR")` cases were assigned a PR strategy and `r sum(stratppttable=="SI")` cases were coded as SI. In Figure 2 below, we plot the proportion of cases for each patient condition that were categorised under each of the reasoning strategies. We note that the types of reasoning strategy used varies by condition (see Figure 2 above), with the MTB and TTP cases in particular exhibiting higher usage of PR than others, whilst HD was used by the majority of participants for the UC and AD cases in particular. 

```{r stratppttableshow, warning=FALSE, message=FALSE, echo=FALSE}

knitr::kable(stratppttable) %>% 
  row_spec(0,bold=TRUE) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 3: Table that shows the strategy coded for each case by participant (rows) and by patient condition (column) after resolving conflicts between both independent coders. Anonymised participant IDs are used._

```{r stratbreakdown, warning=FALSE, message=FALSE, echo=FALSE}

stratBreakdown <- TAData %>%
  group_by(Condition, InterraterStrat) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(m_N = mean(N)/16)

colnames(stratBreakdown) <- c("Condition","Strategy","m_N")

nCases <- nrow(TAData)
props <- c(nrow(TAData[TAData$InterraterStrat=="PR",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="HD",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="SI",])/nCases,
           nrow(TAData[TAData$InterraterStrat=="NONE",])/nCases)

temp <- data.frame(c("Overall","Overall","Overall","Overall"),
                   c("PR","HD","SI","NONE"),
                   props)
colnames(temp) <- c("Condition","Strategy","m_N")

stratBreakdown <- rbind(stratBreakdown,temp)

stratBreakdown$Strategy <- factor(stratBreakdown$Strategy, levels = c('HD', 'PR', 'SI', 'NONE'))

p <- ggplot(stratBreakdown, aes(fill=Strategy, y=m_N, x=Condition)) + 
    geom_bar(position="dodge", stat="identity",alpha=0.7) +
    scale_fill_manual(values=c("#E69F00", "#56B4E9", "#009E73",  "grey")) +
  scale_x_discrete(limits = c("UC","GBS","TTP","AD","TA","MTB","Overall")) +
  labs(x="Case (Decreasing Accuracy)",y="Proportion of Participants") +
  theme_minimal() +
    theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        plot.title=element_text(size=18,face="bold"),
        line = element_blank())

print(p)

```
_Figure 2: Proportion of participants who use each type of reasoning strategy for each condition/case, with the overall proportions across all cases shown by the rightmost bars. The strategies shown are: Hypothetico-Deductive (where multiple differentials are considered simultaneously, orange), Pattern Recognition (where a single differential is considered in turn, blue), Scheme-Inductive (where participants evaluate pathophysiological systems as causes of patients rather than specific conditions, green) and None (for cases where a clear differential is not mentioned or if there are not enough utterances to infer a clear strategy, grey)._

\newpage

#### Reasoning Strategies' Effect on Dependent Variables {.unnumbered}

```{r strattable, warning=FALSE, message=FALSE, echo=FALSE}

strattable <- TAData %>%
  group_by(InterraterStrat) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(N = mean(N),
                    Accuracy = round(mean(Correct),2),
                   `Differential Evaluations` = round(mean(DEs),2), 
                   `Information Seeking` = round(mean(proportionOfInfo),2),
                   `Confidence Change` = round(mean(confidenceChange)/100,2))

colnames(strattable)[1] <- c("Strategy")

strattable <- strattable[order(strattable$N, decreasing = TRUE), , drop = FALSE]

knitr::kable(strattable) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

colnames(strattable) <- c("Strategy","N","Accuracy","DiffEval", "InfoSeeking","ConfidenceChange")

```
_Table 4: Mean values for dependent variables broken down by the reasoning strategy coded after resolving conflicts between the two independent coders. From left to right, Accuracy refers to the proportion of cases where a correct differential was mentioned. Differential Evaluations refers to the number of coded utterances under one of the subcodes (Differential Added, Differential Removed, Increased Likelihood, Decreased Likelihood). Information Seeking refers to the proportion of available information sought across cases. Confidence Change refers to difference between initial confidence and final confidence._
\

Next we look at our coding of reasoning strategies at a case level (see Table 4 above). Accuracy was higher for cases coded as Hypothetico-Deductive (`r strattable[strattable$Strategy=="HD",]$Accuracy`) compared to both Pattern Recognition cases (`r strattable[strattable$Strategy=="PR",]$Accuracy`) and Scheme Inductive (`r strattable[strattable$Strategy=="SI",]$Accuracy`). On cases with a SI strategy, participants gained more confidence over the case (`r strattable[strattable$Strategy=="SI",]$ConfidenceChange`) when compared to PR (`r strattable[strattable$Strategy=="PR",]$ConfidenceChange`) and HD cases (`r strattable[strattable$Strategy=="HD",]$ConfidenceChange`). Participants evaluated differentials more often during HD cases (`r strattable[strattable$Strategy=="HD",]$DiffEval`) when compared to other strategies. Information seeking was fairly consistent across reasoning strategies. 

#### Dominant Reasoning Strategies {.unnumbered}

```{r dominantstrats, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

TAData$caseDominantStrat <- ifelse(TAData$Condition %in% c("AD","UC","GBS"),"HD","PR")
TAData$matchingCaseDominantStrat <- ifelse(TAData$caseDominantStrat==TAData$InterraterStrat,1,0)

dominantStrat <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(caseDominantStrat,matchingCaseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2),                     
              `Differential Evaluations` = round(mean(DEs),2), 
              `Information Seeking` = round(mean(proportionOfInfo),2),
              `Confidence Change` = round(mean(confidenceChange)/100,2))

model <- summary(lme4::glmer(Correct ~ matchingCaseDominantStrat + (1|ID), data = TAData[TAData$InterraterStrat!="NONE",], family = binomial))

dominantStratCompare <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(matchingCaseDominantStrat) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2)) 

```

We aim now to establish if participants are more accurate when using each condition's dominant strategy. This is based on the assumption that each medical condition has an 'optimal' reasoning strategy that should be used to maximise accuracy. We first categorise each of the 6 cases as having a ‘dominant’ reasoning strategy based on which was utilised the most across participants. Through this process, we categorise three conditions as HD (AD, GBS, UC), three conditions as PR (MTB, TTP, TA, we note that there was an equal number of PR and SI cases for TA condition, but we use PR as its dominant strategy to easily compare HD and PR directly). HD was assigned to `r round(stratBreakdown[stratBreakdown$Condition=="AD"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of AD cases, `r round(stratBreakdown[stratBreakdown$Condition=="GBS"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of GBS cases, and `r round(stratBreakdown[stratBreakdown$Condition=="UC"&stratBreakdown$Strategy=="HD",]$m_N*100,2)`% of UC cases. PR was assigned to `r round(stratBreakdown[stratBreakdown$Condition=="MTB"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of MTB cases, `r round(stratBreakdown[stratBreakdown$Condition=="TTP"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of TTP cases, and `r round(stratBreakdown[stratBreakdown$Condition=="TA"&stratBreakdown$Strategy=="PR",]$m_N*100,2)`% of TA cases. Overall, participants matched the dominant strategy on `r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$N` cases (`r round(dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$N/sum(dominantStratCompare$N)*100,1)`% of cases, excluding those cases without a clear reasoning strategy). Accuracy was found to be higher for cases when participants matched the condition's dominant strategy (`r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==1,]$Accuracy`) compared to when they did not (`r dominantStratCompare[dominantStratCompare$matchingCaseDominantStrat==0,]$Accuracy`). However, this difference was not found to be significant via a mixed effects logistic regression (on accuracy as a binary outcome measure with participant as a random effect) ($\beta$ = `r round(model$coefficients[2],2)`, SE = `r round(model$coefficients[,"Std. Error"][2],2)` t = `r abs(round(model$coefficients[,"z value"][2],2))`, p = `r round(model$coefficients[,"Pr(>|z|)"][2],2)`). 

```{r dominantstratstable, include = TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(dominantStrat)[1] <- "Dominant Strategy"
colnames(dominantStrat)[2] <- "Matching Dominant Strategy"

dominantStrat[,2] <- as.character(dominantStrat[,2])
dominantStrat[1,2] <- "No"
dominantStrat[2,2] <- "Yes"
dominantStrat[3,2] <- "No"
dominantStrat[4,2] <- "Yes"

knitr::kable(dominantStrat) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))


```
_Table 5: Table showing average accuracy values by cases where the participants used or did not use the dominant reasoning strategy for that case. Dominant strategies are decided based on which of the reasoning strategies was utilised by the majority of participants in the think-aloud study. Cases without a coded reasoning strategy are excluded from this table. The first column refers to the dominant strategy for that condition, whilst the second column refers to whether the cases' coded strategy matches the condition's dominant strategy._

#### Subjective Reasoning Strategies {.unnumbered}

```{r subjstrategies, include = FALSE,warning=FALSE, message=FALSE, echo=FALSE}

subjstrategies <- as.data.frame(read.csv("./assets/TASubjectiveCodes.csv",header=TRUE))

colnames(subjstrategies)[1] <- "ID"

colnames(subjstrategies)[3] <- "SubjectiveStrategy"

TAData <- merge(TAData, subjstrategies[, c("ID","SubjectiveStrategy")], by = "ID", all.x = TRUE)

TAData$SubjectiveStrategyMatched <- ifelse(TAData$SubjectiveStrategy==TAData$InterraterStrat,1,0)

subjectiveMatchedTable <- TAData[TAData$InterraterStrat!="NONE",] %>%
              group_by(SubjectiveStrategyMatched) %>%
              dplyr::mutate(N = n()) %>%
              dplyr::summarise(N = mean(N),
              Accuracy = round(mean(Correct),2),                     
              `Differential Evaluations` = round(mean(DEs),2), 
              `Information Seeking` = round(mean(proportionOfInfo),2),
              `Confidence Change` = round(mean(confidenceChange)/100,2))


model <- summary(lme4::glmer(Correct ~ SubjectiveStrategyMatched + (1|ID), data = TAData[TAData$InterraterStrat!="NONE",], family = binomial))

```

In addition to reasoning strategies being coded based on the participants' think-aloud utterances, we also asked participants about their diagnostic process during the debrief interviews that can be used to infer the reasoning strategies participants think they use in their regular medical practice. We use this to determine if participants are more accurate when using their subjectively preferred reasoning strategy. In Table 6 below, we categorise participants based on their subjective reflection of their diagnostic process. Through this process, we categorise `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="HD",])` participants under a HD reasoning strategy, `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="PR",])` participants as PR and `r nrow(subjstrategies[subjstrategies$SubjectiveStrategy=="SI",])` participants as SI. Given these categorisations of reasoning strategy based on subjective reflection by participants, we compare these participant-level strategies to the case-level strategies assigned by our independent coders. We find that there are `r sum(TAData$SubjectiveStrategyMatched)` cases (`r round((sum(TAData$SubjectiveStrategyMatched)/nrow(TAData[TAData$InterraterStrat!="NONE",]))*100,2)`%, excluding cases without a coded reasoning strategy) where participants match the reasoning strategy during the case to their subjectively defined strategy that they tend to use for diagnostic decisions. Accuracy was found to be higher for cases when participants matched their subjective strategy (`r subjectiveMatchedTable[subjectiveMatchedTable$SubjectiveStrategyMatched==1,]$Accuracy`) compared to when they did not (`r subjectiveMatchedTable[subjectiveMatchedTable$SubjectiveStrategyMatched==0,]$Accuracy`) (see Table 5 below). However, this difference was not found to be significant via a mixed effects logistic regression (with accuracy as a binary outcome measure and participant as a random effect) ($\beta$ = `r round(model$coefficients[2],2)`, SE = `r round(model$coefficients[,"Std. Error"][2],2)` t = `r abs(round(model$coefficients[,"z value"][2],2))`, p = `r round(model$coefficients[,"Pr(>|z|)"][2],2)`). 
\

```{r subjstrategiesmatched, include = TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(subjectiveMatchedTable)[1] <- "Matched to Subjective Strategy"

subjectiveMatchedTable[,1] <- as.character(subjectiveMatchedTable[,1])
subjectiveMatchedTable[1,1] <- "No"
subjectiveMatchedTable[2,1] <- "Yes"

knitr::kable(subjectiveMatchedTable) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))

```
_Table 6: Dependent variables by cases where the reasoning strategy used (as categorised by the independent coders) matches the subjective strategy coded for that participant (as per responses to the debrief interview, see table 7 below)._

```{r subjstrategiestable, include=TRUE,warning=FALSE, message=FALSE, echo=FALSE}

colnames(subjstrategies) <- c("Participant","Full Quote","Coded Strategy","Condensate","Interpretation")

knitr::kable(subjstrategies, longtable = TRUE) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","repeat_header")) %>%
  column_spec(1, width = "5em") %>%
  column_spec(2, width = "13em") %>%
  column_spec(3, width = "4em") %>%
  column_spec(4, width = "6em") %>%
  column_spec(5, width = "6em") 

```
_Table 7: Categorisation of participants under one of three possible reasoning strategies based on their responses during the debrief interview. We capture here the subjective reasoning strategy for each participant based on how they reflect on how tend to make diagnostic decisions. In the second column are key highlighted quotes related to each of the participants' diagnostic decision making process. In the fourth column, we provide our summary of the quote and then in the fifth column, our interpretation of the quote that explains the choice of reasoning strategy for that participant._

\

### Thematic Analysis from Debrief Questionnaire {.unnumbered}

In this section, we present key themes from the thematic analysis of participant responses to the debrief questionnaire. The questionnaire was designed to ask participants how they think they tend to make diagnostic decisions and what their main considerations are during the decisional process. We provide quotes from participants belonging to each of these themes. Participants are referred to by their anonymised identifiers. 

#### Avoidance of Anchoring {.unnumbered}

A key consideration, as mentioned by six participants, was the concept of anchor bias. This was explained by one of these participants as follows:
\

_"I'm quite aware that there's, I've tried to remember what it's called, I think it's called anchor bias where you have, you can leap onto one thing early on, and then you want other things to fit that. I think we are all vulnerable to it to an extent. And we will look for things that support our initial idea, but I try and keep an open mind." (k5376h)_
\

These participants showed awareness of this phenomenon, whereby clinicians may focus too early on a particular diagnosis and then seek information to confirm this existing belief (a form of confirmation bias). This can then prevent participants from consideration alternative diagnoses early on in their decisional process. Given their awareness of this bias and its pitfalls, we can then infer that participants approach their diagnoses in such a way as to avoid this bias. Other participants cited this as a consideration of theirs when making diagnoses:
\

_"I'm quite rubbish, I often get fixated like 'I think this is this'’'...but I'm not that good at thinking, 'oh, what else could it be' into like, 'I've got something that's proved to me it's not.'" (4khzxs)_

_"I try to, but I think my brain can sometimes get stuck on an idea. And it's difficult to pull away from that." (gdq7tc)_

_"I think I probably do think about that the whole way through, which probably can be beneficial, but can also sometimes hold me back from looking at other options" (593ybw)_
\

One reason cited for such a bias to occur is that medical students are relatively early into their medical experience. As a result, they have not developed as much medical knowledge as experienced clinicians and may focus on diagnoses that they have more familiarity with. Medical students seem to also report making a conscious effort to keep an open mind with regards to alternative differentials:
\

_"But then I do think I, at this stage, I'm quite kind of biased towards what I know more about, if that makes sense. So, I think the things which I don't know about, I'm just hoping it’s not that." (3lkzjq)_

_"I think I try to keep an open mind perhaps because I'm just like, the student and I don't have as much knowledge, as someone who's been training for a long time." (dcjymb)_

_"My knowledge isn't broad enough...to remember all the differentials for everything." (rslkq8)_

This is important to note for three reasons. Firstly, medical students take their relative inexperience into account as a factor when making diagnoses. This could then mean that as medical students become intermediate/experienced clinicians, their decision making style may change to reflect their increased medical knowledge. Secondly, medical students may be more likely to express uncertainty if there are more diagnoses/conditions that they are unfamiliar with due to their lack of knowledge. Thirdly, the awareness shown for the relative inexperience of medical students indicates that students would be less likely to tend toward overconfidence, given this sense of 'humility' about what knowledge they have and do not have. Taken together, medical students are likely to approach medical decisions very differently from experienced clinicians mainly because they have different perceptions of their own medical knowledge. 

#### Standard Tendencies {.unnumbered}

Participants reflected a few general tendencies (or rules of thumb) when making diagnoses. Firstly, seven participants mentioned that they prioritised any serious/emergency differentials early on when making diagnoses, which would affect the urgency with which they would approach ruling these differentials out. This suggests that some focus would be taken away from determining likely diagnoses and instead ruling out more serious diagnoses that would require more immediate medical attention. This also indicates that the manner in which medical students approach diagnoses is dependent on the nature of the patient being treated and whether serious diagnoses are being considered. 
\

_"I do have an approach, the first (thing) I always want to think is if I miss something, is this patient gonna be bad? So, like, thinking about emergency stuff." (4khzxs)_

_"I think probably, especially as a medical student, we get taught to rule out red flag stuff...a lot of my thinking is like, what really worrying thing could this be that we need to rule out? And what tests do I need to rule it out?" (5lvg8j)_

_"If it's...an acute versus a non-acute thing, I think that would change the pace I approach it." (dcjymb)_

_"Like, if someone's coming in with a presentation that could be quite urgent and serious, then obviously you want to rule out like a stroke, you want to rule that out quite quickly." (gdq7tc)_

_"If I think something's remotely possible, that's really like say, so like for GBS, I'm even thinking about Cauda Equina syndrome, like, regardless of how high my index of suspicion for it is, even if it's pretty low. If it's an urgent diagnosis, I'll just do it anyway." (gs6zbl)_

_"I'm trying to rule out the most serious things." (rslkq8)_

_"And essentially if there’s any serious conditions, I make sure to rule those out...and then go from there. I probably should go through each one and weigh each one individually. Because that would avoid being as biased. But it’s not something I do as much as I should...The other big things, are there any of the red flag symptoms that are really important that should influence what I’m thinking? Like fevers especially, that sort of thing...So generally, acute situations are where I narrow a little bit." (y86m2n)_

\
Another tendency was for participants to report a form of progressive investigation that stems from the patient's history. In this sense, participants report a decision process that quite closely matches our experimental procedure of gradually seeking information based on patient's medical history to build up a picture of them. This illustrates the importance of a comprehensive medical history for the patient being available and how much it guides medical students' decisional process. We can also ascertain from this theme that the initial diagnostic differentials generated from the patient's history has a large influence on the subsequent diagnostic process: 
\

_"Definitely start like history...I think to go from there and like, kind of think about that in the context of the patient. Yeah, I feel I've definitely been taught in terms like that methodical, like do it in that order." (593ybw)_

_"I guess going through like a system of starting with the history and sort of gathering as much information as I can there and thinking already what I think might be happening. And then examining them and seeing if that sort of changed anything, but then sort of getting investigations." (5lvg8j)_

_"You can get a lot from the history. So I think sort of, I guess my general approach is like, take your history, and then from the history, have a little, it's not like, if you wrote it down, it'd be like a little bubble, like brainstorming thing as...the key big differentials I'm considering." (clhtyq)_

_"But if it was a patient...who had sort of not very clear symptoms, but wanted to be a bit more thorough, like take a history first and then looking at any test they've had, starting with like more basic tests like observations blood tests, and then and then, depending on the cause, or the symptoms, doing more invasive tests, perhaps." (dcjymb)_

_"Take a history, like detailed history, formulate my top differentials. And then basically, look at investigations and examinations to confirm or rule out these differentials." (l3jd8r)_ 

_"I think, start, think systematically. So start with a thorough history, asking kind of about what's happening currently, and then going through the kind of past medical history and focusing on that, asking what the patient thinks might be going on. And then focus on a thorough examination which sometimes for the interest of time is focused on the, the kind of symptom at hand, but you should do a kind of formal full checkthrough as well...see if there's anything that points you towards a diagnosis. I think it (the experiment) was set up in the way that I go about things in the way that you do the history first, you do the examination, you do the investigations." (ly9kzg)_

_"Just sort of work through from the most like, basic things like history and examination, least invasive tests, and then try to work up from there." (rslkq8)_

_"So probably getting history, I look at initial observations first...And then I look at ECG as well, then I want to get initial bloods being guided by what I think could be going on and then think about potential images." (ytpshg)_
\

Within this process of progressive investigation, six participants (including the quote above from participant ytpshg) noted that there are pieces of information or tests that they would seek for all patients (regardless of their condition) as part of a routine diagnostic approach. This indicates that some aspects of the diagnostic process are seen as fairly standardised by medical students:
\

_"I would always want to do like full blood count, VBG… Probably, as I said, I think like most people in the emergency department get a chest X ray" (4khzxs)_

_"With the examination, I think, normally, I would like...auscultating the heart and feeling the heart, abdomen, etc. These are things I think I would do in any patient, irrespective." (5lvg8j)_

_"And then for investigations...I'll take all the bloods, do an ECG, chest X Ray, just in case. Yeah, yeah. So I am a bit more like, on the side of caution." (d9b1qf)_

_"I think if you went to your senior and you said I'm really concerned about this patient, but I've not done an FBC, a U&E, an ECG, VBG...They'd be like, what are you on about? So there's a few that you would do anyway, that are largely non invasive, in terms of...higher degree investigations, very much depends on anatomically, what you're seeing, what your differential is." (k5376h)_

_"I wasn't sure (during the experiment) whether I should try and be very focused to the presentation at hand...because for example, when I was going through the examinations, in reality, I would do a full exam on someone, even if they presented with something that was very specific, like a very specific symptom just so that you can have a full kind of clerking assessment." (ly9kzg)_

#### Challenges of the Diagnostic Process {.unnumbered}

Participants cited a number of challenges related to our diagnostic task. These were related to ways in which our study did not emulate real-world aspects of the diagnostic process. Firstly, three participants noted that it was difficult to retain all of the information they needed during the task:
\

_"I kept thinking that I couldn't like hold onto all of the information." (4khzxs)_

_"I'm quite a visual person. So...reading on a screen is quite different to I don't know, actually having seen a patient, seeing the exam findings, or even looking at the scans myself. I feel like I find that easier to retain the information. Whereas when it's, I find it, it's kind of hard to take it in when it's like just written down" (593ybw)_

_"I think just thinking on the spot and coming up with the diagnosis quite quickly, it's quite hard remembering the management afterwards as well." (rslkq8)_
\

Four participants noted that, in their real medical practice, they would be consulting other doctors, frameworks or online resources, which made our task difficult given that these were not available to participants:
\

_"I'd find as much information as possible and to ask for help...I think I'm someone who looks up stuff a lot. Like I rely a lot on looking up things. And that gives me a lot of comfort. I feel like when I don't have those tools, yeah, I feel a bit shaky...And I often kind of just take my phone out and look at that, and even just glancing at them kind of helps me structure my thoughts. So, yeah, I would wish it would be kind of more, more of an organic process. But at the moment, I think I rely quite a lot on prompts and things like that, or guidelines, even if I don't read them thoroughly, I'd need kind of reminders, especially when I feel like there's so much that it could be and I lose myself a little bit in the possibilities." (3lkzjq)_

_"I guess in real life, I also have, like, Google. So at points where I forgot the disease, or like, what is the first line, I would have like checked before, before typing up my management plan...when I'm confused, I’m definitely going to approach the senior. So I wouldn't be the one making the diagnostic decision. So I think it's a bit harder to do it alone." (d9b1qf)_

_"When I'm not sure, I'll definitely running my my train of thought past my consultants." (l3jd8r)_

_"I think it's weird doing it in isolation. Because I guess in an actual clinical setting, you kind of bounce ideas off someone else." (ytpshg)_

### Reasoning Strategies in Study 2 {.unnumbered}

#### Coded Strategies {.unnumbered}

```{r stratClassifier, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE, fig.height=7}

trainingDataStrats <- infoSeekingMatrixTA
testingDataStrats <- infoSeekingFullMatrix[infoSeekingFullMatrix$ParticipantType=="p",]

testingDataStrats$initialDiagnoses <- studentCaseDf$initialDifferentials
testingDataStrats$confidenceChange <- studentCaseDf$confidenceChange
testingDataStrats$differentialChange <- studentCaseDf$differentialChange
testingDataStrats$likelihoodAcc <- studentCaseDf$likelihoodOfCorrectDiagnosis

trainingDataStrats <- trainingDataStrats[trainingDataStrats$Strat!="NONE",]

trainingDataStrats$CaseLabel <- as.factor(trainingDataStrats$Strat)

colnames(trainingDataStrats)[1:29] <- c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7",  
                                      "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                      "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")

colnames(testingDataStrats)[1:29] <- c("T1", "T2",  "T3",  "T4",  "T5",  "T6",  "T7",  
                                        "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                        "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22","T23", "T24", "T25", "T26", "T27", "T28", "T29")


thresh<-seq(0,1,0.001)
#specify the cross-validation method
ctrl <- trainControl(method = "repeatedcv", repeats = 5, number=10, savePredictions = TRUE, summaryFunction = multiClassSummary)

set.seed(101)

trainingDataStrats$stratLabel <- as.factor(trainingDataStrats$Strat)

# shuffle rows in case of order effect
trainingDataStrats <- trainingDataStrats[sample(1:nrow(trainingDataStrats)), ]

model <- caret::train(stratLabel ~ T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 + T10 +
                    T11 + T12 + T13 + T14 + T15 + T16 + T17 +  T18 + T19 + T20 +
                    T21 + T22 + T23 + T24 + T25 + T26 + T27 + T28 + T29, method = "glmnet", family = "multinomial", data = trainingDataStrats, trControl = ctrl, tuneGrid = expand.grid(alpha = 0:1,  # Alpha 0 = ridge, Alpha 1 = lasso
                                      lambda = seq(0.01, 1, by = 0.01)))

predictions <- predict(model, newdata = trainingDataStrats)
trainingDataStrats$predictedStrat <- predictions
success <- sum(trainingDataStrats$predictedStrat==trainingDataStrats$stratLabel)/nrow(trainingDataStrats)
confusion <- table(predictions, trainingDataStrats$stratLabel)

# Make predictions
predictions <- predict(model, newdata = testingDataStrats)

testingDataStrats$classifiedStrat <- predictions

testingDataStrats$infoAmount <- rowSums(testingDataStrats[,c(1:29)])/29

testingDataStrats$caseDominantStrat <- ifelse(testingDataStrats$Condition %in% c("AD","GBS","UC"),"HD","PR")

testingDataStrats$usingCaseDominantStrat <- ifelse(testingDataStrats$caseDominantStrat==testingDataStrats$classifiedStrat,1,0)

########################
# Add value

infoValueDf <- infoSeekingFullMatrix[,c(1:29)]
colnames(infoValueDf)[1:29] <- c("T1","T2","T3","T4","T5","T6","T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                                              "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22", 
                                              "T23", "T24", "T25", "T26", "T27", "T28", "T29")

infoValueDf$Correct <- infoSeekingFullMatrix$Correct
infoValueDf$Condition <- infoSeekingFullMatrix$Condition
infoValueDf$ID <- infoSeekingFullMatrix$ID


temp <- infoSeekingFullMatrix[,c(1:29)]
colnames(temp)[1:29] <- c("T1","T2","T3","T4","T5","T6","T7", "T8",  "T9", "T10", "T11", "T12", "T13", "T14", 
                          "T15", "T16", "T17", "T18", "T19", "T20", "T21", "T22", 
                          "T23", "T24", "T25", "T26", "T27", "T28", "T29")

temp$Condition <- infoSeekingFullMatrix$Condition
temp$ID <- infoSeekingFullMatrix$ID

temp <- temp[!grepl("e1|e2|e3|e4|e5|e6|e7", rownames(temp)),]

standard <- "student" #student/expert
if (standard == "student")
{
  infoValueDf <- infoValueDf[!grepl("e1|e2|e3|e4|e5|e6|e7", rownames(infoValueDf)),]
} else
{
  infoValueDf <- infoValueDf[grepl("e1|e2|e3|e4|e5|e6|e7", rownames(infoValueDf)),]
}


for (n in 1:nrow(temp)) #row
{
  for (m in 1:29) #column
  {
    accSet <- c()
    currentID <- temp[n,]$ID # cross validation
    infoSelectCase <- infoValueDf[infoValueDf$Condition==temp[n,]$Condition,]
    infoSelect <- infoSelectCase[,m]
    infoSelect <- as.data.frame(infoSelect)
    infoSelect <- cbind(infoSelect,infoSelectCase$ID)
    infoSelect <- cbind(infoSelect,infoSelectCase$Correct)
    colnames(infoSelect) <- c("Info","ID","Correct")
    infoSelect <- infoSelect[infoSelect$ID!=currentID,]
    infoSelect <- infoSelect[, !(colnames(infoSelect) %in% c("ID"))] 
    accPresent <- mean(infoSelect[infoSelect$Info==1,]$Correct,na.rm=TRUE)
    accNotPresent <- mean(infoSelect[infoSelect$Info==0,]$Correct,na.rm=TRUE)
    if (nrow(infoSelect[infoSelect$Info==0,]) > 1)
    {
      temp[n,m] <- ifelse(temp[n,m]==1,accPresent-accNotPresent,NA)
      if (is.nan(temp[n,m]))
      {
        temp[n,m] <- 0
      }
    }
  }
}
temp = subset(temp, select = -c(Condition,ID))


#temp$infoValue <- rowMeans(temp,na.rm = TRUE)
temp$infoValue <- rowSums(temp,na.rm = TRUE)


testingDataStrats$value <- temp$infoValue

```

```{r stratClassifierNullShuffle, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE}

nShuffles <- 20

trainingDataStratsShuffle <- trainingDataStrats
trainingDataStratsShuffle$nullStrat <- sample(trainingDataStratsShuffle$stratLabel)

accs <- c()

for (i in 1:nShuffles)
{
  trainingDataStratsShuffle$nullStrat <- sample(trainingDataStratsShuffle$nullStrat)
  
  model <- caret::train(nullStrat ~ T2 + T3 + T4 + T5 + T6 + T7 + T8 + T9 + T10 +
                    T11 + T12 + T13 + T14 + T15 + T16 + T17 +  T18 + T19 + T20 +
                    T21 + T22 + T23 + T24 + T25 + T26 + T27 + T28 + T29, method = "glmnet", family = "multinomial",  data = trainingDataStratsShuffle, trControl = ctrl, tuneGrid = expand.grid(alpha = 0:1,  # Alpha 0 = ridge, Alpha 1 = lasso
                                      lambda = seq(0.01, 1, by = 0.01)))

  predictions <- predict(model, newdata = trainingDataStratsShuffle)
  trainingDataStratsShuffle$predictedStratNull <- predictions
  success <- sum(trainingDataStratsShuffle$predictedStratNull==trainingDataStratsShuffle$nullStrat)/nrow(trainingDataStratsShuffle)
  
  accs[i] <- success
  
}

nullMean <- mean(accs)


```

```{r testingpptbreakdown, include=TRUE, echo=FALSE, warning = FALSE, message=FALSE, out.width='100%', fig.align='center'}

testingPptBreakdown <- testingDataStrats %>%
  group_by(classifiedStrat) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(`Number of Cases` = mean(N),
                   Accuracy = round(mean(likelihoodAcc/10),2),
                   `Information Amount` = round(mean(infoAmount),2),
                   `Information Value` = round(mean(value),2),
                   `Change in Confidence` = round(mean(confidenceChange/100),2),
                   `Initial Diagnoses` = round(mean(initialDiagnoses),2),
                   `Change in Diagnoses` = round(mean(differentialChange),2))

colnames(testingPptBreakdown)[1] <- "Strategy"

knitr::kable(testingPptBreakdown) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))   

```
_Table 8: Dependent variables in the Study 2 dataset broken down by the reasoning strategy assigned to each case via a multinomial classifier._

```{r stratcomparisons, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE}

accTest <- lmerTest::lmer(likelihoodAcc ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats)
accTest <- summary(accTest)

conTest <- lmerTest::lmer(confidenceChange ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats)
conTest <- summary(conTest)

diffTest <- lmerTest::lmer(initialDiagnoses ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats)
diffTest <- summary(diffTest)

infoTest <- lmerTest::lmer(infoAmount ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats)
infoTest <- summary(infoTest)

valTest <- lmerTest::lmer(value ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats)
valTest <- summary(valTest)

```

We show a table of cases from Study 2 by their coded reasoning strategy in Table 8. We trained our multinomial classifier using all cases with a coded reasoning strategy in the think-aloud study. When comparing predicted strategies from our model with the actual strategies assigned by the independent coders, our model had an accuracy of `r round(success*100,2)`%, which is higher than the chance level of 33% accuracy (given that there are three possible reasoning strategies that could be assigned). This indicates that our model is accurate at identifying reasoning strategies in the training data. 
\

We now look to compare our key dependent variables by strategy, in particular comparing PR and HD cases. We observed higher accuracy for HD cases (M = `r round(mean(testingDataStrats[testingDataStrats$classifiedStrat=="HD",]$likelihoodAcc/10),2)`, SD = `r round(sd(testingDataStrats[testingDataStrats$classifiedStrat=="HD",]$likelihoodAcc/10),2)`) on average when compared to PR cases (M = `r round(mean(testingDataStrats[testingDataStrats$classifiedStrat=="PR",]$likelihoodAcc/10),2)`, SD = `r round(sd(testingDataStrats[testingDataStrats$classifiedStrat=="PR",]$likelihoodAcc/10),2)`) and SI cases (M = `r round(mean(testingDataStrats[testingDataStrats$classifiedStrat=="SI",]$likelihoodAcc/10),2)`, SD = `r round(sd(testingDataStrats[testingDataStrats$classifiedStrat=="SI",]$likelihoodAcc/10),2)`). We observe evidence for an effect of reasoning strategy on accuracy ($\beta$ = `r abs(round(accTest$coefficients[2],2))`, SE = `r round(accTest$coefficients[,"Std. Error"][2],2)` t = `r abs(round(accTest$coefficients[,"t value"][2],2))`, p = `r round(accTest$coefficients[,"Pr(>|t|)"][2],2)`). In line with our expectations based on our definitions of the reasoning strategies, we observed more initial differentials were considered during HD cases (M = `r round(mean(testingDataStrats[testingDataStrats$classifiedStrat=="HD",]$initialDiagnoses),2)`, SD = `r round(sd(testingDataStrats[testingDataStrats$classifiedStrat=="HD",]$initialDiagnoses),2)`) relative to PR cases (M = `r round(mean(testingDataStrats[testingDataStrats$classifiedStrat=="PR",]$initialDiagnoses),2)`, SD = `r round(sd(testingDataStrats[testingDataStrats$classifiedStrat=="PR",]$initialDiagnoses),2)`). Change in confidence was not significantly different between PR and HD cases (p = `r round(conTest$coefficients[,"Pr(>|t|)"][2],2)`) and neither was the amount of information seeking (p = `r round(infoTest$coefficients[,"Pr(>|t|)"][2],2)`).

#### Predictors of Confidence {.unnumbered}

```{r conlmms, include=FALSE, echo=FALSE, warning = FALSE, message=FALSE}

# HD
modelData <- testingDataStrats[testingDataStrats$classifiedStrat=="HD",]
model <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange + (1 | Condition) + (1 | ID), data=modelData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange + (1 | ID), data=modelData)
#anova(model2,model)
HDmodel <- summary(model)

# PR - initial diagnoses

modelData <- testingDataStrats[testingDataStrats$classifiedStrat=="PR",]
model <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange +  (1 | Condition) + (1 | ID), data=modelData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange +  (1 | Condition), data=modelData)
#anova(model2,model)
PRmodel <- summary(model2)

# SI - information seeking

modelData <- testingDataStrats[testingDataStrats$classifiedStrat=="SI",]
model <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange + (1 | Condition) + (1 | ID), data=modelData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(confidenceChange ~ initialDiagnoses + infoAmount + differentialChange + (1 | ID), data=modelData)
#anova(model2,model)
SImodel <- summary(model)

```

As discussed in the Data Analysis, we next look at predictors of changes in confidence within each strategy's cases. We fit a linear mixed effects model for cases categorised under each of the three reasoning strategies that predict Confidence Change using fixed effects of the Number of Initial Differentials, Change in Differentials and Amount of Information Seeking. We use participant and patient condition as random effects. We find that for HD cases, there was a marginal effect of Change in Differentials on Confidence Change ($\beta$ = `r abs(round(HDmodel$coefficients[4],2))`, SE = `r round(HDmodel$coefficients[,"Std. Error"][4],2)`, t = `r abs(round(HDmodel$coefficients[,"t value"][4],2))`, p = `r round(HDmodel$coefficients[,"Pr(>|t|)"][4],2)`). For PR cases, we find that the Number of Initial Differentials was predictive of Confidence Change ($\beta$ = `r abs(round(PRmodel$coefficients[2],2))`, SE = `r round(PRmodel$coefficients[,"Std. Error"][2],2)`, t = `r abs(round(PRmodel$coefficients[,"t value"][2],2))`, p = `r round(PRmodel$coefficients[,"Pr(>|t|)"][2],2)`). Finally, for SI cases, we find that the Amount of Information Seeking was predictive of Confidence Change ($\beta$ = `r abs(round(SImodel$coefficients[3],2))`, SE = `r round(SImodel$coefficients[,"Std. Error"][3],2)`, t = `r abs(round(SImodel$coefficients[,"t value"][3],2))`, p = `r round(SImodel$coefficients[,"Pr(>|t|)"][3],2)`). All other coefficients for these models were not significant (ps > .1).

#### Predictors of Accuracy {.unnumbered}

We next look at reasoning strategy as a predictor of accuracy. In particular, we hypothesise that reasoning strategy has an interaction effect with the Number of Initial Differentials which then affects accuracy. This is based on the assumption that it makes more sense for clinicians to narrow or broaden their set of diagnostic hypotheses based on if the patient presentation and history brings to mind a larger or smaller number of initial hypotheses. Below in Figure 3, we plot fitted regression lines that show the relationship between Initial Differentials and Accuracy. We fit a mixed effects model to predict Accuracy in cases from Study 2 via an interaction effect between the Number of Initial Differentials and Reasoning Strategy. We test for an interaction effect using stepwise model comparison. Starting from a null model with no predictors and fixed intercept, we compute chi-square and change in AIC for each increase in model complexity. Table 9 below shows the model comparison metrics for each added model parameter. We find marginal evidence for an interaction effect between Initial Differentials and Reasoning Strategy. 

\newpage

```{r accinteract, include=TRUE, echo=FALSE, warning = FALSE, message=FALSE, out.width='100%', fig.align='center'}

testingDataStrats$Accuracy <- (testingDataStrats$likelihoodAcc/10)

testingDataStrats$classifiedStrat <- as.factor(testingDataStrats$classifiedStrat )

contrasts(testingDataStrats$classifiedStrat) <- contr.treatment(levels(testingDataStrats$classifiedStrat), base = which(levels(testingDataStrats$classifiedStrat) == "HD"))
model <- lmer(Accuracy ~ classifiedStrat*initialDiagnoses + (1|ID),data=testingDataStrats)

intplot <- interact_plot(model, pred = initialDiagnoses, modx = classifiedStrat) +
  labs(y="Accuracy", x = "Initial Diagnoses", colour = "Reasoning Strategy") +
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=16),
        legend.title=element_text(size=16),
        legend.text=element_text(size=16))
print(intplot)

testingDataStrats$caseDominantStrat <- ifelse(testingDataStrats$Condition %in% c("AD","UC","GBS"),"HD","PR")
testingDataStrats$matchingCaseDominantStrat <- ifelse(testingDataStrats$caseDominantStrat==testingDataStrats$classifiedStrat,1,0)

```
_Figure 3: Fitted regression lines for model to explore the interaction between the number of initial diagnoses/differentials (x-axis) and reasoning strategy (as per our multinomial classifier, HD = blue, PR = orange, SI = green) for the online dataset from Study 2 and this interaction's effect on diagnostic accuracy (y-axis)._
\

```{r modelcomparisontable, include=TRUE, echo=FALSE, warning = FALSE, message=FALSE, out.width='100%', fig.align='center'}


model <- lmer(Accuracy ~ classifiedStrat*initialDiagnoses + (1|Condition) + (1|ID),data=testingDataStrats)
summary(model)

pcaModel <- rePCA(model)

model2 <- lmer(Accuracy ~ classifiedStrat*initialDiagnoses + (1|Condition),data=testingDataStrats)

modelcomp <- anova(model2,model)

Anova(model)

################################################

modelnull <- lmer(Accuracy ~ 1 + (1|Condition) + (1|ID),data=testingDataStrats) 

model2 <- lmer(Accuracy ~ classifiedStrat + (1|Condition) + (1|ID),data=testingDataStrats) 

model3 <- lmer(Accuracy ~ initialDiagnoses + (1|Condition) + (1|ID),data=testingDataStrats) 

model4 <- lmer(Accuracy ~ classifiedStrat + initialDiagnoses + (1|Condition) + (1|ID),data=testingDataStrats) 

modelinteraction <- lmer(Accuracy ~ classifiedStrat*initialDiagnoses + (1|Condition) + (1|ID),data=testingDataStrats) 

tbl <- as.data.frame(anova(modelnull,model2,model3,model4,modelinteraction))

rownames(tbl) <- c("Null Model", "Strategy Main Effect", "Diagnoses Main Effect", "Strategy + Diagnoses Main Effects", "Strategy + Diagnoses Interaction")

tbl <- round(tbl,2)

drops <- c("deviance","Df")
tbl <- tbl[ , !(names(tbl) %in% drops)]

colnames(tbl) <- c("Num. Parameters", "AIC", "BIC", "Log Likelihood", "Chi Square", "p value")

knitr::kable(tbl) %>% 
  kableExtra::kable_styling(latex_options=c("HOLD_position","scale_down"))   

```
_Table 9: Incremental model comparisons of linear mixed effects model to predict Accuracy using the Number of Initial Diagnoses and Reasoning Strategy as fixed effects and participants as a random effect. From top-down, the model forms (as expressed in model syntax) are as follows: 'Null Model' <- Accuracy ~ 1 + (1|Participant), 'Strategy Main Effect' <- Accuracy ~ Strategy + (1|Participant), 'Diagnoses Main Effect' <- Initial Differentials + (1|Participant), 'Strategy + Diagnoses Main Effect' <- Strategy + Initial Differentials + (1|Participant), 'Strategy + Diagnoses Interaction' <- Strategy * Initial Differentials + (1|Participant). From left to right, we show values for each model: Number of model parameters (complexity), Aikake Information Criterion, Bayesian Information Criterion, Log Likelihood, Chi Square, p value of Chi Square test._

## Discussion {.unnumbered}

This study of 16 medical students explored the usage of a think-aloud methodology to understand thought processes during medical diagnoses. Using our online interface and recorded verbalisations by students, we aimed to detect clinical reasoning strategies based on criteria adapted from Coderre et al. (2003). The strength of this paradigm is in qualitatively recording medical students' thought process as it involves with information as per our flexible, evolving vignette-based interface design. By recording how participants consider different diagnoses in real time, we are able to understand the reasoning approach students are applying for each case and how this affects their information seeking and confidence behaviour. We are also able to investigate if these reasoning strategies affect diagnostic accuracy.
\

We found that participants' confidence judgements were well calibrated to their objective accuracy, similar to the previous study. The measure of accuracy was different in this study by necessity, in that a case was considered 'correct' with the mention of a correct differential at some point during the case. This measure is most similar to the lenient measure of accuracy from our previous study, in that participants were marked as correct if they considered a correct differential without taking into account its relative likelihood compared to other differentials. However, given that accuracy and our test of calibration being different in this study, it is difficult to compare these findings directly. Our finding of calibrated confidence in both studies however is an indication that medical students express uncertainty appropriately. Similar to the previous study, we also find that medical students are reticent to remove differentials from consideration. In this study, participants report low occurrences of disregarding differentials. This would correspond with our assumption that medical students attempt to remain open minded in their diagnoses. By rarely removing differentials from consideration, students are then observed to broaden their differentials with more information (as the number of differentials being considered at once only increases). 

\
Based on our qualitative findings, we provide support for findings from our previous study. Firstly, several participants reported progressively investigating patient symptoms based on the patient's history and their initial set of diagnostic differentials. This corresponds with our finding in the previous study that the number of initial differentials considered based on the patient history was predictive of information seeking and changes in confidence. This supports evidence for the large weighting on early information received by clinicians, especially to do with history taking, because early information is responsible for the initial set of diagnoses that then guide subsequent information seeking. Secondly, we find a qualitative theme that participants report certain information being standard to seek regardless of the patient case. This corresponds with the finding from our previous study that lower information seeking variability was associated with higher accuracy, with certain information requests in our task being associated with higher accuracy when sought across cases. This further corroborates our evidence for a degree of standardisation in information seeking being useful for maximising diagnostic accuracy.

\
We were able in this study to use think-aloud utterances to detect reasoning strategies on the part of the medical students. We considered three different strategies: Hypothetico-Deductive (HD), Pattern Recognition (PR) and Scheme-Inductive (SI). These strategies represent different approaches to diagnosis, either seeking to be comprehensive in both the information sought and differentials considered or focusing in on a single diagnosis. We found that these choice in reasoning strategies were not solely determined by either an individual's general decision making approach or by specific patient conditions. Whilst these reasoning strategies carry some differences qualitatively, our study was used to investigate how these strategies actually manifest in differences to information seeking and confidence. We found that HD reasoning was associated with reevaluting the diagnoses considered more often when compared to the other approaches, as well as higher diagnostic accuracy. This is different to the results to Coderre et al. (2003), who found PR was associated with accuracy. We would interpret our findings as HD being a 'better' approach for medical students and PR would be more suitable for experienced clinicians who have more cases to draw from.  

\
We also found that reasoning strategies resulted in different factors being predictive of confidence. We found evidence for changes in confidence being predicted by the number of initial differentials for PR cases and by the amount of information seeking for SI cases (as well as information seeking being higher for SI cases). This is potentially interesting for future medical educators around communication of uncertainty and how it relates to clinical reasoning. For a PR approach, participants are likely to be more confident in choosing the correct diagnosis to focus on if they have initially generated a wide of options to consider. For SI reasoning, the focus is on a more systematic, structured approach to diagnosis where participants are more likely to seek information to give each pathophysiological system proper consideration (rather than focusing on specific conditions). When considering these findings in the context of confidence calibration, past findings of miscalibration may in turn be related to systematic tendencies to seek information or generate differentials suboptimally given the reasoning approach used by clinicians. Future work could explore this further by comparing the reasoning between novices and experts with regards to information seeking and confidence. 

\
These two studies together provide a nuanced and in-depth look of the diagnostic process as demonstrated with our vignette-based task. We should consider however the generalisability and ecological validity of these studies. By using a vignette-based paradigm, participants do not actually interact with, observe and treat a patient. We are also limited in terms of the information that is available for clinicians to seek. In addition, participants completed the studies in relatively controlled environments, outside of their usual medical context. Our next study hence aims to study the link between information seeking and confidence, but with a more naturalistic paradigm. To alleviate these concerns of generalisability, we require a paradigm that allows for more open-ended information seeking, observation of a patient that can be treated during cases and the use of a clinical environment akin to the one in which clinicians operate. As previously explored in our systematic review, the use of in-situ research lacks objective markers of accuracy that we utilise. To this end, we utilise virtual reality (VR) in our next study. This allows for a realistic, interactive paradigm where participants observe a (virtual) patient in real-time and can administer treatment (and observe reactions to this treatment in the patient). There is also more openness in terms of the information that can be sought and clinical actions taken, making its use more analogous to real medical contexts. 




