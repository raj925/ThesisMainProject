---
output:
  bookdown::pdf_document2:
    template: templates/template.tex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: [bibliography/references.bib, bibliography/additional-references.bib]
---

# Study 4 - Diagnostic Uncertainty and Information Seeking in Virtual Reality Paediatric Scenarios {.unnumbered}

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache =TRUE)
```

```{r install_packages, include=FALSE}
source('scripts_and_filters/install_packages_if_missing.R')

```

```{r AggregateData1, include=FALSE, eval=knitr::is_latex_output()}
vrData <- as.data.frame(read.csv("./study4data.csv",header=TRUE))
```

```{r infomatrix, include=FALSE, warning=FALSE, message=FALSE}

actioncategories <- read.csv("./assets/actionCategories.csv",header=TRUE,sep=",")

# Get set of actions categorised under History, Exams or Testing
# From external file actioncategories
historyActions <- actioncategories$History
historyActions <- historyActions[which(historyActions != "")]

examActions <- actioncategories$Examination
examActions <- examActions[which(examActions != "")]

TestingActions <- actioncategories$Testing
TestingActions <- TestingActions[which(TestingActions != "")]

treatmentActions <- actioncategories$Treatment
treatmentActions <- treatmentActions[which(treatmentActions != "")]

# All actions considered here are combination of these three
actionsMasterList <- c(historyActions, examActions, TestingActions)
actionsMasterList <- actionsMasterList[nzchar(actionsMasterList)]

infoSeekingMatrixVR <- data.frame(matrix(nrow = nrow(vrData), ncol = length(actionsMasterList)))

# Add to matrix from vectors stored as strings in vrData so we get a binary matrix.
for (n in 1:nrow(vrData))
{
  infoSeekingMatrixVR[n,] <- str_split(vrData$actionVector[n],"\n")[[1]]
}

infoSeekingMatrixVR <- apply(infoSeekingMatrixVR, 2, as.numeric)

infoSeekingMatrixVR <- as.data.frame(infoSeekingMatrixVR)
colnames(infoSeekingMatrixVR) <- actionsMasterList

# Use colSums to calculate the sum of each column
sumVals <- colSums(infoSeekingMatrixVR)

# Use the column sums to filter columns with at least one 1
filtered_data <- infoSeekingMatrixVR[, sumVals > 0]

# Get pairwise distances/dissimilarity
distancesVR <- infoSeekingMatrixVR %>% proxy::dist(method = "Hamman") %>% as.matrix()

infoSeekingMatrixVR$scenario <- vrData$Scenario
infoSeekingMatrixVR$OMSScore <- vrData$OMSScore
infoSeekingMatrixVR$t1Confidence <- vrData$t1Confidence
infoSeekingMatrixVR$t1DiagnosisScore <- vrData$t1DiagnosisScore

```

```{r completedata, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}
# Get data where participants have completed two scenarios

ids <- vrData$ParticipantID
# Create a frequency table
freq_table <- table(ids)
# Identify elements with more than one occurrence
repeatIDs <- names(freq_table[freq_table > 1])
vrCompleteData <-  vrData[vrData$ParticipantID %in% repeatIDs,]
vrCompleteData <- vrCompleteData[!is.na(vrCompleteData$t2Confidence),]
nFullPpts <- nrow(vrCompleteData)/2
```

```{=tex}
\adjustmtc
\markboth{VR Study}{}
```
<!-- For PDF output, include these two LaTeX commands after unnumbered chapter headings, otherwise the mini table of contents and the running header will show the previous chapter -->

## Introduction {.unnumbered}

In this study, we aim to extend our previous findings using a virtual reality (VR) experimental paradigm that is more naturalistic to real medical practice. Participants were unable to see the patient in the vignette task, which is important given that the visual state (or distress) of a patient can be informative for a doctor in diagnosing the patient. By simulating this, we aim to investigate the link between information seeking and confidence in a more open-ended clinical situation that has a wider range of possible options for history taking, physical examination, testing and treatment options when compared to our vignette paradigm (which constrained the amount of information available on each case for usability). Given this increased flexibility, we can look at more fine-grained aspects of information seeking, as well as the effect of ongoing treatment of patients on confidence. Our vignette task was static in time, in that the patient does not change over the course of a case (i.e. improving or deteriorating over time). This VR paradigm then allows for doctors to start managing the patient’s symptoms and even using reactions to their treatment plan in order to change their understanding of the patient. 

\
In our previous two studies, we have found evidence for a general tendency for medical students to broaden the range of differentials they are considering as they receive more information. These studies, made use of patient vignettes where there was no requirement to treat the patient and no subsequent observation of improvement or deterioration in the patients' state. This begs the question of whether medical students still show a tendency to broaden the differentials they are considering when beginning a treatment plan for a patient requires a degree of narrowing of diagnoses. We predict then that with the use of VR scenarios where patients are deteriorating and require treatment, medical students will be more likely to narrow the differentials they are considering due to the medical situation demanding it. Similar to our previous studies, we measure the range of diagnostic differentials that students are considering at multiple points during the scenarios. Our online study found the initial diagnostic breadth of students was predictive of their subsequent information seeking and changes in confidence. We not only look to replicate this finding in a more naturalistic medical context but also investigate whether initial diagnostic breadth is predictive of patient treatment too. 

\
One of the aforementioned benefits of using a VR paradigm is that we are able to simulate a real medical environment. This includes the wide range of possible actions available to a clinician. Using our paradigm, we are able to record every action or information request made by participants. These actions can then be categorised into a number of areas: Patient History, Physical Examinations, Testing and Treatment. Our findings around initial diagnostic breadth and the qualitative theme from the previous study on the importance of an in-depth history to base diagnosis on necessitate a deeper look at history taking during diagnoses. Our vignette paradigm used fairly limited patient histories, with a perceived lack of detail potentially explaining why some participants expressed diagnostic uncertainty. In the VR paradigm, there is much detail available on the patient's medical history, including follow-up questions to patients to access more detail on their condition. For example, if a patient is feeling pain, the interactive nature of VR allows participants to ask about the nature of the pain (e.g. whether it is a dull or sharp pain, whether anything makes the pain better/worse etc.). With the wider range of actions available to participants, we not only look at information seeking as a whole, but also information seeking with each of these categories. In particular, we are interested in how the comprehensiveness of participants' history taking affects their subsequent confidence and considered diagnoses. Given that VR also simulate active medical situations, participants can be graded based on the information they seek, the tests they run and treatment they administer. 


## Methods {.unnumbered}

### Participants {.unnumbered}

We recruited medical students based at the University of Oxford in their second year of clinical training (which equates to three or four years of educational experience). 76 students completed this study.

### Materials {.unnumbered}

We used VR scenarios implemented by Oxford Medical Simulation (OMS, [https://oxfordmedicalsimulation.com/](https://oxfordmedicalsimulation.com/)), a company that implements bespoke VR software for medical education and simulation. Participants in this study were medical students based in Oxford who were at the time taking part in VR-based teaching sessions as part of their medical degrees. Students performed the scenarios using Oculus Quest 2 VR headsets. Scenarios were based in paediatrics, meaning that the patients in the scenario were children who were attending the hospital with their legal guardian. Each scenario features a visual 3D implementation of a basic ward room in a hospital. Participants are shown a (child) patient, their guardian and a nurse who can help with certain treatment and testing. All of the ‘avatars’ in the scenario can be questioned by the participant using a predefined set of requests/actions (e.g. asking the nurse to check blood pressure, asking the patient/child about if they are in pain). The scenarios have full sound (e.g. being able to hear the patient’s lung auscultation) and the avatars are voiced. 

```{r screenshot1, include=TRUE, echo=FALSE, out.width='100%', fig.align='center'}

knitr::include_graphics("./assets/VRScreenshot1.png")

```

```{r screenshot2, include=TRUE, echo=FALSE, out.width='100%', fig.align='center'}

knitr::include_graphics("./assets/VRScreenshot2.png")

```

\
Each participant completed two scenarios over two separate VR sessions. The sessions were held around one month apart. During each session, the participants each performed one scenario in VR and observed their partner during their scenario. Participants also engaged in peer-to-peer feedback discussions as part of their education. The scenarios presented in each sessions are described below (students are split into two groups, shown below as groups A and B, each performing a different pair of scenarios in a fixed order):

\
* **Session One:**
 + * _Group A:_ patient/child is a 6-year-old-girl presenting with a 1 day history of central abdominal pain and thirst. She was generally unwell for 2 days prior, with reduced appetite and a sore throat. Collateral history reveals Type 1 Diabetes and erratic blood sugars. (**Underlying Condition: Diabetic Ketoacidosis**)
 + * _Group B:_ patient/child is a 5-year-old boy presenting with worsening shortness of breath, wheeze, and signs of respiratory distress, on the background of 2 days of likely viral illness. He has a medical history of asthma and has had similar exacerbations in the past. (**Underlying Condition: Acute Severe Exacerbation of Asthma**)

\
* **Session Two:**
 + * _Group A:_ patient/child is a 5-year-old boy presenting with shortness of breath and drowsiness (**Underlying Condition: Chest Sepsis/Pneumonia**)
 + * _Group B:_ patient/child is a 5-year-old girl with a 1 day history of sore throat and fever. She starts having a generalised tonic clonic seizure during the scenario. (**Underlying Condition: Febrile seizure on background of tonsillitis**)

### Procedure {.unnumbered}

The aim for students in the scenarios was to diagnose the patient, begin treatment and hand over the case to a senior with appropriate understanding of the patient (handovers were conducted using a standardised framework known as SBAR, meaning that clinicians have to brief the senior on the Situation, Background, Assessment and Recommendation for the patient). They were expected to take a clinical history, complete a physical examination, start emergency treatment to stabilise the patient and escalate to a senior clinician for further input. Whilst in the scenario, participants can learn about the patient’s medical history, check key parameters (such as temperature, pulse, blood pressure, respiratory rate etc), perform physical exams/tests and begin certain treatment actions (such as administering oxygen or prescribing medication). Participants were also expected by the end of the scenario to be able to give an explanation of the situation to the patient's parent/guardian. All participants have the same starting point in each scenario and the patient in the scenario deteriorates in an identical way if the participant takes no action. If participants undertake certain actions, the patient improves both in terms of vital signs (e.g. blood pressure, heart rate, oxygen saturation etc.) and in their response to questions (e.g responding "Yes, I feel a bit better" to a question of how they are feeling). If participants select irrelevant actions, the patient does not improve, whilst some actions will result in the patient's state deteriorating.

\
After 5 minutes in the scenario (by which point it is expected that participants would have a history of the patient and have started some early assessment of the patient), participants are asked to pause the scenario (taking off their VR headset) and fill in a brief questionnaire on paper. Multiple VR participants were performing the scenario simultaneously and were paired with another student who would watch their performance. This other student would aid with administering the questionnaire, with the student subsequently switching roles for the other scenario. The VR participant was asked in the questionnaire to answer the follow (this is considered time point 1):

* "Please say all the conditions that you are currently considering or are concerned about for this patient. Include any/all common, rare or contributing conditions you are considering. For each, please rate how likely you think they are on a scale of 1 (low) to 5 (high)."
* "On a scale of 1-10, how confident are you that you understand the patient’s condition?"
* "How severe do you think the patient’s condition is on a scale of 1 to 10?" (Each point of the scale represented a different clinical action/course, with 1 representing "Discharge in <4 hours, no follow up" and 10 representing "Requires arrest/peri arrest team.")

The questionnaire was kept relatively short to minimise disruption to the scenario. This was due to the extra time that could be expended by asking participants to take off and put on the headset again to readjust to VR. Participants were given 20 minutes to complete the scenario, but could end the scenario early if they feel that they have completed the necessary care and tests for the patient. After completing the scenario, participants completed a second questionnaire on a separate sheet (this is considered time point 2). The second questionnaire featured the same three questions as the first questionnaire (see above), as well as the following questions:

* "To what extent would you be prepared to leave the patient prior to a senior review" (this question was answered using a visual analogue scale)
* "Did you complete all the history, examinations and investigations necessary? If not, what else would you do if given more time?"

### Data Analysis {.unnumbered}

The dependent variables that we derive are as follows:

* Performance Score: The OMS software implements a series of objectives for each scenario, which are tasks or actions that the participant is expected to have completed within the allotted time. This can include administering oxygen, prescribing a particular medication or calculating the Patient Early Warning Score (PEWS). The proportion of completed objectives is used as a score of the participant’s performance during the scenario. 

* Confidence Change: the participants’ confidence in their understanding of the patient’s condition is recorded at two time points, with the first being after 5 minutes (out of the 20 minute time limit) and the second being after the participant has finished the scenario. Confidence at each stage is recorded on a 10 point scale (1-10). The difference between the second and the first confidence rating is taken, such that a positive value indicates that the participant has increased their confidence over the course of the scenario. 

* Number of Differentials: participants are asked to record all the diagnostic differentials that they are considering at the two aforementioned time points. Hence, the total number of differentials is recorded at each stage. The Initial Number of differentials is the number of diagnoses provided at the pause point.

* Diagnostic Appropriateness: each participant's set of differentials are assessed for how appropriate they are for the scenario. Each scenario has a set of differentials that are considered most likely, probable and improbable (with any others considered incorrect). To calculate a score for how appropriate the diagnoses are, we sum the likelihood values provided for all differentials that were marked as most likely or probable. We then add these to the sum of likelihood values for improbable differentials divided by two. This sum is divided by the total sum of all differentials. This overall measure then measures what proportion of the participants’ likelihoods are dedicated to probable differentials. However, we also penalised participants for providing few differentials, such that high scoring sets of differentials are larger sets of likely or probably differentials. 

\
We also derived measures of information seeking similar to previous studies. The VR scenarios are far richer in terms of the available set of information for participants when compared to the vignette paradigm. For our analysis, we record all actions (or ‘clicks) made by participants whilst in the scenario. Actions are categorised into a number of groups. The main categories are labelled as History, Examination or Testing, similar to in the vignette study. This set of information is mostly similar across scenarios though there are minor differences especially in the History category. Across scenarios, there are 35 possible History actions, 29 Examination actions and 18 Testing actions. This especially means that in comparison to the vignette paradigm, participants can take more detailed patient histories and can receive very different pieces of information depending on what they request from patient documentation and from asking the patient/guardian in the scenario. Outside of these categories, there are other actions available to participants, such as administering medication for the patient, calling for help or providing reassurance to the patient/guardian, but these are not used for our analysis. After categorising the participants’ actions, we define a number information seeking measures:

* History Taking: this is the number of History actions for a given scenario that take place before the pause point. 
* Total Information Seeking: this is the number of actions classified under History, Examination or Testing across the scenario.
* Information Value: to calculate the value of each information sought across these categories, we calculate the difference in OMS performance score for participants with or without that information. We then sum all sought information values for each participant within each of the information categories (History, Examination, Testing).
* Amount of Treatment: this is the number of actions classified as treatment of the patient across the scenario. 

As all actions are recorded with timestamps in the output dataset, we categorise whether actions occurred before or after the pause point (5 minutes in). Hence, we can investigate information seeking before and after the pause point where participants record their initial diagnoses and confidence. 

## Results {.unnumbered}

### Overall Performance {.unnumbered}

Say how many participants completed each scenario

```{r anovadf, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

anovaDf <- data.frame(c(vrData$t1Confidence,vrData$t2Confidence),
                      c(vrData$t1Severity,vrData$t2Severity),
                      c(vrData$t1numOfDiagnoses,vrData$t2numOfDiagnoses),
                      c(rep("1",nrow(vrData)),rep("2",nrow(vrData))),
                      c(rep(vrData$Scenario,2)),
                       c(rep(vrData$scenGroup,2)),
                      c(rep(vrData$ParticipantID,2)))

colnames(anovaDf) <- c("Confidence","Severity","Diagnoses", 
                       "Timepoint","Scenario","ScenarioGroup","ID")

```

Correlate performance scores?

Table 1 below shows average values for our key dependent variables by scenario.

```{r descstats, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

vrDescStats <- vrData %>%
  group_by(Scenario) %>%
  dplyr::mutate(N = n()) %>%
  dplyr::summarise(n = mean(N),
                   MeanPerformance = round(mean(OMSScore,na.rm=T),2),
                   MeanInformationSeeking = round(mean(filteredActions,na.rm=T),2),
                   MeanInitialConfidence = round(mean(t1Confidence,na.rm=T),2),
                   MeanConfidenceChange = round(mean(confidenceChange,na.rm=T),2),
                   MeanInitialDiagnoses = round(mean(t1numOfDiagnoses,na.rm=T),2))

knitr::kable(vrDescStats) %>% 
  kableExtra::kable_styling(latex_options="HOLD_position")

```

Diagnoses by timepoint and scenario

```{r diagtime, include=TRUE, echo=FALSE, out.width='100%', fig.align='center', warning=FALSE, message=FALSE}
p <- ggplot(anovaDf, aes(x=Timepoint, y=Diagnoses, fill=Scenario)) + 
    geom_violin() +
    stat_summary(fun.data=data_summary, geom="crossbar", width=0.05, position=position_dodge(0.1)) +
    facet_wrap(~Scenario) +
    ylim(0,8) +
    theme_classic()

print(p)
```

Confidence by timepoint and scenario

```{r confidencetime, include=TRUE, echo=FALSE, out.width='100%', fig.align='center', warning=FALSE, message=FALSE}
p <- ggplot(anovaDf, aes(x=Timepoint, y=Confidence, fill=Scenario)) + 
    geom_violin() +
    stat_summary(fun.data=data_summary, geom="crossbar", width=0.05, position=position_dodge(0.1)) +
    facet_wrap(~Scenario) +
    theme_classic()

print(p)
```

### Initial Diagnostic Breadth {.unnumbered}

ConfidenceChange ~ InitialDiagnoses + (1|Scenario) + (1|Participant)

InformationSeeking ~ InitialDiagnoses + (1|Scenario) + (1|Participant)

### Information Seeking and Confidence {.unnumbered}

Initial Confidence ~ Information Seeking Before

```{r initialconfidence, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

model <- lmerTest::lmer(t1Confidence ~ numOfHistoryActionsBeforePause + numOfExamActionsBeforePause + numOfTestingActionsBeforePause + (1|Scenario) + (1 | ParticipantID), data=vrData)

pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(t1Confidence ~ numOfHistoryActionsBeforePause + numOfExamActionsBeforePause + numOfTestingActionsBeforePause + (1|Scenario), data=vrData)
modelcomp <- anova(model2,model)

# Get the fitted values from the model
fittedValues <- fitted(model2)

# Get the actual observed values
obsValues <- vrData[!is.na(vrData$t1Confidence),]$t1Confidence

# Create a data frame for plotting
plotDf <- data.frame(
  observedConfidence = obsValues,
  fittedConfidence = fittedValues
)

# Plot using ggplot2
#ggplot(plotDf, aes(x = observedConfidence, y = fittedConfidence)) +
 # geom_point() +
 # geom_abline(slope = 1, intercept = 0, color = "red") +
 #  geom_smooth(method = "lm", se = TRUE, color = "blue") + 
 # labs(x = "Observed Confidence", y = "Fitted Confidence (mixed effects model)") +
 # theme_classic(base_size = 20)


```

Information Seeking After ~ Initial Confidence

```{r subinfoseeking, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

### Initial confidence predicting subsequent information seeking

model <- lmerTest::lmer(numOfHistoryActionsAfterPause ~ t1Confidence + (1|Scenario) + (1 | ParticipantID), data=vrData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(numOfHistoryActionsAfterPause ~ t1Confidence + (1 | ParticipantID), data=vrData)
modelcomp <- anova(model2,model)

model <- lmerTest::lmer(numOfExamActionsAfterPause ~ t1Confidence + (1|Scenario) + (1 | ParticipantID), data=vrData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(numOfExamActionsAfterPause ~ t1Confidence + (1|Scenario), data=vrData)
modelcomp <- anova(model2,model)

model <- lmerTest::lmer(numOfTestingActionsAfterPause ~ t1Confidence + (1|Scenario) + (1 | ParticipantID), data=vrData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(numOfTestingActionsAfterPause ~ t1Confidence + (1|Scenario), data=vrData)

# Get the fitted values from the model
fittedValues <- fitted(model2)

# Get the actual observed values
obsValues <- vrData[!is.na(vrData$t1Confidence),]$numOfTestingActionsAfterPause

# Create a data frame for plotting
plotDf <- data.frame(
  observedTestingActions = obsValues,
  fittedTestingActions = fittedValues
)

```


```{r subinfoseekingplot, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, out.width='100%', fig.align='center'}

# Plot using ggplot2
ggplot(plotDf, aes(x = observedTestingActions, y = fittedTestingActions)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
    geom_smooth(method = "lm", se = TRUE, color = "blue") + 
  labs(x = "Observed Testing Actions", y = "Fitted Testing Actions (mixed effects model)") +
  theme_classic(base_size = 20)

```

Final Confidence ~ Treatment + Time to Ask for Help

```{r finalconfidence, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

helpedData <- vrData[vrData$totalHelpStart<9999,]

model <- lmerTest::lmer(t2Confidence ~ numOfTreatmentActions + totalHelpStart + (1|Scenario) + (1 | ParticipantID), data=helpedData)
pcaModel <- rePCA(model)
model2 <- lmerTest::lmer(t2Confidence ~ numOfTreatmentActions + totalHelpStart + (1 | ParticipantID), data=helpedData)
modelcomp <- anova(model2,model)

# Get the fitted values from the model
fittedValues <- fitted(model)

# Get the actual observed values
obsValues <- helpedData[!is.na(helpedData$t2Confidence),]$t2Confidence

# Create a data frame for plotting
plotDf <- data.frame(
  observedConfidence = obsValues,
  fittedConfidence = fittedValues
)

```


```{r finalconfidenceplot, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, out.width='100%', fig.align='center'}

# Plot using ggplot2
ggplot(plotDf, aes(x = observedConfidence, y = fittedConfidence)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red") +
    geom_smooth(method = "lm", se = TRUE, color = "blue") + 
  labs(x = "Observed Final Confidence", y = "Fitted Final Confidence (mixed effects model)") +
  theme_classic(base_size = 20)


```
### Diagnostic Appropriateness {.unnumbered}

Diagnostic Appropriateness ~ Value
Diagnostic Appropriateness split with binary classifier

```{r pcaclassifier, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

#######################################
# Use of regular PCA

infoSeekingMatrixVRPCA <- infoSeekingMatrixVR[,1:82]
infoSeekingMatrixVRPCA <- infoSeekingMatrixVRPCA[,-which(names(infoSeekingMatrixVRPCA) %in% c("past medical history","measure oxygen saturations & heart rate","measure blood pressure"))] 
# Remove columns that are sought on most scenarios

# Perform PCA on binary data
pca_result <- PCA(infoSeekingMatrixVRPCA, graph = FALSE)

# Scree plot shows elbow around 5
#qplot(c(1:79), pca_result$eig[,2]) + 
#  geom_line() + 
#  xlab("Principal Component") + 
#  ylab("Variance Explained") +
#  ggtitle("Scree Plot") +
#  theme_classic()

#########

pca_result <- principal(infoSeekingMatrixVRPCA, nfactors = 4, rotate = "promax")
pcs <- pca_result$scores
weights <- pca_result$loadings

topPCS <- weights


rownames(topPCS) <- c(1:79)

# Sort the columns alphabetically
colnames(topPCS) <- c("RC1", "RC2", "RC3", "RC4")
topPCS <- topPCS[, order(colnames(topPCS))]


# Determine the maximum absolute value in the loadings matrix
max_abs <- max(abs(c(min(topPCS), max(topPCS))))
# Normalize the data to the range [-1, 1]
normalized_topPCS <- topPCS / max_abs
# Define a custom color palette with clear differentiation between negative, zero, and positive values
n_colors <- 20
color_palette <- colorRampPalette(c("blue", "white", "red"))(n_colors)

#apcluster::heatmap(normalized_topPCS,
 #                  main = "Variable Weights (Normalised) Clustered by PC",
#                   xlab = "PCs", ylab = "Test",
#                   col = color_palette, scale = "none", breaks = seq(-1, 1, length.out = n_colors + 1), Colv = NA)

# Define colors for legend
#legend_colors <- c("blue", "white", "red")

# Add color key
#legend("topleft", legend = c("Negative", "Zero", "Positive"),
   #    fill = legend_colors, title = "Color Key", cex = 0.8)

pcs <- pcs[, order(colnames(pcs))]
pcDF <- data.frame(pcs[,1],pcs[,2],pcs[,3],pcs[,4])
colnames(pcDF) <- c("PC1","PC2","PC3","PC4")
pcDF$pid <- vrData$ParticipantID
pcDF$condition <- vrData$Scenario
pcDF$diagChange <- vrData$diagnosticChange
pcDF$confidenceChange <- vrData$confidenceChange
pcDF$OMSScore <- vrData$OMSScore
pcDF$diagScore <- vrData$t1DiagnosisScore

mixedPCModel = lm(diagScore ~ PC1 + PC2 + PC3 + PC4 + condition, data = pcDF)
#summary(mixedPCModel)

pcDF$diagGroup <- ifelse(pcDF$diagScore < median(pcDF$diagScore),0,1)
pcDF$diagGroup <- as.factor(pcDF$diagGroup)
pcDF$conditionFactor <- as.factor(pcDF$condition)

thresh<-seq(0,1,0.001)
#specify the cross-validation method
ctrl <- trainControl(method = "LOOCV", savePredictions = TRUE)

modelglmVR<-train(diagGroup ~ PC1 + PC2 + PC3 + PC4, method = "glm", family = binomial(link=probit), data = pcDF, trControl = ctrl)
prediglmVR<-predict(modelglmVR,type = "prob")[2]

```

```{r pcaclassifierplot, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, out.width='100%', fig.align='center'}

# Plot all test results on one ROC curve
rocPlot <- roc.plot(x=pcDF$diagGroup=="1",pred=cbind(prediglmVR),legend = T,
                    leg.text = c("GLM"),thresholds = thresh)$roc.vol

print(rocPlot)

```